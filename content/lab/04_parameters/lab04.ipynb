{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../shared/img/banner.svg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04 - Comparing Bootstrap and Model-Based Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "\n",
    "import utils.util as util\n",
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Recognize the similarity between sampling from a model posterior and bootstrap sampling and note the differences.\n",
    "1. Understand the benefits and drawbacks of incorporating strong and weak prior information into a model.\n",
    "1. Learn to work with the following `pyMC` tools: the `observed` kwarg, `sample_posterior_predictive`, and `sample_prior_predictive`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll look a variety different models of the same data.\n",
    "The data will be Gaussian, or Normal, data, with varying numbers of samples.\n",
    "\n",
    "The first model will be a bootstrapping-based model, of the type used in data8.\n",
    "The remaining models will be `pyMC` models.\n",
    "They are defined below, using the function `define_normal_model`.\n",
    "We will compare and contrast the bootstrapping-based model to the `pyMC` models,\n",
    "in terms of their prior distributions, their posterior distributions, and the inferences drawn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the data.\n",
    "The true mean and standard deviation of the data are set by the variables `true_mu` and `true_sigma`.\n",
    "The resulting sample will almost surely have a different mean and standard deviation.\n",
    "\n",
    "In a real scenario, we wouldn't know the values of `true_mu` and `true_sigma`.\n",
    "We make models to try to represent and quantify our uncertainty about what those values might be.\n",
    "Working with a case where we know what the real answer is, even if it's a bit artificial, helps us gain a better understanding of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu = 2.\n",
    "true_sigma = 1.\n",
    "\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util.generate_normal_data(true_mu, true_sigma, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, before we start modeling data, even with bootstrapping,\n",
    "we should "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()[0], df.std()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df, width=0.25); sns.rugplot(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've done previously, we can use bootstrap sampling to estimate our uncertainty about the values of parameters.\n",
    "\n",
    "In the cells below, run bootstrapping on the data to estimate the uncertainty in the value of $\\mu$\n",
    "by producing a histogram of the bootstrapped values.\n",
    "Don't be afraid to reuse code from previous labs/homeworks.\n",
    "\n",
    "The first cell produces a list of length `num_boots` of pandas `Series`, each of which is a bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boots = 1000\n",
    "\n",
    "boots = [df[0].sample(frac=1, replace=True) for _ in range(num_boots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Normal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our normal models will all have a graph as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.make_normal_model_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the variable x, which we observe $N$ times,\n",
    "is parameterized by a variable we don't observe: $\\mu$.\n",
    "\n",
    "This variable sets the mean of our data.\n",
    "\n",
    "The function `define_normal_model` creates a model of this kind,\n",
    "where $\\mu$ is a Normal random variable whose mean and standard deviation are set by\n",
    "the first and second arguments, `mu_prior_mean` and `mu_prior_sd`.\n",
    "\n",
    "The random variable $\\mu$ represents our state of belief about what the mean of our data might be,\n",
    "before we have observed any data.\n",
    "On average, this variable will have the value `mu_prior_mean` and samples will be spread around that value\n",
    "with standard deviation `mu_prior_sd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_normal_model(mu_prior_mean, mu_prior_sd, observed_data):\n",
    "    with pm.Model() as normal_model:\n",
    "        mu = pm.Normal(\"mu\", mu=mu_prior_mean, sd=mu_prior_sd)\n",
    "        \n",
    "        pm.Normal(\"xs\", mu=mu, sd=true_sigma, observed=observed_data)\n",
    "    return normal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three models: `agnostic_model`, `right_model`, and `wrong_model`.\n",
    "\n",
    "Agnostic means, loosely, [\"knowing nothing\"](https://en.wiktionary.org/wiki/agnostic).\n",
    "This model makes much weaker assumptions about what the value of $\\mu$ probably is.\n",
    "This is the model we have when we know little about our data, aside from the form of the model.\n",
    "\n",
    "The `right_model`, on the other hand, makes a much stronger assumption on what the value of $\\mu$ is,\n",
    "and it has the right guess.\n",
    "This is the kind of model we have when we are well-informed about our data.\n",
    "\n",
    "The `wrong_model` makes just as strong an assumption as the right model does,\n",
    "but its assumption is wrong. It expresses the belief that the mean of the data is within ±0.75 of 0, but it is not.\n",
    "This is the kind of model we have when we are mis-informed about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnostic_model = define_normal_model(0, 10, df)\n",
    "\n",
    "wrong_model = define_normal_model(0, 0.25, df)\n",
    "\n",
    "right_model = define_normal_model(true_mu, 0.25, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our priors on $\\mu$ for each model. Remember that the prior is what we believe about our data before looking at the particular values we observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw samples of `mu` from the prior using `sample_prior_predictive` for each of the three `pyMC` models.\n",
    "The cell below shows how to do this for the `right_model`.\n",
    "\n",
    "Plot a histogram for the samples from each model and answer the questions below.\n",
    "\n",
    "Why is it fair to say that the agnostic model makes weaker assumptions about the value of $\\mu$ than do the other models?\n",
    "What parameter choice, in the definition of the model, is responsible for this?\n",
    "Above, it was claimed that the wrong model encodes the belief that $\\mu$ is within about ±0.3 of 0.\n",
    "How would you determine that from the histogram?\n",
    "In these terms, what belief does the right model represent?\n",
    "Why don't we plot a prior for our bootstrapping-based model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prior_samples = 10000\n",
    "\n",
    "right_prior = pm.sample_prior_predictive(samples=num_prior_samples,\n",
    "                                         model=right_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Posteriors of the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior represents what we believe about our data after we've looked at the values we've observed.\n",
    "\n",
    "First, let's look at what each of our models believes about the true mean and the standard deviation of the data\n",
    "after having observed the values.\n",
    "Plot histograms of the samples of $\\mu$ from each of our models\n",
    "(the three `pyMC` models and the bootstrapping model)\n",
    "and then answer the questions below.\n",
    "\n",
    "Remember that, for unobserved variables in `pyMC` models, we sample from the posterior by running `pm.sample`.\n",
    "Make sure to keep the `trace` around, since we'll need that to look at our posterior over future values of the data.\n",
    "\n",
    "For the unosberved variables in the bootstrapping model, we need to calculate their values on all of the bootstrapped samples.\n",
    "\n",
    "\n",
    "For the `pyMC` models, how have the beliefs about $\\mu$ changed?\n",
    "Do they differ more for some of the models than for others? If so, which one(s)?\n",
    "Can you give an intuitive explanation, in your own words, for why each model's beliefs changed the way they did?\n",
    "Directly compare the posterior for the agnostic model to that for the bootstrapping model.\n",
    "If they are different, why are they different? If they are similar, why are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Posterior of the Observed Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we know the correct value of $\\mu$, since we're simulating the entire process.\n",
    "In a real experiment, we wouldn't know the true value, so we wouldn't know which model to trust.\n",
    "That is, in our simulation, we know which model is the right model and which model is the wrong model,\n",
    "but we wouldn't know that in most real life situations.\n",
    "\n",
    "So how do we determine which model to believe?\n",
    "\n",
    "The first thing to check is whether the model's posterior over the observed variable matches\n",
    "the data we observed. If it doesn't, then the model doesn't consider the data we observed very likely.\n",
    "And while it's possible that the data we drew was unrepresentative, the larger a sample, the less likely that is,\n",
    "and so the more evidence we have that the model is incorrect.\n",
    "\n",
    "Because it can be used to predict future values of the observed variable, this distribution is known as the \"posterior predictive\" distribution.\n",
    "\n",
    "The quantitative version of this approach to model comparison is called _maximum likelihood modeling_,\n",
    "and it's especially common in cases where an uninformative prior is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below will generate samples from the posterior over the observed variable x for the right model using `sample_posterior_predictive`. Note that it requires a copy of a `trace` from the same model.\n",
    "\n",
    "The cells beneath that will compare the posterior predictive to the observed data using a histogram and rugplot.\n",
    "\n",
    "Use `sample_posterior_predictive` to draw samples according to the posteriors of the wrong and agnostic models.\n",
    "Then, add the results to the `postpreds` list, as in the example code below (order is important!),\n",
    "and then run the cell containing `compare_multiple_postpreds_to_observed`\n",
    "to plot the comparisons.\n",
    "Then answer the questions below.\n",
    "\n",
    "Example Code:\n",
    "```python\n",
    "postpreds = [right_postpred[\"xs\"], agnostic_postpred[\"xs\"], wrong_postpred[\"xs\"]]\n",
    "```\n",
    "\n",
    "How well does the posterior predictive distribution match the data for each model?\n",
    "Which model is doing worst according to this criterion?\n",
    "How do its predictions differ from the observed data? Why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posterior_samples = 1000\n",
    "\n",
    "right_postpred = pm.sample_posterior_predictive(\n",
    "    right_trace, samples=num_posterior_samples, model=right_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postpreds = [right_postpred[\"xs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.compare_multiple_postpreds_to_observed(\n",
    "    postpreds, df, titles=[\"Right Model\", \"Agnostic Model\", \"Wrong Model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of picking models by how well their samples match the data has its flaws.\n",
    "For example, the bootstrap always come out looking better than any other model possibly can.\n",
    "Let's see why this is.\n",
    "\n",
    "The observed values for 100 samples from the posterior predictive distribution of each model will be plotted below.\n",
    "\n",
    "Note that the \"bootstrapping\" is the same as \"sampling from the posterior predictive\" for a bootstrap-based model.\n",
    "Sample values will appear as black tick marks, with the observed values overlaid in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boots_np = np.asarray([np.asarray(boot) for boot in boots])\n",
    "\n",
    "util.elementwise_compare_multiple_postpreds_to_observed(\n",
    "    [boots_np] + postpreds, df,\n",
    "    titles=[\"Bootstraps\", \"Right Model\", \"Agnostic Model\", \"Wrong Model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain, in your own words, why model comparison on the basis of posterior predictive similarity to the data would always pick the bootstrap model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison by Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantitatively characterize which models are best supported by the data,\n",
    "we need to consider not just how well the models predict the data,\n",
    "but also how well the prior characterizes the data.\n",
    "\n",
    "The full details of how this is done are outside of the scope of this class, since\n",
    "1) comparing models via sampling is an area of active development in `pyMC` and in research\n",
    "2) existing methods tend to use ideas outside of the core concepts of this class.\n",
    "\n",
    "The curious can look up [Bayes factors](https://en.wikipedia.org/wiki/Bayes_factor).\n",
    "\n",
    "But in short, we can perform a form of model comparison by computing a statistic called\n",
    "the [surprise](https://charlesfrye.github.io/stats/2016/03/29/info-theory-surprise-entropy.html),\n",
    "or the negative log-likelihood, on samples from the prior.\n",
    "A model with lower surprise, averaged over samples from the prior, is better.\n",
    "This method is known as marginal likelihood comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surprise(model, prior):\n",
    "    model_logp = model.observed_RVs[0].logp\n",
    "    surprises = [-model_logp(mu=mu) for mu in prior[\"mu\"]]\n",
    "    return np.mean(surprises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_surprise(right_model, right_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_surprise(agnostic_model, agnostic_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_surprise(wrong_model, wrong_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the surprise for the bootstrap model is independent of the data.\n",
    "It's just the number of datapoints times the log of the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_surprise(right_model, right_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_logp = N * np.log(N)\n",
    "\n",
    "bootstrap_logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this \"surprise\" criterion, which model is best?\n",
    "How does the agnostic model perform? Why? _Hint: what do samples from the agnostic prior look like?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens with more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the amount of data increases, the effect of the prior decreases, and the posteriors of all models become about the same.\n",
    "\n",
    "Return to the top of the lab and set the variable `N` to `500` instead of `10`.\n",
    "Re-run the entire lab and answer the questions below.\n",
    "All of the code should run correctly with this change.\n",
    "If you're working with a partner, have one person change the value of `N` and the other keep it the same,\n",
    "as the questions involve comparing the results across values of `N`.\n",
    "\n",
    "Do the priors change? Why or why not?\n",
    "What's different about the posteriors over $\\mu$? How aobut the predictive posteriors over x?\n",
    "Keep an eye of the scale of the x-axis.\n",
    "What happens to the surprises? Does the ranking of the models by surprise change or not?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
