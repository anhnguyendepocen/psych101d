{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../shared/img/banner.svg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04 - Comparing Bootstrap and Model-Based Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "\n",
    "import utils.util as util\n",
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = Notebook(\"ok/config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "def compare_inferential_distributions(samples, labels=None, true_value=None, xlabel=None, ax=None):\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(figsize=(12, 4))\n",
    "        \n",
    "    if labels is None:\n",
    "        labels = [labels] * len(samples)\n",
    "        \n",
    "    [ax.hist(sample, label=label, normed=True, alpha=0.5)\n",
    "     for sample, label in zip(samples, labels)]\n",
    "    \n",
    "    if true_value is not None:\n",
    "        ax.vlines(true_value, 0, ax.get_ylim()[1] * 0.25)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Recognize the similarities between sampling from a `pyMC` model posterior and bootstrap sampling and note the differences.\n",
    "1. Understand the benefits and drawbacks of incorporating strong and weak prior information into a model.\n",
    "1. Learn to work with the following `pyMC` tools: the `observed` kwarg, `sample_posterior_predictive`, and `sample_prior_predictive`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll look a variety different models of the same data.\n",
    "The data will be Gaussian, or Normal, data,\n",
    "and the models will have different priors about the parameters of that data.\n",
    "\n",
    "The first model will be a bootstrapping-based model, of the type used in data8.\n",
    "The remaining models will be `pyMC` models.\n",
    "They are defined below, using the function `define_normal_model`.\n",
    "We will compare and contrast the bootstrapping-based model to the `pyMC` models,\n",
    "in terms of their prior distributions, their posterior distributions, and the inferences drawn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the data with a small utility, `util.generate_normal_data`.\n",
    "The true mean and standard deviation of the data are set by the variables `true_mu` and `true_sigma`.\n",
    "The resulting sample will almost surely have a different mean and standard deviation.\n",
    "For convenience, the `seed` argument to this utility sets the state of Python's random number generator\n",
    "and ensures that the data generated will be the same each time it is run.\n",
    "\n",
    "In a real scenario, we wouldn't know the values of `true_mu` and `true_sigma`.\n",
    "We make models to try to represent and quantify our uncertainty about what those values might be.\n",
    "Working with a case where we know what the real answer is, even if it's a bit artificial, helps us gain a better understanding of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu = 2.\n",
    "true_sigma = 1.\n",
    "\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "Teacher Note: the values for the bounds on a number of the auto-graded questions below\n",
    "were estimated using simple Monte Carlo methods based on the parameters above.\n",
    "If these parameters are adjusted, the bounds will need to be adjusted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util.generate_normal_data(true_mu, true_sigma, N, seed=seed.SHARED_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, before we start modeling data, even with bootstrapping,\n",
    "we should calculate descriptive statistics and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0].mean(), df[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've done previously, we can use bootstrap sampling to estimate our uncertainty about the values of parameters.\n",
    "\n",
    "In the cells below, run bootstrapping on the data to estimate the uncertainty in the value of the true mean.\n",
    "First, produce a `Series` of bootstrap sample means, called `boot_mus`,\n",
    "and then produce a histogram of the bootstrapped values with `distplot`.\n",
    "\n",
    "Don't be afraid to reuse plotting code from previous labs/homeworks.\n",
    "\n",
    "The first cell produces a list of length `num_boots` of pandas `Series`, each of which is a bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boots = 1000\n",
    "\n",
    "boots = [df[0].sample(frac=1, replace=True) for _ in range(num_boots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "boot_mus = pd.Series([boot.mean() for boot in boots])\n",
    "\n",
    "compare_inferential_distributions([boot_mus],\n",
    "                                  [\"Bootstrapping Model\"],\n",
    "                                  true_value=true_mu, xlabel=r\"$\\mu$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.grade(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Normal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models will all have a graph as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = util.make_normal_model_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the random variable x, which we observe $N$ times,\n",
    "is parameterized by a random variable we don't observe, $\\mu$,\n",
    "which is presumed to have the same, unknown value for each x.\n",
    "\n",
    "This variable sets the \"true\" mean of our data.\n",
    "\n",
    "The function `define_normal_model` creates a model of this kind,\n",
    "where $\\mu$ is a Normal random variable whose mean and standard deviation are set by\n",
    "the first and second arguments, `mu_prior_mean` and `mu_prior_sd`.\n",
    "\n",
    "The random variable $\\mu$ represents our state of belief about what the mean of our data might be,\n",
    "before we have observed any data.\n",
    "On average, this variable will have the value `mu_prior_mean` and samples will be spread around that value\n",
    "with standard deviation `mu_prior_sd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_normal_model(mu_prior_mean, mu_prior_sd, observed_data, true_sigma=true_sigma):\n",
    "    \"\"\"Defines a model with Normal prior over mu, parameters determined by first two arguments,\n",
    "    a Normal likelihoood with mean mu and standard deviation equal to the optional arg true_sigma,\n",
    "    and observed values equal to the argument observed_data\n",
    "    \"\"\"\n",
    "    with pm.Model() as normal_model:\n",
    "        mu = pm.Normal(\"mu\", mu=mu_prior_mean, sd=mu_prior_sd)\n",
    "        \n",
    "        pm.Normal(\"xs\", mu=mu, sd=true_sigma, observed=observed_data)\n",
    "    return normal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three models: `agnostic_model`, `right_model`, and `wrong_model`.\n",
    "\n",
    "Agnostic means, loosely, [\"knowing nothing\"](https://en.wiktionary.org/wiki/agnostic).\n",
    "This model makes much weaker assumptions about what the value of $\\mu$ probably is.\n",
    "This is the model we have when we know little about our data, aside from the form of the model.\n",
    "\n",
    "The `right_model`, on the other hand, makes a much stronger assumption on what the value of $\\mu$ is,\n",
    "and it has the right guess.\n",
    "This is the kind of model we have when we are well-informed about our data.\n",
    "\n",
    "The `wrong_model` makes just as strong an assumption as the right model does,\n",
    "but its assumption is wrong. It expresses the belief that the mean of the data is within ±0.5 of 0, but it is not.\n",
    "This is the kind of model we have when we are mis-informed about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnostic_model_prior_mu = wrong_model_prior_mu = 0\n",
    "agnostic_model_prior_sd = 10\n",
    "wrong_model_prior_sd = right_model_prior_sd = 0.25\n",
    "\n",
    "agnostic_model = define_normal_model(agnostic_model_prior_mu, agnostic_model_prior_sd, df)\n",
    "\n",
    "wrong_model = define_normal_model(wrong_model_prior_mu, wrong_model_prior_sd, df)\n",
    "\n",
    "right_model = define_normal_model(true_mu, right_model_prior_sd, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our priors on $\\mu$ for each model. Remember that the prior is what we believe about our data before looking at the particular values we observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a number of samples equal to `num_prior_samples` of `mu` from the prior using `sample_prior_predictive` for each of the three `pyMC` models.\n",
    "\n",
    "Name the two series of samples `agnostic_prior_mus` and `wrong_prior_mus`.\n",
    "The cell below shows how to do this for the `right_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prior_samples = 10000\n",
    "\n",
    "right_prior = pm.sample_prior_predictive(samples=num_prior_samples,\n",
    "                                         model=right_model)\n",
    "right_prior_mus = pd.Series(right_prior[\"mu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "agnostic_prior = pm.sample_prior_predictive(samples=num_prior_samples,\n",
    "                                            model=agnostic_model)\n",
    "\n",
    "wrong_prior = pm.sample_prior_predictive(samples=num_prior_samples,\n",
    "                                         model=wrong_model)\n",
    "\n",
    "agnostic_prior_mus = pd.Series(agnostic_prior[\"mu\"])\n",
    "wrong_prior_mus = pd.Series(wrong_prior[\"mu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.grade(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram for the samples of `mu` from each model and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "compare_inferential_distributions([right_prior_mus, wrong_prior_mus],\n",
    "                                  labels=[\"Right Model\", \"Wrong Model\"],\n",
    "                                  true_value=true_mu, xlabel=r\"$\\mu$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "compare_inferential_distributions([agnostic_prior_mus], labels=[\"Agnostic Model\"],\n",
    "                                  true_value=true_mu, xlabel=r\"$\\mu$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Why is it fair to say that the agnostic model makes weaker assumptions about the value of $\\mu$ than do the other models?  What parameter choice, in the definition of the model, is responsible for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "The samples from the agnostic model's prior cover a wider range of values for $\\mu$:\n",
    "samples with values between -30 and +30 are fairly common.\n",
    "This is a much broader range of values than the other models produce.\n",
    "\n",
    "This width is determined by `mu_prior_sd`,\n",
    "the standard deviation of the Normal random variable that represents our state of belief about $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Above, it was claimed that the wrong model encodes the belief that $\\mu$ is within about ±0.5 of 0. How would you determine that from the histogram? In these terms, what belief does the right model represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "This fact can be read off by looking at what values appear in the samples from the prior:\n",
    "sampled values of $\\mu$ outside the range of 0 ± 0.5 are rare for the wrong model.\n",
    "\n",
    "The right model represents the belief that the value of $\\mu$ is within ±0.5 of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Why don't we plot a prior for our bootstrapping-based model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "Our bootstrapping model does not have a prior! Bootstrapping can only represent the information contained in the data, not anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Posteriors of the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior represents what we believe about our data after we've looked at the values we've observed.\n",
    "\n",
    "First, let's look at what each of our models believes about the true mean of the variable $X$\n",
    "after having observed the data.\n",
    "Plot histograms of the samples of $\\mu$ from each of our models\n",
    "(the three `pyMC` models and the bootstrapping model)\n",
    "and then answer the questions below.\n",
    "Name the samples from the models `agnostic_samples` and `wrong_samples`.\n",
    "\n",
    "Remember that, for unobserved variables in `pyMC` models, like $\\mu$,\n",
    "we sample from the posterior by running `pm.sample`.\n",
    "Make sure to keep around the `trace`, which is returned by `pm.sample`, since we'll need that to look at our posterior predictions for future values of the data, the `posterior_predictive`.\n",
    "This is done by the `get_trace_and_samples` function.\n",
    "\n",
    "For unobserved variables in a bootstrapping model, we need to estimate their values on all of the bootstrapped samples. You've already done this above, in the computation of `boot_mus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_and_samples(model):\n",
    "    with model:\n",
    "        trace = pm.sample(draws=1000, n_chains=4)\n",
    "        samples = shared_util.samples_to_dataframe(trace)\n",
    "    return trace, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_trace, right_samples = get_trace_and_samples(right_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "agnostic_trace, agnostic_samples = get_trace_and_samples(agnostic_model)\n",
    "\n",
    "wrong_trace, wrong_samples = get_trace_and_samples(wrong_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.grade(\"q3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "compare_inferential_distributions([wrong_samples.mu, right_samples.mu, agnostic_samples.mu],\n",
    "                                  [\"Wrong Model\", \"Right Model\", \"Agnostic Model\"],\n",
    "                                  true_value=true_mu, xlabel=r\"$\\mu$\")\n",
    "plt.gca().set_xlim([0, 3.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "compare_inferential_distributions([agnostic_samples.mu, boot_mus],\n",
    "                                  [\"Agnostic Model\", \"Bootstrapping Model\"],\n",
    "                                  true_value=true_mu, xlabel=r\"$\\mu$\")\n",
    "plt.gca().set_xlim([0, 3.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q For the `pyMC` models, how have the beliefs about $\\mu$ changed? That is, compare the posteriors over $\\mu$ to the priors. Do they differ more for some of the models than for others? If so, which one(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "For the right and wrong models, the prior and posterior look about the same,\n",
    "though the posterior for the wrong model might have scooted upwards a bit.\n",
    "Intuitively, these models were very sure in their beliefs, and 10 data points weren't enough to change them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Can you give an intuitive explanation, in your own words, for why each model's beliefs changed the way they did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N \n",
    "\n",
    "The agnostic model's posterior is very different from its prior:\n",
    "instead of being broadly distributed across values from -30 to +30,\n",
    "it's distributed across values between about 1 and 3.\n",
    "Intuitively, this model had weakly-held beliefs about the values of the parameter,\n",
    "and 10 data points were sufficient to change them dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Directly compare the posterior for the agnostic model to that for the bootstrapping model. If they are different, why are they different? If they are similar, why are they similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "Usually, the agnostic model and the bootstrapping model have fairly similar posteriors,\n",
    "and they should for the dataset that was drawn.\n",
    "\n",
    "They are similar because the bootstrapping model makes no assumptions about the likely values of $\\mu$,\n",
    "while the agnostic model only makes weak assumptions about the likely values of $\\mu$.\n",
    "\n",
    "However, they can be different, depending on the data sample,\n",
    "which might happen if a student changes around the data sampling code.\n",
    "In that case, a student might answer that they are different because\n",
    "the bootstrap model has no prior, and so is like a model with a `Flat` distribution for `mu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What do you suspect would happen to the posterior of the wrong and agnostic models if the number of samples was 100 or 1000 instead of just 10? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "They would move towards the true value of $\\mu$ and become less wide.\n",
    "As the number of samples increases, the posterior for any model should converge to the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Posterior of the Observed Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we know the correct value of $\\mu$, since we're simulating the entire process.\n",
    "In a real experiment, we wouldn't know the true value, so we wouldn't know which model to trust.\n",
    "That is, in our simulation, we know which model is the right model and which model is the wrong model,\n",
    "but we wouldn't know that in most real life situations.\n",
    "\n",
    "So how do we determine which model to believe?\n",
    "\n",
    "The first thing to check is whether the model's posterior over the observed variable matches\n",
    "the data we observed. If it doesn't, then the model doesn't consider the data we observed very likely.\n",
    "And while it's possible that the data we drew was unrepresentative, the larger a sample, the less likely that is,\n",
    "and so the more evidence we have that the model is incorrect.\n",
    "\n",
    "Because it can be used to predict future values of the observed variable, based on the values we've already observed, this distribution is known as the \"posterior predictive\" distribution.\n",
    "\n",
    "The quantitative version of this approach to model comparison is called _maximum likelihood modeling_,\n",
    "and it's especially common in cases where an uninformative prior is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below will generate samples from the posterior over the observed variable x for the right model using `sample_posterior_predictive`. Note that it requires a copy of a `trace` from the same model.\n",
    "\n",
    "The cells beneath that will compare the posterior predictive to the observed data using a histogram and rugplot.\n",
    "They will run as written, producing only a plot of the posterior samples from the `right_model`.\n",
    "\n",
    "Use `sample_posterior_predictive` to draw samples according to the posteriors of the wrong and agnostic models.\n",
    "Then, add the results to the `postpreds` list, as in the example code below (order is important!),\n",
    "and then run the cell containing `compare_multiple_postpreds_to_observed`\n",
    "to plot the comparisons.\n",
    "Then answer the questions below.\n",
    "\n",
    "Example Code:\n",
    "```python\n",
    "wrong_postpred = pm.sample_posterior_predictive(?)\n",
    "agnostic_postpred = pm.sample_posterior_predictive(?)\n",
    "postpreds = [right_postpred[\"xs\"], agnostic_postpred[\"xs\"], wrong_postpred[\"xs\"]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posterior_samples = 1000\n",
    "\n",
    "right_postpred = pm.sample_posterior_predictive(\n",
    "    right_trace, samples=num_posterior_samples, model=right_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "wrong_postpred = pm.sample_posterior_predictive(\n",
    "    wrong_trace, samples=num_posterior_samples, model=wrong_model)\n",
    "\n",
    "agnostic_postpred = pm.sample_posterior_predictive(\n",
    "    agnostic_trace, samples=num_posterior_samples, model=agnostic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postpreds = [right_postpred[\"xs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "postpreds = [right_postpred[\"xs\"], agnostic_postpred[\"xs\"], wrong_postpred[\"xs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = util.compare_multiple_postpreds_to_observed(\n",
    "    postpreds, df, titles=[\"Right Model\", \"Agnostic Model\", \"Wrong Model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q How well does the posterior predictive distribution match the data for each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "The predictions of the right and agnostic model match the data very closely, while the predictions of the wrong model are way off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Which model is doing worst according to this criterion? How do its predictions differ from the observed data? Why might this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "The wrong model is doing the worst. Its predictions differ from the data by underestimating the mean, since its posterior over the mean is down at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits to Sampling from the Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of picking models by how well their samples match the data has its flaws.\n",
    "For example, the bootstrap always come out looking better than any other model possibly can.\n",
    "Let's see why this is.\n",
    "\n",
    "The observed values for 100 samples from the posterior predictive distribution of each model will be plotted below.\n",
    "\n",
    "Note that the \"bootstrapping\" is the same as \"sampling from the posterior predictive\" for a bootstrap-based model.\n",
    "Sample values will appear as black tick marks, with the observed values underneath in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boots_np = np.asarray([np.asarray(boot) for boot in boots])\n",
    "\n",
    "f, axs = util.elementwise_compare_multiple_postpreds_to_observed(\n",
    "    [boots_np] + postpreds, df,\n",
    "    titles=[\"Bootstraps\", \"Right Model\", \"Agnostic Model\", \"Wrong Model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain, in your own words, why model comparison on the basis of posterior predictive similarity to the data would always pick the bootstrap model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "The bootstrap model will only ever produce values from the dataset, so its samples will always perfectly match the data. Other models will typically produce values not from the dataset, so their samples won't perfectly match the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison by Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantitatively characterize which models are best supported by the data,\n",
    "we need to consider not just how well the models predict the data,\n",
    "but also how well the prior characterizes the data.\n",
    "\n",
    "The full details of how this is done are outside of the scope of this class, since\n",
    "\n",
    "1) comparing models via sampling is an area of active development in `pyMC` and in research\n",
    "\n",
    "2) existing methods tend to use ideas outside of the core concepts of this class.\n",
    "\n",
    "The curious can look up [Bayes factors](https://en.wikipedia.org/wiki/Bayes_factor).\n",
    "\n",
    "But in short, we can perform a form of model comparison by computing a statistic called\n",
    "the average [surprise](https://charlesfrye.github.io/stats/2016/03/29/info-theory-surprise-entropy.html),\n",
    "or the negative log-likelihood, on samples from the prior.\n",
    "This number will be high if the choice of parameter makes the data unlikely\n",
    "and low if the choice of parameter makes the data likely.\n",
    "A model with lower surprise, averaged over samples from the prior, is better.\n",
    "This method is known as _marginal likelihood comparison_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surprise(model, prior):\n",
    "    model_logp = model.observed_RVs[0].logp\n",
    "    surprises = [-model_logp(mu=mu) for mu in prior[\"mu\"]]\n",
    "    return np.mean(surprises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_surprise = calculate_surprise(right_model, right_prior)\n",
    "right_surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done above for the `right_model`,\n",
    "calculate and the surprise for the agnostic and wrong models,\n",
    "name them `agnostic_surprise` and `wrong_surprise`,\n",
    "and use their values to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "agnostic_surprise = calculate_surprise(agnostic_model, agnostic_prior)\n",
    "agnostic_surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "\n",
    "wrong_surprise = calculate_surprise(wrong_model, wrong_prior)\n",
    "wrong_surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.grade(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q According to this \"surprise\" criterion, which model is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "The right model performs best here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q How does the agnostic model perform? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL'N\n",
    "\n",
    "The agnostic model performs poorly. Many of the samples from the prior will have means very far away from the true value (-30 to +30), so the data will look very unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hint for why: what do samples from the agnostic prior look like? Try plotting the distribution of_`agnostic_prior[\"xs\"].flatten()` _compared to the observed data (with_ `sns.rugplot`_)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SOL'N\n",
    "compare_inferential_distributions([agnostic_prior[\"xs\"].flatten()], labels=[\"agnostic\"], xlabel=\"x\");\n",
    "sns.rugplot(df, ax=plt.gca());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
