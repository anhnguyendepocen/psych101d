{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Parameters, Priors, and Posteriors 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import daft\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import utils.daft\n",
    "import utils.plot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if we don't know the value of the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often, our models are incomplete: the parameters of some random variables are, themselves, random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For example, we considered a model of the process of measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"S\", \"M\", observed=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We looked at the joint distribution of both the signal and the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as measurement_model:\n",
    "    signal = pm.Normal(\"signal\", mu=0, sd=1)\n",
    "    measurement = pm.Normal(\"measurement\", mu=signal, sd=1)\n",
    "    \n",
    "measurement_samples = shared_util.samples_to_dataframe(shared_util.sample_from(\n",
    "    measurement_model, draws=10000, progressbar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(measurement_samples.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Once we have observed the measurement, we want to know likely values of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"S\", \"M\", observed=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The signal is what we _really_ want to know --\n",
    "the sound of someone's voice speaking,\n",
    "an episode of _Game of Thrones_,\n",
    "the location of another car on the highway.\n",
    "\n",
    "But the measurement --\n",
    "the output of a radio,\n",
    "the pixel values on our TV screen,\n",
    "the voltages in our LIDAR detector --\n",
    "is what we _actually get_, and so we must **work backwards**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p(S \\lvert M = m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### According to one view, this is also the problem of perception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Working backwards_\n",
    "is precisely the problem faced by the mind when it tries to turn sensation into perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, the eyes detect the fluctuations of electromagnetic fields (a measurement),\n",
    "but they are intended to do things like identify the presence or absence of predators (a signal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hence the idea that the brain solves this problem using the tools of probability and statistics\n",
    "that we learn in this class, which dates back\n",
    "to [the 19th century](https://en.wikipedia.org/wiki/Unconscious_inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Normally, this proceeds automatically, and we don't notice,\n",
    "but sometimes we can catch it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"img/necker_cube.png\", width=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "By <a href=\"//commons.wikimedia.org/w/index.php?title=User:BenFrantzDale&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:BenFrantzDale (page does not exist)\">BenFrantzDale</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"http://creativecommons.org/licenses/by-sa/3.0/\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=2040007\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Having observed this image, a two-dimensional pattern of dark and bright pixels,\n",
    "the mind seeks to determine what causes or factors in the world\n",
    "might have given rise to it.\n",
    "\n",
    "Two 3-dimensional objects could have given rise to this image:\n",
    "a cube whose front face points down and to the left or up and to the right.\n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Necker_cube) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"Cube\", \"Image\", scale=3.7, observed=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This idea is sometimes called the\n",
    "[Bayesian model of perception](https://www.sciencedirect.com/science/article/pii/S0022249615000061).\n",
    "An even more recent view has it that the brain performs\n",
    "[MCMC sampling](http://www.cnbc.cmu.edu/~tai/papers/lee_mumford_josa.pdf),\n",
    "just like pyMC!\n",
    "One piece of evidence is the fact that\n",
    "your perceptions switch back and forth,\n",
    "[as though your mind were \"drawing samples\"](https://link.springer.com/article/10.1186/1471-2202-12-S1-P320).\n",
    "Bayesian inference with MCMC has also been considered as a model for\n",
    "[Charles Bonnet Syndrome](https://papers.nips.cc/paper/4097-hallucinations-in-charles-bonnet-syndrome-induced-by-homeostasis-a-deep-boltzmann-machine-model),\n",
    "where individuals who go blind late in life have complex visual hallucinations.\n",
    "\n",
    "Of course, the brain has to deal with models much more complicated\n",
    "than just two one-dimensional normal distributions or the cube example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Once we have observed the measurement, we want to know likely values of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"S\", \"M\", observed=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this is different from the case of observed or known values of the _parameters_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_parameters_graph(observed=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, we could just set the values of the parameters to be equal to the known or observed values:\n",
    "```python\n",
    "X = pm.Foo(\"X\", beta_0=beta_0_known, beta_1=beta_1_known, beta_2=beta_2_known)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This strategy won't work for the model of signals and measurements,\n",
    "because the measurement is not a parameter for the signal.\n",
    "\n",
    "Remember that we typically draw our arrows coming _from_ a variable that is used to determine\n",
    "the values of another variable.\n",
    "These represent the \"natural order\" for thinking about our model.\n",
    "\n",
    "Often, this comes from a loosely mechanistic model of our data:\n",
    "here, that imprecise mechanism is the argument from the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Incorporating observations into models is done by the `observed` keyword argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Presume we have taken a measurement and observed the value `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "observations = 1\n",
    "# N = 10  # uncomment for multiple observations\n",
    "# observations = pm.Normal.dist(mu=1, sd=1).random(size=N)  # uncomment for multiple observations\n",
    "\n",
    "with pm.Model() as measurement_observed:\n",
    "    signal = pm.Normal(\"signal\", mu=0, sd=1)\n",
    "    measurement = pm.Normal(\"measurement\", mu=signal, sd=1, observed=observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we add that information to our model\n",
    "by passing `observed=1` to the `measurement` variable,\n",
    "then calling `pm.sample` will draw from the conditional distribution\n",
    "$p(S \\lvert M = 1)$ instead of the marginal distribution $p(S)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Put another way:\n",
    "the original model specified our uncertainty about the values of the signal and the measurement,\n",
    "and samples from it were used to numerically and visually represent that uncertainty.\n",
    "\n",
    "Once we've observed the value of one of the random variables,\n",
    "the state of our knowledge changes,\n",
    "and so we'd like the samples to change to reflect that.\n",
    "\n",
    "pyMC does this for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More generally, if we have observed $K$ random variables in our model,\n",
    "`pm.sample` will produce values from the conditional distribution of the $J$ remaining variables:\n",
    "\n",
    "$$\n",
    "p(\\beta_1, \\beta_2, \\dots \\beta_J \\lvert X_1=x_1, X_2=x_2 \\dots X_K=x_K)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "measurement_observed_samples = shared_util.samples_to_dataframe(shared_util.sample_from(\n",
    "    measurement_observed, draws=500, chains=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(measurement_observed_samples.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that the `measurement` doesn't show up among our samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.distplot(measurement_samples[\"signal\"], ax=ax, label=\"before obs\")  # notice: from model _without_ observed\n",
    "sns.distplot(measurement_observed_samples[\"signal\"], ax=ax, label=\"after obs\", color=\"C2\");\n",
    "ax.set_xlim([-4, 4]); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Change `observations` to something else, like `5` or `-2`, and then re-run the model definition, sampling, and plotting code.\n",
    "\n",
    "You'll see the `after obs` distribution move around:\n",
    "for values larger than `1`, it will move further to the right,\n",
    "for values less than  `1`, it will move to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This example is simple to the point of perhaps being uninteresting.\n",
    "But if you apply this principle to images,\n",
    "and treat the pixels as dependent, Normally-distributed signals,\n",
    "you are en route to discovering\n",
    "[JPEG](http://nautil.us/blog/the-math-trick-behind-mp3s-jpegs-and-homer-simpsons-face)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Latin, \"from before\" and \"from after\" are _a priori_ and _a posteriori_,\n",
    "and so these distributions are typically called the **prior** and the **posterior** on $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.distplot(measurement_samples[\"signal\"], ax=ax, label=r\"Prior: $p(S)$\")  # notice: from model _without_ observed\n",
    "sns.distplot(measurement_observed_samples[\"signal\"], ax=ax, label=\"\"\"Posterior:\\n$p(S \\\\vert M=m)$\"\"\", color=\"C2\");\n",
    "ax.set_xlim([-4, 4]); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is the _killer feature_ of pyMC and other MCMC libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You write down a _forwards model_\n",
    "that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. describes <font color=\"#003262\"> **uncertainty about unknown quantities** </font> like parameters,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is the <font color=\"#003262\">**prior**</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and then\n",
    "\n",
    "2. explains what <font color=\"#FDB515\">**the distribution of the data _would be_**</font>,\n",
    "if you did know those unknown quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is the <font color=\"#FDB515\">**likelihood**</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "pyMC can then \"work backwards\" and tell you\n",
    "\n",
    "3. what <font color=\"#2ca02c\"> **your new beliefs\n",
    "about the unknown quantities are**</font>,\n",
    "once you've seen that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is the <font color=\"#2ca02c\">**posterior**</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These new beliefs are expressed as _samples_, drawn by `pm.sample`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sampling from Models with `observed` variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "with pm.Model() as model:\n",
    "    # random variable to model uncertainty about the truth before seeing data: the _prior_\n",
    "    parameter_var = pm.SomeRandomVariable(\n",
    "        \"prior_variable\", parameter=value)  \n",
    "    # random variable to model uncertainty in data, given parameters: the _likelihood_\n",
    "    observed_var = pm.OtherRandomVariable(\n",
    "        \"data_variable\", parameter=parameter_var,\n",
    "        observed=data_we_saw)\n",
    "    \n",
    "# samples to estimate uncertainty about the truth after seeing data: the _posterior_\n",
    "samples = shared_util.sample_from(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In contrast, frequentist model fitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One only writes down a forwards model for going from parameters to data:\n",
    "there is only the <font color=\"#FDB505\">**likelihood**</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is no mention of uncertainty in parameters.\n",
    "After you observe data, you search for parameters that make the observed data most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Your answer is then a single number for each parameter,\n",
    "rather than a collection of samples or,\n",
    "more generally, a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is called _maximum likelihood estimation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the previous lecture, when I picked the \"best\" binomial fits for the conditional distributions of $S$,\n",
    "I used maximum likelihood methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The _working backwards_ is done via Bayes' Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(r\"$\\beta$\", \"X\", observed=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider the above model, where\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Foo}(\\beta) \\\\\n",
    "\\beta \\sim \\text{Bar}(\\lambda)\n",
    "$$\n",
    "\n",
    "meaning that $\\beta$ is the parameter for the distribution of $X$\n",
    "and we know the distribution $p(\\beta)$, which has fixed parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "p(X, \\beta) = \\color{green}{p(\\beta\\lvert X)} * p(X) = \\color{darkgoldenrod}{p(X\\lvert\\beta)} * \\color{darkblue}{p(\\beta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The two expressions in the center and right are the two\n",
    "equivalent expressions for the joint distribution of $X, \\beta$\n",
    "(the leftmost expression).\n",
    "\n",
    "The colored components are known as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p style=\"text-align: center;\">\n",
    "the $\\color{green}{\\text{posterior on } \\beta}$,\n",
    "the $\\color{darkgoldenrod}{\\text{likelihood}}$,\n",
    "and the $\\color{darkblue}{\\text{prior on } \\beta}$,\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The likelihood and the prior are part of our model:\n",
    "they are $\\text{Foo}$ and $\\text{Bar}$.\n",
    "\n",
    "But the posterior is what we really want:\n",
    "it tells us what we believe about $\\beta$,\n",
    "once we know something about $X$.\n",
    "\n",
    "So we rearrange to get the posterior alone:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\color{green}{p(\\beta\\lvert X)} = \\color{darkgoldenrod}{p(X\\lvert\\beta)} * \\color{darkblue}{p(\\beta)}\\ /\\ p(X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This equation is known as\n",
    "[Bayes' Rule](https://charlesfrye.github.io/stats/2016/02/04/bayes-rule.html).\n",
    "\n",
    "And it looks like the value $p(X)$ is the last piece we need:\n",
    "it's not specified in our model, but it's showing up in this equation.\n",
    "\n",
    "In statistical physics, it is known as the\n",
    "[partition function](https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)),\n",
    "in mathematical statistics, it is a logarithm away from the\n",
    "[cumulant-generating function](https://en.wikipedia.org/wiki/Cumulant).\n",
    "In these disciplines,\n",
    "it is a holy grail to be quested after.\n",
    "Computing it is, for large problems,\n",
    "insanely difficult, except in special cases.\n",
    "This problem is \"definitely hard\" in the sense that,\n",
    "if it were generically possible to solve easily,\n",
    "many other hard problems would turn out to be easy as well.\n",
    "\n",
    "Side note: if you've heard of NP problems,\n",
    "([simple explanation](https://simple.wikipedia.org/wiki/P_versus_NP),\n",
    "[less simple explanation](https://stackoverflow.com/questions/1857244/what-are-the-differences-between-np-np-complete-and-np-hard)),\n",
    "perhaps from the show\n",
    "[_Numb3rs_](https://en.wikipedia.org/wiki/Uncertainty_Principle_(Numbers)#Writing),\n",
    "know that this comes from a class of harder problems,\n",
    "[#P](https://en.wikipedia.org/wiki/%E2%99%AFP).\n",
    "\n",
    "And so it is something like the\n",
    "the greatest book that could ever be written in 410 pages,\n",
    "that is lost inside an almost-infinite library,\n",
    "in a short story by Jorge Luis Borges,\n",
    "[_La Biblioteca de Babel_](https://en.wikipedia.org/wiki/The_Library_of_Babel):\n",
    "we know it exists, is one a finite number of objects,\n",
    "and has all the answers we seek,\n",
    "but we have no hope of finding it.\n",
    "\n",
    "One of the reasons for the success of MCMC methods,\n",
    "like those used in pyMC,\n",
    "is that they can avoid the requirement to know, compute, or estimate $p(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\color{green}{p(\\beta\\lvert X)} \\propto \\color{darkgoldenrod}{p(X\\lvert\\beta)} * \\color{darkblue}{p(\\beta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We won't go into detail here about how this is achieved,\n",
    "but the \"price\" is that though we had mathematical forms for the distributions\n",
    "$p(X\\vert\\beta)$ and $p(\\beta)$,\n",
    "we don't have a mathematical form for the distribution\n",
    "$p(\\beta\\vert X)$.\n",
    "Instead, we only have samples from that distribution,\n",
    "which we can use to estimate or approximate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That is why this class\n",
    "emphasizes the view of distributions as \"what our samples approximate\"\n",
    "rather than the view of distributions as \"mathemetical formulas that tell us probabilities\".\n",
    "\n",
    "We will always be able to draw samples,\n",
    "but we will not always have formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many reasons why frequentist modeling won out for the first century or so of statistical modeling\n",
    "including the fact that, for a long time, calculating $p(X)$ was a major bottleneck for all except certain special models, just as computing the form of the sampling distribution was a bottleneck for frequentist statistics.\n",
    "\n",
    "The advent of powerful computers and easy-to-use MCMC libraries changed that,\n",
    "and so is changing the practice of statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But another reason was the intense hostility of one of the founding fathers of statistics,\n",
    "Ronald A. Fisher, to what was then called _inverse probability_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> This is not the place to enter into the subtleties of a prolonged controversy;\n",
    "it will be sufficient in this general outline of the scope of Statistical Science\n",
    "to reaffirm my personal conviction, [which I have sustained elsewhere](https://projecteuclid.org/download/pdf_1/euclid.ba/1340370565),\n",
    "that **the theory of inverse probability is founded upon an error,\n",
    "and must be wholly rejected**.\n",
    "\n",
    "- R. A. Fisher, _Statistical Methods for Research Workers_, 1925."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This \"error\" was, essentially, to define probability subjectively, in terms of beliefs,\n",
    "instead of objectively, in terms of frequencies and populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When we want to sample from other distributions, we use other pyMC methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "with pm.Model() as model:\n",
    "    # random variable to model uncertainty about the truth before seeing data: the _prior_\n",
    "    parameter_var = pm.SomeRandomVariable(\"prior_variable\", parameter=value)  \n",
    "    # random variable to model uncertainty in data, given parameters: the _likelihood_\n",
    "    observed_var = pm.OtherRandomVariable(\"data_variable\", paramter=parameter_var, observed=data_we_saw)\n",
    "    \n",
    "# samples to estimate uncertainty before seeing data\n",
    "pripred_samples = pm.sample_prior_predictive(model=model)\n",
    "\n",
    "with model:\n",
    "    # samples to estimate uncertainty about the parameters after seeing data: the _posterior_\n",
    "    post_samples_trace = pm.sample()\n",
    "\n",
    "# samples to estimate uncertainty in what future data we might see:\n",
    "postpred_samples = pm.sample_posterior_predictive(post_samples_trace, model=model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The distribution we place on the parameter is called a _prior_: it represents our beliefs before, or prior to, observing any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The random variables representing our parameters will themselves have parameters.\n",
    "It is common to call these _hyperparameters_, when it is important to be clear.\n",
    "\n",
    "If I am uncertain about the parameters of my parameters,\n",
    "I might replace them with random variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "with pm.Model() as deep_model:\n",
    "    # random variable to model uncertainty about the hyperparameter\n",
    "    hyperparameter_var = pm.SomeOtherRandomVariable(\"hyperprior_variable\", parameter=value)\n",
    "    # random variable to model uncertainty about the parameter\n",
    "    # before seeing data, given hyperparameter: the _prior_\n",
    "    parameter_var = pm.SomeRandomVariable(\"prior_variable\", parameter=hyperparameter_var)  \n",
    "    # random variable to model uncertainty in data, given parameters: the _likelihood_\n",
    "    observed_var = pm.OtherRandomVariable(\"data_variable\", paramter=parameter_var, observed=data_we_saw)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Somewhat less commonly, the distribution of this random variable is called a _hyperprior_.\n",
    "\n",
    "Note the possibility of infinite regress:\n",
    "the hyperprior will have parameters,\n",
    "about which we might be uncertain,\n",
    "and so use random variables,\n",
    "which themselves will have parameters,\n",
    "about which we might be uncertain,\n",
    "and so on.\n",
    "\n",
    "At a certain point,\n",
    "you must stop and say that there is at least one distribution you're willing to assume, _a priori_.\n",
    "Compare this to the\n",
    "[Münchhausen trilemma](https://en.wikipedia.org/wiki/M%C3%BCnchhausen_trilemma),\n",
    "the famous problem in epistemology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Priors are broader than just the distributions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, every time we called `pm.sample` on a model with no `observed` variables,\n",
    "the samples were drawn according to our prior over the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general: a prior expresses what we know about a quantity we don't know the exact value of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $2^{2^{100}}$th digit of $\\pi$ is equally likely to be odd or even\n",
    "- I will die at [age 76, ±16 years](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3285408/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Which of these do you believe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A news program announces that \"a study has shown\" that smoking does not increase risk of lung cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A news program announces that \"a study has shown\" that people\n",
    "[do not read instruction manuals](https://academic.oup.com/iwc/article/28/1/27/2363584)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A news program announces that \"a study has shown\" that vaping leads to [lipoid pneumonia](https://www.healthline.com/health/lipoid-pneumonia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are some priors you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Priors prevent us from being at the mercy of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bootstrapping avoided the need to specify a prior.\n",
    "\n",
    "With bootstrapping, we could estimate a posterior\n",
    "if we could frame our inference problem in terms of a statistic.\n",
    "\n",
    "Frequentist techniques in general avoid priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But this flexibility comes at a price:\n",
    "- for some problems, there is no statistic of the data that supports our inference\n",
    "- if we just look at likelihoods, we can end up making very silly inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What we do in the shadows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You awaken in your bed to find your room dark.\n",
    "\n",
    "There are two explanations for this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. You have awoken in the night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. The sun has gone out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"Sun\\nGone\", \"Room\\nDark\", scale=3.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Specifying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### <font color=\"#003262\"> Prior </font>\n",
    "\n",
    "We put 100:1 odds against the sun having gone out.\n",
    "```python\n",
    "sun_gone_out = pm.Categorical(\"sun_gone\", p=[100, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### <font color=\"#FDB515\"> Likelihood </font>\n",
    "\n",
    "If the sun is gone, then the room will be dark:\n",
    "```\n",
    "pm.Categorical(\"room_dark\", p=[0, 1])\n",
    "```\n",
    "\n",
    "If the sun is not gone, then the room may be dark or light, with equal probability.\n",
    "```\n",
    "pm.Categorical(\"room_dark\", p=[0.5, 0.5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as sun_model:\n",
    "    sun_gone_out = pm.Categorical(\"sun_gone\", p=[100, 1])  # our prior beliefs: sun is probably not gone\n",
    "    ps = [[0.5, 0.5], [0, 1]]   # likelihood: if sun is gone, it's definitely dark, otherwise 50/50\n",
    "    is_dark = pm.Categorical(\"room_dark\", p=pm.math.switch(sun_gone_out, ps[1], ps[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sun_samples = shared_util.samples_to_dataframe(shared_util.sample_from(sun_model, draws=10000))\n",
    "N_sun = len(sun_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots();\n",
    "ax.bar([0, 1], sun_samples[\"room_dark\"].value_counts() / N_sun);\n",
    "ax.set_xticks([0, 1.]); ax.set_xticklabels([\"room bright\", \"room dark\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, our prior for the room being dark:\n",
    "both observations have roughly equal prior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots();\n",
    "ax.bar([0, 1], sun_samples[\"sun_gone\"].value_counts() / N_sun);\n",
    "ax.set_xticks([0, 1.]); ax.set_xticklabels([\"sun still here\", \"sun gone\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And our prior for the sun going out:\n",
    "the sun is much more likely to be present than absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "posts_f, ax = plt.subplots(figsize=(8, 8));\n",
    "plt.bar([0, 1], np.log(sun_samples[\"sun_gone\"].value_counts()), 0.25, label=\"before observing\", lw=4, ec=\"k\");\n",
    "plt.ylabel(\"log probabilities\"); plt.legend();\n",
    "ax.set_xticks([0.25, 1.25]); ax.set_xticklabels([\"sun still here\", \"sun gone\"]); plt.tight_layout();\n",
    "ax.set_ylim(0, 25); ax.set_xlim(-0.25, 1.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To make it easier to compare the values, we look at the logarithms of the probabilities (up to a constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"Sun\\nGone\", \"Room\\nDark\", observed=True, scale=3.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, to model how we update our beliefs having observed the brightness level of the room.\n",
    "\n",
    "First, let's assume that we have seen, one time, that the room is dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as sun_model_observed:\n",
    "    sun_gone_out = pm.Categorical(\"sun_gone\", p=[100, 1])\n",
    "    ps = [[0.5, 0.5], [0, 1]]\n",
    "    is_dark = pm.Categorical(\"room_dark\", p=pm.math.switch(sun_gone_out, ps[1], ps[0]), observed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sun_post_samples = shared_util.samples_to_dataframe(shared_util.sample_from(sun_model_observed, draws=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax.bar([0.25, 1.25], np.log(sun_post_samples[\"sun_gone\"].value_counts()),\n",
    "       0.25, label=\"after observing\\ndark\", color=\"C2\", lw=4, ec=\"k\");\n",
    "ax.legend(); plt.tight_layout(); posts_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"Sun\\nGone\", \"Room\\nDark\", observed=True, scale=3.8, plate=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, let's assume that we have observed this same fact $N>1$ times (below, `1000`).\n",
    "\n",
    "It is common to make $N$ independent observations of the same random variable (or variables),\n",
    "while the parameters stay fixed.\n",
    "To prevent the need to draw $N$ copies,\n",
    "we instead put a square around the node (or nodes) we're repeatedly observing.\n",
    "This is called a _plate_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as sun_model_many_observed:\n",
    "    sun_gone_out = pm.Categorical(\"sun_gone\", p=[100, 1])\n",
    "    ps = [[0.5, 0.5], [0, 1]]\n",
    "    is_dark = pm.Categorical(\"room_dark\", p=pm.math.switch(sun_gone_out, ps[1], ps[0]), observed=[1]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sun_many_post_samples = shared_util.samples_to_dataframe(\n",
    "    shared_util.sample_from(sun_model_many_observed, draws=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax.bar([0.5, 1.5], np.log(sun_many_post_samples[\"sun_gone\"].value_counts()),\n",
    "       0.25, label=\"after observing\\ndark 1000 times\", color=\"C4\", lw=4, ec=\"k\");\n",
    "ax.legend(); plt.tight_layout(); posts_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We can sample from priors even in models with `observed` data\n",
    "\n",
    "We just use **`sample_prior_predictive`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prior_samples = shared_util.samples_to_dataframe(pm.sample_prior_predictive(\n",
    "    samples=10000, model=sun_model_observed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The output of `pm.sample_prior_predictive`\n",
    "is still compatible with `shared_util.samples_to_dataframe`,\n",
    "but it's slightly different from the output of `pm.sample`:\n",
    "it is literally a dictionary,\n",
    "rather than being a `MultiTrace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prior_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 8));\n",
    "plt.bar([0, 1], sun_samples[\"room_dark\"].value_counts() / len(sun_samples),\n",
    "        width=0.4, label=\"pm.sample, no observed\");\n",
    "plt.bar([0.4, 1.4], prior_samples[\"room_dark\"].value_counts() / len(prior_samples),\n",
    "        width=0.4, label=\"pm.sample_prior_predictive\"); plt.ylim(0, 1.); plt.legend();\n",
    "ax.set_xticks([0.2, 1.2]); ax.set_xticklabels([\"sun still here\", \"sun gone\"]); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing Your Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Common Priors, and what they imply about your beliefs**\n",
    "\n",
    "`Categorical`: These are my beliefs about each possible value\n",
    "\n",
    "(`Discrete`)`Uniform`: The value is between these two numbers\n",
    "\n",
    "`Normal`: I know the value approximately, up to some spread\n",
    "\n",
    "`LogNormal`: I know the order of magnitude approximately, up to some spread\n",
    "\n",
    "`Cauchy`: I know almost nothing about this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cromwell's Rule: Never Say Never"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When setting priors, take the advice of Oliver Cromwell, Lord Protector of the Commonwealth of England, Scotland, and Ireland, 1653 - 1658."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"img/oliver_cromwell.jpg\", width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Oliver_Cromwell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Is it therefore infallibly agreeable to the Word of God, all that you say?\n",
    "**I beseech you, in the Bowels of Christ, consider the Possibility that you may be mistaken**.\n",
    "\n",
    "-Oliver Cromwell, [Letter to the Kirk of Scotland](http://www.olivercromwell.org/Letters_and_speeches/letters/Letter_129.pdf), August 3rd, 1650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The rule was coined by Bayesian statistician [Dennis Lindley](https://en.wikipedia.org/wiki/Dennis_Lindley), whose quote about it is illuminating for the \"has the sun gone out?\" example above:\n",
    "\n",
    "> \\[L\\]eave a little probability for the moon being made of green cheese; it can be as small as 1 in a million, but have it there since otherwise an army of astronauts returning with samples of the said cheese will leave you unmoved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This also applies to likelihoods:\n",
    "it was probably unwise to say the room would _certainly_ be dark if the sun had gone out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Flatter priors imply less knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "superwide_normal_samples = pd.Series(\n",
    "    pm.Normal.dist(mu=0, sd=10).random(size=10000))  # bigger scale parameter, wider\n",
    "wide_normal_samples = pd.Series(\n",
    "    pm.Normal.dist(mu=0, sd=3).random(size=10000))  # bigger scale parameter, wider\n",
    "narrow_normal_samples = pd.Series(\n",
    "    pm.Normal.dist(mu=0, sd=1).random(size=10000))  # smaller scale parameter, skinnier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.distplot(superwide_normal_samples, label=r\"Very Uncertain Prior\")\n",
    "sns.distplot(wide_normal_samples, label=r\"Intermediate Prior\")\n",
    "sns.distplot(narrow_normal_samples, color=\"C3\", label=r\"More Certain Prior\")\n",
    "plt.xlabel(\"value\"); plt.legend(loc=[0.6, 0.8], framealpha=1); plt.xlim(-10, 10); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In general: parameters that change spread change degree of uncertainty,\n",
    "while parameters that change location change estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For variables with infinitely-many possibilities, improper priors are the flattest priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "They make the most limited assumptions about the value of the variable:\n",
    "they only assume its domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Improper Priors**\n",
    "\n",
    "`Flat`: I know nothing about this variable, except it is a number (improper!)\n",
    "\n",
    "`HalfFlat`: I know nothing about this variable, except that it's positive (improper!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This allows us to recapitulate the results of frequentist procedures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "observed_vals = pd.Series(pm.Normal.dist(mu=0, sd=1).random(size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as improper_model:\n",
    "    mu = pm.Flat(\"mu\"); sigma = pm.HalfFlat(\"sigma\")\n",
    "    X = pm.Normal(\"X\", mu=mu, sd=sigma, observed=observed_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "improper_trace = shared_util.sample_from(improper_model, target_accept=0.9)\n",
    "improper_samples = shared_util.samples_to_dataframe(improper_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.distplot(improper_samples[\"mu\"], label=\"model with\\nflat prior\");\n",
    "sns.distplot([np.mean(observed_vals.sample(frac=1, replace=True)) for _ in range(100)], label=\"bootstrapping\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The above plot demonstrates that the results of applying a model with a flat prior to the data\n",
    "are very similar to applying bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But at a cost: improper distributions lose some of the good things about generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    improper_pripred = pm.sample_prior_predictive(model=improper_model)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Important: prior must match any constraints on that variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_bivariate_graph(\"sigma\", \"X\", observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where $\\sigma$ sets the standard deviation of X.\n",
    "The standard deviation cannot be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Say we choose as our prior a very wide `Normal` centered at `0`,\n",
    "attempting to choose a relatively flat, unopinionated prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "observed_vals = 0\n",
    "\n",
    "with pm.Model() as prior_model:\n",
    "    sigma = pm.Normal(\"sigma\", 0, 1000)\n",
    "    X = pm.Normal(\"X\", mu=0, sd=sigma, observed=observed_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attempting to sample from that model can cause problems: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    shared_util.sample_from(prior_model, draws=1000)\n",
    "except pm.parallel_sampling.ParallelSamplingError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Depending on the exact sample drawn, you will either get an error or just warnings:\n",
    "black text against a red background.\n",
    "\n",
    "The message `Bad initial energy`, accompanied by `X   -inf`,\n",
    "indicates that one of the samples had a likelihood of 0:\n",
    "something impossible occurred.\n",
    "\n",
    "In this case, it happens when the standard deviation is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"#FDB515\"> Likelihoods </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Likelihoods say:\n",
    "if I could observe the _parameters_ but not the _data_,\n",
    "then this distribution would express my uncertainty about what data I might see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Likelihoods, and how they relate the parameters to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`Normal`: values get more unlikely as they get further from `mu`, at a rate determined by `1/sd` (aka `tau`)\n",
    "\n",
    "`Binomial`: data is the outcome of a number of `N` independent attempts, each with probability `p` of occurring\n",
    "\n",
    "`Poisson`: data is the outcome of independent attempts where `N` is large or infinite and `p` is small or infinitesimal, with `mu=N * p`.\n",
    "\n",
    "`Exponential`: values get more unlikely as they get larger, at a rate determined by `lam`\n",
    "OR\n",
    "data is the time in between events in a memoryless process, occuring about `lam` every unit time\n",
    "\n",
    "`Laplace`: values get more unlikely as they get further from `mu` in absolute difference, at a rate determined by `1/b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that some are mechanistic, others are not:\n",
    "the `Normal` is not particularly mechanistic,\n",
    "but the `Poisson` and `Binomial` are.\n",
    "\n",
    "The `Exponential` might be mechanistic in some models,\n",
    "but not in others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We don't typically sample directly from a likelihood in the course of modeling, but if you wanted to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "pm.Foo.dist(parameter_0=parameter_0_fixed, ... ).random(size=N)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"#2ca02c\"> Posteriors </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Posteriors represent our beliefs after we have observed data for a random variable in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We do not specify them directly:\n",
    "instead, they are computed from the combination of\n",
    "our <font color=\"#FDB515\">prior</font>,\n",
    "our <font color=\"#003262\">**likelihood**</font>,\n",
    "and the data we observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we don't interact with them as pyMC random variables with distributions;\n",
    "instead we interact with them as _collections of samples_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sampling from posteriors works differently for unobserved and observed variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For unobserved variables, use `pm.sample`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We've covered this fairly extensively, so moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For observed variables, use `pm.sample_posterior_predictive`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The output of `pm.sample_posterior_predictive`\n",
    "is still compatible with `shared_util.samples_to_dataframe`,\n",
    "but it's slightly different from the output of `pm.sample`:\n",
    "it is literally a dictionary,\n",
    "rather than being a `MultiTrace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mu = 0; N = 50\n",
    "observed_vals = pm.Normal.dist(mu=mu, sd=1).random(size=N)\n",
    "\n",
    "with pm.Model() as observed_normals:\n",
    "    sd = pm.HalfNormal(\"sd\", sd=10)\n",
    "    X = pm.Normal(\"X\", mu=0, sd=sd, observed=observed_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with observed_normals:\n",
    "    # pm.sample returns samples from posterior of unobserved variables as a trace\n",
    "    posterior_trace = pm.sample(target_accept=0.99)\n",
    "    \n",
    "# pm.sample_posterior_predictive returns samples from the posterior of observed variables as a dict\n",
    "postpreds_dict = pm.sample_posterior_predictive(posterior_trace, model=observed_normals)\n",
    "postpreds_df = shared_util.samples_to_dataframe(postpreds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(postpreds_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(postpreds_df.iloc[0][\"X\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice: each row of `postpreds_df` has a sample\n",
    "the same size as the `observed` dataset.\n",
    "\n",
    "Each one is a sample drawn from the likelihood of that observed variable\n",
    "with the parameters given by a sample from the posterior, one from each of the values in the `posterior_trace`.\n",
    "Each one is a dataset that might have been observed if the parameter had been equal to that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6));\n",
    "[sns.distplot(row[\"X\"], color=\"k\") for ii, row in postpreds_df.sample(frac=1).iloc[:5].iterrows()]\n",
    "sns.distplot(observed_vals * 1, color=\"C2\", rug=True, label=\"observed\");\n",
    "plt.ylim(*(1.5 * np.array(plt.ylim()))); plt.legend(); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This plot compares individual rows of the `postpreds_df` to the observed data.\n",
    "\n",
    "When the posterior is tight around the correct value of the parameter,\n",
    "the samples from `posterior_predictive` will look like the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If the samples from `posterior_predictive` don't look like your data,\n",
    "that doesn't mean your model is wrong,\n",
    "it just means that there's residual uncertainty in your posteriors of the parameters\n",
    "that's sufficient to show .\n",
    "\n",
    "There might be another model that does better,\n",
    "one whose posterior predictions, given that data,\n",
    "would look like the observed data.\n",
    "But it could also be that the model just needs more observations:\n",
    "increase `N` to `50` from `5` and re-run the cells afterwards.\n",
    "\n",
    "Also, just because the samples look like the data provided via `observed`,\n",
    "that doesn't mean that the model is correct.\n",
    "You don't get points for being able to predict data you've already seen.\n",
    "\n",
    "Resolving this issue and figuring out which model for a given dataset is best\n",
    "is one of the core problems of statistics and data science.\n",
    "We'll look at some basic methods for model comparison in lab this week,\n",
    "and then we'll see more sophisticated methods in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary of Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "with pm.Model() as model:\n",
    "    # random variable to model uncertainty about the truth before seeing data: the _prior_\n",
    "    parameter_var = pm.SomeRandomVariable(\"prior_variable\", parameter=value)  \n",
    "    # random variable to model uncertainty in data, given parameters: the _likelihood_\n",
    "    observed_var = pm.OtherRandomVariable(\"data_variable\", paramter=parameter_var, observed=data_we_saw)\n",
    "    \n",
    "# samples to estimate uncertainty before seeing data\n",
    "pripred_samples = pm.sample_prior_predictive(model=model)\n",
    "\n",
    "with model:\n",
    "    # samples to estimate uncertainty about the parameters after seeing data: the _posterior_\n",
    "    post_samples_trace = pm.sample()\n",
    "\n",
    "# samples to estimate uncertainty in what future data we might see:\n",
    "postpred_samples = pm.sample_posterior_predictive(post_samples_trace, model=model)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
