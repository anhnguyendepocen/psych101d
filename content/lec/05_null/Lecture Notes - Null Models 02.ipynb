{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Null Models 02 - Null Models for Means: _t_-Tests and Randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import daft\n",
    "from IPython.display import Image, YouTubeVideo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_plot(df, kde=False, print_t=True, print_p=False, style=\"kde\"):\n",
    "    if print_t:\n",
    "        print(f\"t = {compute_t_df(df)}\")\n",
    "    if print_p:\n",
    "        print(f\"p = {compute_p_df(df)}\")\n",
    "        \n",
    "    group0_selector = df[\"variety\"] == 0\n",
    "    group1_selector = df[\"variety\"] == 1\n",
    "    group0_values = df[\"yield\"][group0_selector]\n",
    "    group1_values = df[\"yield\"][group1_selector]\n",
    "    \n",
    "    if style == \"dist\":\n",
    "        f, ax = plt.subplots(figsize=(10, 10))\n",
    "        sns.distplot(group0_values, label=\"Variety 0\", kde=kde, norm_hist=True)\n",
    "        sns.distplot(group1_values, label=\"Variety 1\", kde=kde, norm_hist=True)\n",
    "        \n",
    "    elif style == \"kde\":\n",
    "        f, ax = plt.subplots(figsize=(8, 8))\n",
    "        sns.kdeplot(group0_values, shade=True, lw=4, label=\"Variety 0\");\n",
    "        sns.kdeplot(group1_values, shade=True, lw=4, label=\"Variety 1\");\n",
    "        ax.set_ylim(1.2 * np.array(ax.get_ylim()));\n",
    "    ax.legend();\n",
    "\n",
    "    \n",
    "def display_t_test_model(\n",
    "    mean_observed=False, sd_observed=False, value_observed=True, title=\"\"):\n",
    "\n",
    "    aspect = 1.5\n",
    "    scale = 2\n",
    "\n",
    "    offset = 0\n",
    "    delta = 2\n",
    "    y = 1.5\n",
    "\n",
    "    group_node = daft.Node(\"group\", \"group\",\n",
    "                           offset + delta, y, scale=scale, aspect=aspect, observed=True)\n",
    "    mean_node = daft.Node(\"$\\mu$\", \"$\\mu$\",\n",
    "                          offset + 2 * delta, y, scale=scale, aspect=1, observed=mean_observed)\n",
    "    value_node = daft.Node(\"value\", \"value\",\n",
    "                           offset + 3 * delta, y, scale=scale, aspect=aspect, observed=value_observed)\n",
    "    sd_node = daft.Node(\"$\\sigma$\", \"$\\sigma$\",\n",
    "                        offset + 2 * delta, y + delta, scale=scale, aspect=1, observed=sd_observed)\n",
    "\n",
    "    nodes = [sd_node, mean_node, value_node, group_node]\n",
    "    edges = [(\"$\\sigma$\", \"value\"), (\"group\", \"$\\mu$\"), (\"$\\mu$\", \"value\")]\n",
    "\n",
    "    gap = 1\n",
    "    left_edge = offset + delta - gap\n",
    "    width = (len(nodes) - 2) * delta + 2 * gap\n",
    "\n",
    "    plate = daft.Plate((left_edge, 0, width, y + 1), shift=0.25,\n",
    "                       label=\"$2 \\cdot N_g$\", position=\"bottom right\")\n",
    "\n",
    "    plates = [plate]\n",
    "\n",
    "    model = daft.PGM(shape=(8, 4 + 0.5), line_width=2)\n",
    "\n",
    "    [model.add_node(node) for node in nodes]\n",
    "    [model.add_plate(plate) for plate in plates]\n",
    "    [model.add_edge(*edge, head_width=0.25) for edge in edges]\n",
    "\n",
    "    # Avoid fill with blue in newer versions of matplotlib\n",
    "    for plate in plates:\n",
    "        plate.bbox[\"fc\"] = \"white\"\n",
    "\n",
    "    model.render();\n",
    "    \n",
    "    plt.title(title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This week, we put on our frequentist hats:\n",
    "we are going to study the most common method for frequentist inference,\n",
    "_null hypothesis significance testing_.\n",
    "\n",
    "Today, we will work through one of the most common tests:\n",
    "testing for a difference in means.\n",
    "We'll do this parameterically, using\n",
    "the $t$-test, specifically the two-sided unpaired Student's $t$-test,\n",
    "and non-parametrically, using a resampling scheme called the _permutation test_.\n",
    "\n",
    "Next week, we'll take a Bayesian approach to similar problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing Proceeds by Applying Thresholds to $p$-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $p$-value for a test is obtained by comparing the value of a statistic,\n",
    "called the _test statistic_,\n",
    "to the sampling distribution of that same statistic under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\n",
    "      \"https://4.bp.blogspot.com/-XvlZhew0MqY/TeL3zIYTvcI/AAAAAAAAANk/i4vN6nVPMnE/s640/Hypothesistestingfigure.png\",\n",
    "     width=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this lecture, we work through a specific example: $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was one of the very first test statistics ever developed,\n",
    "in the 1910s by\n",
    "[William Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset),\n",
    "working pseudonymously under the name \"Student\"\n",
    "for Guinness & Co. Brewing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gossett was interested in determining which varieties of barley provided the highest yields,\n",
    "but he was plagued by the fact that he was only able to observe a small number of plantings.\n",
    "\n",
    "If a barley variety A produced on average 3 bushels, averaged over 5 plantings,\n",
    "and another B produced 5, averaged over the same number,\n",
    "was it reasonable to conclude that variety B has a higher average yield?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barley_A_yield = pd.Series([3, 5, 1, 2, 4])\n",
    "barley_B_yield = pd.Series([7, 5, 3, 4, 6])\n",
    "\n",
    "yields = pd.concat([barley_A_yield, barley_B_yield])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barley_df = pd.DataFrame({\"yield\": yields, \"variety\": [0] * 5 + [1] * 5})\n",
    "\n",
    "print(barley_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: these numbers are entirely fictional, though the example is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_plot(barley_df, print_t=False, style=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is our estimate of our uncertainty in the yields\n",
    "of the two varieties, is it reasonable to conclude that the means are different,\n",
    "or might this have occurred due to chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gossett pioneered the approach of defining a test statistic and determining the distribution of that statistic under the null.\n",
    "\n",
    "He chose the following statistic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "t = \\frac{\\mu_A - \\mu_B}{\\sigma\\sqrt{\\frac{2}{N_g}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mu_A$ for a pandas series `A` is `A.mean()`,\n",
    "and $N_g$ is `len(A)`, which is presumed equal to `len(B)`.\n",
    "The other value the denominator, $\\sigma$, is\n",
    "the estimate of the group standard deviation\n",
    "and is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^2 = \\frac{\\sigma^2_A + \\sigma^2_B}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\sigma_A^2$ is `A.var()`.\n",
    "That is, we average the estimated variances of each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is called $t$ because it was the $t$est statistic\n",
    "for one of the first modern statistical tests.\n",
    "We will work through the rationale for this statistic below,\n",
    "but for now,\n",
    "take it as a given.\n",
    "Notice that its magnitude will be large\n",
    "if the observed means of the groups are very different,\n",
    "relative to their spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_t(a, b):\n",
    "    sp = compute_sigma_pooled(a, b)\n",
    "    t = (a.mean() - b.mean()) / (sp * np.sqrt(2 / len(a)))\n",
    "    return t\n",
    "\n",
    "\n",
    "def compute_sigma_pooled(a, b):\n",
    "    sigma_pooled = np.sqrt((a.var() + b.var()) / 2)\n",
    "    return sigma_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, a convenient function that lets us apply `compute_t` directly to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_t_df(df):\n",
    "    group0_selector = df[\"variety\"] == 0\n",
    "    group1_selector = df[\"variety\"] == 1\n",
    "    \n",
    "    return compute_t(df[\"yield\"][group0_selector], df[\"yield\"][group1_selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_t_df(barley_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is that large? Is that small?\n",
    "\n",
    "Just as we have seen in previous instances of inferential thinking,\n",
    "we need to compare a value to a distribution in order to get a sense of scale.\n",
    "Previously, the question was how uncertain we were in a given value,\n",
    "and we obtained an answer by _bootstrapping_.\n",
    "Now, the question is how likely a value like the above might occur by chance,\n",
    "and we will obtain the answer by _comparison to the null_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the $t$ Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual null hypothesis associated with this statistic is\n",
    "that\n",
    "\n",
    "$$\n",
    "\\mu_A = \\mu_B\n",
    "$$\n",
    "\n",
    "and that the sample means have a `Normal` distribution.\n",
    "\n",
    "This will be exactly true if the individual observations are also `Normal`.\n",
    "It will be approximately true, even if that is not the case,\n",
    "in many cases, so long as the number of samples is sufficiently high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example of Gossett,\n",
    "this is the statement that the two varieties of barley have the same average yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've also assumed that the true standard devation of the groups is the same\n",
    "in writing down the definition of $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_t_test_model(\n",
    "    mean_observed=True, sd_observed=True, value_observed=False,\n",
    "    title=\"Graphical Representation of $t$-Test Null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where _group_ is a binary variable that determines which group an observation is in,\n",
    "$\\mu$ is a list of means for each group,\n",
    "_value_ is the value that is being observed,\n",
    "$N_g$ is the number of observations in each group,\n",
    "and $\\sigma$ is the standard deviation, which is presumed the same in each group,\n",
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{value} \\sim \\text{Normal}(\\mu\\left[\\text{group}\\right], \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the rectangle, called a \"plate\",\n",
    "means that we observe the contents multiple times,\n",
    "with the number of observations given in the small box in the bottom right.\n",
    "For the barley example, $N_g = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_t_model(true_sigma, group_size, group0mean=0, delta_mean=0):\n",
    "    \n",
    "    group_labels = [0] * group_size + [1] * group_size\n",
    "    \n",
    "    with pm.Model() as t_model:\n",
    "        group = pm.Categorical(\"varieties\", p=[1/2, 1/2],\n",
    "                               observed=group_labels)\n",
    "        group_means = shared_util.to_pymc(\n",
    "            [group0mean, group0mean + delta_mean])\n",
    "        \n",
    "        value = pm.Normal(\"yields\",\n",
    "                          mu=shared_util.to_pymc(group_means)[group],\n",
    "                          sd=true_sigma, shape=len(group_labels))\n",
    "        \n",
    "    return t_model, group_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the barley example, the \"group\" variable is _variety_\n",
    "and the observed \"value\" is _yield_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of $\\mu$ are determined by our null hypothesis:\n",
    "they are equal, and equal to the mean we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = barley_df[\"yield\"].mean()\n",
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_pooled = compute_sigma_pooled(barley_A_yield, barley_B_yield)\n",
    "sigma_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fully specified our null model,\n",
    "we can now draw samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_t, group_labels = make_t_model(sigma_pooled, group_size, group0mean=mean_pooled, delta_mean=0)\n",
    "samples = shared_util.samples_to_dataframe(shared_util.sample_from(null_t, draws=2500, chains=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each resulting sample is like the data from a fictitious experiment:\n",
    "a window into what we might observe in an alternate world where the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But each row doesn't yet _look_ like the data from our barley experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.iloc[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(barley_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so we cannot use the analysis tools we applied to our real data,\n",
    "like `compute_t_df` or `t_plot`.\n",
    "\n",
    "To allow their use,\n",
    "we need to transform our samples into something that looks like our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_experiment_df(sample, group_labels):\n",
    "    return pd.DataFrame({\"yield\": sample, \"variety\": group_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't just a programming convenience.\n",
    "This is synecdochal of the entire approach:\n",
    "the samples from the null are _plausible alternative datasets_,\n",
    "given that the null is true,\n",
    "and so our analysis should respect and reflect that fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_experiment_df(samples[\"yields\"].iloc[0], group_labels).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get a sense for the sampling distribution of the data under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_t_dfs = [sample_to_experiment_df(sample[\"yields\"], group_labels) for _, sample in samples.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = random.choice(null_t_dfs)\n",
    "t_plot(target_df, print_t=True, style=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that on many of these samples, especially for a small group size,\n",
    "there appears to be an obvious pattern:\n",
    "the variance of one is much larger,\n",
    "the mean of one variety is much smaller.\n",
    "But because we are sampling from the null,\n",
    "we know that those patterns are illusory:\n",
    "the groups have the same mean and the same variance.\n",
    "\n",
    "It is the job of statistics to inform us when the pattern we are claim is liable to be illusory,\n",
    "because it occurs sufficiently often under the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern we are claiming is meaningful in our data is that the $t$ value is large in magnitude.\n",
    "\n",
    "In order to test that pattern,\n",
    "we must compute `t` on all of our samples from the null,\n",
    "and then compare the magnitudes we sampled to the magnitude we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_distribution_t = pd.Series(\n",
    "    [compute_t_df(null_t_df) for null_t_df in null_t_dfs], name=\"$t$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_f, null_ax = plt.subplots(figsize=(10, 6))\n",
    "sns.distplot(null_distribution_t, bins=100, ax=null_ax, label=\"MC Estimate\");\n",
    "plt.xlim(-10, 10); plt.ylim(1.5 * np.array(plt.ylim())); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to perform our $t$ test,\n",
    "we need to compare the value we obtained to the values under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barley_t = compute_t_df(barley_df)\n",
    "barley_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_ax.vlines(barley_t, 0, null_ax.get_ylim()[1] * 0.25, lw=6, label=\"Observed\")\n",
    "null_ax.legend(); null_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the histogram,\n",
    "we add up the areas of all of the bars\n",
    "corresponding to values larger in magnitude than the one we actually observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(null_distribution_t) >\n",
    "     np.abs(compute_t(barley_A_yield, barley_B_yield))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional way to compute $p$ is that we compare the value of $t$ optained to a table of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://www.ruf.rice.edu/~bioslabs/tools/stats/ttable.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Degrees of freedom\" is the _only_ parameter for the null distribution of $t$,\n",
    "and it is determined by the group sizes you choose.\n",
    "It is also unaffected by the group means or by the pooled standard deviation,\n",
    "as you can confirm for yourself by changing\n",
    "those numbers in the definition of the pyMC model `null_t`.\n",
    "\n",
    "Degrees of freedom is, in general,\n",
    "equal to the number of data points from the dataset I would\n",
    "need to tell you,\n",
    "along with the value of the statistic,\n",
    "before you could guess the remaining values.\n",
    "\n",
    "In our case, it's $2\\cdot N_g - 2$:\n",
    "we calculate $t$ from the pooled standard deviations of the two groups,\n",
    "and each has $N_g - 1$ degrees of freedom.\n",
    "\n",
    "For the barley example,\n",
    "this value is 8,\n",
    "so we can say that our $p$ value is $>0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't get a continuous value for $p$ out of this method.\n",
    "For this reason, it was customary up until very recently to only report\n",
    "whether the $p$ value was below a certain threshold.\n",
    "\n",
    "So why was this ever done?\n",
    "\n",
    "Because before there were ubiquitous computers,\n",
    "it was infeasible to calculate the sampling distribution of $t$ yourself,\n",
    "either by estimation or by means of a formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get those values,\n",
    "we need to know the a mathematical or analytical form for the shape of the\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape looks very much like a `Normal`,\n",
    "but it is in fact ever so slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_estimate_params = null_distribution_t.mean(), null_distribution_t.std()\n",
    "norm_estimate_pdf = scipy.stats.norm(*norm_estimate_params).pdf\n",
    "ts = np.linspace(min(null_distribution_t), max(null_distribution_t), num=1000)\n",
    "null_ax.plot(ts, norm_estimate_pdf(ts), lw=4, label=\"Normal Estimate\")\n",
    "null_ax.legend(); null_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As William Gosset showed,\n",
    "this distribution is actually given by\n",
    "a curve that now bears his pseudonym:\n",
    "[_Student's t distribution_](https://en.wikipedia.org/wiki/Student%27s_t-distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_true_pdf = scipy.stats.t(df=2 * group_size - 2).pdf\n",
    "\n",
    "student_true_pdf = scipy.stats.t(df=2 * group_size - 2).pdf\n",
    "null_ax.plot(ts, student_true_pdf(ts), lw=4, label=\"Analytical Student\")\n",
    "null_ax.legend(); null_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is like a `Normal`, but with more values close to $0$ and more values very far from $0$.\n",
    "\n",
    "pyMC allows you to sample directly from the $t$ distribution with `pm.StudentT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, for tests that used to have tables,\n",
    "we can use computers to calculate the values of $p$ instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(barley_A_yield, barley_B_yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_df(df):\n",
    "    group0_selector = df[\"variety\"] == 0\n",
    "    group1_selector = df[\"variety\"] == 1\n",
    "    t, p = scipy.stats.ttest_ind(df[\"yield\"][group0_selector], df[\"yield\"][group1_selector])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possession of a formula allows us to demonstrate something interesting:\n",
    "the sampling distribution of the $p$ statistic under the null hypothesis.\n",
    "\n",
    "We simply calculate $p$ for each of our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_distribution_p = pd.Series(\n",
    "    [compute_p_df(null_t_df) for null_t_df in null_t_dfs], name=\"$p$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_null_f, ax = plt.subplots(figsize=(6, 12))\n",
    "sns.distplot(null_distribution_p, color=\"C0\",\n",
    "             bins=20, kde=False, norm_hist=True, label=\"MC Estimate\", ax=ax);\n",
    "ax.hlines(1, 0, 1, lw=4, label=\"Uniform($0$, $1$)\")\n",
    "ax.set_ylim(0, 1.7);\n",
    "ax.set_title(\"Null Distribution of $p$\"); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it is approximately uniform.\n",
    "Apply bootstrap sampling to `null_distribution_p` if you're skeptical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can also see\n",
    "why the false positive rate is directly the value of the threshold we apply to $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_null_f, ax = plt.subplots(figsize=(6, 12))\n",
    "sns.distplot(null_distribution_p, bins=20, kde=False, norm_hist=True, label=\"True Negatives\");\n",
    "# note: this is only true negatives because the positives are hidden behind the bar plotted below\n",
    "ax.hlines(1, 0, 1, lw=4, label=\"Uniform($0$, $1$)\")\n",
    "\n",
    "bar_heights, _ = np.histogram(null_distribution_p, bins=20, density=True)\n",
    "ax.bar(0.025, bar_heights[0], width=0.05, color=\"C1\", label=\"False Positives\");\n",
    "ax.set_ylim(1.7 * np.array(ax.get_ylim()));\n",
    "ax.set_title(\"Null Distribution of $p$\"); ax.legend();\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bar_heights[0] * 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fraction of observations, under the null hypothesis,\n",
    "whose $p$ values will be below a value $\\alpha$,\n",
    "is exactly equal to $\\alpha$,\n",
    "because $p$ is uniformly distributed when the null is true.\n",
    "\n",
    "The false positive rate is the chance that we incorrectly reject the null hypothesis\n",
    "given that it is true:\n",
    "the chance that the value of $p$ is below our threshold of $\\alpha$ if our data\n",
    "is generated according to the null hypothesis.\n",
    "But as can be seen above, this is just equal to $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But Why $t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the definition of $t$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "t = \\frac{\\mu_A - \\mu_B}{\\sigma\\sqrt{\\frac{2}{N_g}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity we were actually interested in was just the difference in means.\n",
    "\n",
    "The confusing part, in the denominator,\n",
    "was something of a distraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instead calculate the difference in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delta_mu_from_df(df):\n",
    "    group0_selector = df[\"variety\"] == 0\n",
    "    group1_selector = df[\"variety\"] == 1\n",
    "    \n",
    "    delta_mu = (df[\"yield\"][group0_selector].mean() - df[\"yield\"][group1_selector].mean())\n",
    "    return delta_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_distribution_delta_mus = [compute_delta_mu_from_df(df) for df in null_t_dfs]\n",
    "# non_null_distribution_delta_mus = [compute_delta_mu_from_df(df) for df in power_t_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barley_delta_mu = compute_delta_mu_from_df(barley_df)\n",
    "barley_delta_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(null_distribution_delta_mus, label=\"Null, MC Est.\", ax=ax)\n",
    "ax.vlines(barley_delta_mu, 0, null_ax.get_ylim()[1] * 0.25, lw=6, label=\"Observed\")\n",
    "ax.set_xlabel(\"$\\mu_A - \\mu_B$\"); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result is approximately the same $p$-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(null_distribution_delta_mus) > np.abs(barley_delta_mu)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side Note: sometimes, this value is below the mystical threshold of `0.05`,\n",
    "even though the value above is not.\n",
    "This is an indication of the inadequacy of the NHST method:\n",
    "it requires a hard threshold,\n",
    "but whether your data passes that threshold can be up to fairly arbitrary decisions,\n",
    "like exactly which statistic to use,\n",
    "and can be buffeted about by random forces.\n",
    "Even if one answer is correct and the other is incorrect,\n",
    "the choice to apply a hard threshold magnifies small errors:\n",
    "the null hypothesis is rejected in one case and not in the other,\n",
    "and one of those must be incorrect.\n",
    "If we retain a continuous value,\n",
    "the outcome of this experiment is the same for both statistics:\n",
    "there is moderately strong evidence against the null hypothesis\n",
    "that the two varieties have the same average yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice this distribution is `Normal`, and so\n",
    "it seems like a promising target for a nice statistical test based on the `Normal` distribution.\n",
    "\n",
    "The trouble is,\n",
    "the width of that distribution is unknown ahead of time.\n",
    "It is different _depending on the value of $\\sigma$_, which is unknown.\n",
    "\n",
    "This was bad for the days when statistical testing was done\n",
    "by reading tables out of books:\n",
    "instead of a single table,\n",
    "with a printed out set of $t$ values for each choice of group size,\n",
    "you'd need a table for every possible value of $\\sigma$,\n",
    "and there are infinitely many values!\n",
    "\n",
    "If you try to estimate the width from the data\n",
    "and then divide it out,\n",
    "so that the distribution of `delta_mu` is always\n",
    "the same width,\n",
    "then you end up with the $t$ distribution!\n",
    "\n",
    "Your attempt to estimate the width adds additional variability,\n",
    "which results in a distribution with bigger tails.\n",
    "As the size of the groups increases,\n",
    "your error is smaller and so the distribution becomes closer to a `Normal`\n",
    "with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the definition of $t$ was chosen for convenience and to handle very small samples:\n",
    "it means someone just had to calculate one big table, once,\n",
    "and then anyone could get a $p$ value by consulting that table,\n",
    "e.g. this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"https://www.ruf.rice.edu/~bioslabs/tools/stats/ttable.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But nowadays, we can estimate the null distribution of $t$,\n",
    "or _almost any other statistic_\n",
    "in seconds, by sampling with MC methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as a _parametric method_ of sampling,\n",
    "since our models have parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, bootstrapping is _non-parametric_:\n",
    "there are no fixed parameters and so there exists no parametric model of the dataset.\n",
    "\n",
    "We can also sample from the null non-parametrically, by means of _randomization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomization Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null hypothesis,\n",
    "there is no difference between the groups\n",
    "in terms of the statistic,\n",
    "and so from the perpsective of the test,\n",
    "the labels are irrelevant.\n",
    "\n",
    "Put another way,\n",
    "they _might as well be random_.\n",
    "\n",
    "For example,\n",
    "if William Gosset had accidentally swapped all of the labels on all of his\n",
    "barley seeds,\n",
    "and some of Variety A were accidentally labeled \"Variety B\" and vice versa,\n",
    "it would change the exact value of $t$ that he calculated,\n",
    "but we wouldn't expect it to change $t$ in any specific way:\n",
    "increasing or decreasing it, for example.\n",
    "\n",
    "Or, in another example,\n",
    "consider two salt shakers.\n",
    "Any grain of salt is exchangeable with any other grain of salt,\n",
    "at least in my view.\n",
    "And so, if someone were to mix the contents of the two shakers together\n",
    "and then pour the resulting mixture back out,\n",
    "I would be happy using either shaker,\n",
    "and the experience would be the same as if they'd never been mixed at all.\n",
    "\n",
    "The situation is very different for a salt and a pepper shaker.\n",
    "If the contents of the two are mixed,\n",
    "the result of applying the mixture to your food is quite different\n",
    "from the result of applying either of the original shakers.\n",
    "\n",
    "The idea of a randomization test is to apply such a procedure to your data\n",
    "and see whether your data behaves more like two salt shakers\n",
    "or more like a salt and a pepper shaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, loosely:\n",
    "we strip the labels off of the data,\n",
    "we put all of it into a bag,\n",
    "and then shuffle it around.\n",
    "Then, we pull it back out,\n",
    "sticking the labels back on randomly as we go.\n",
    "\n",
    "Finally, we treat the resulting data just like the data we observed,\n",
    "and compute our statistic of choice on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform such a randomization, also known as a _permutation_,\n",
    "in pandas, we simply sample the Series, with replacement,\n",
    "and put it back into the DataFrame.\n",
    "The resulting DataFrame then goes into our statistical computation pipeline,\n",
    "just as though it were the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat this process over and over again,\n",
    "obtaining lots of different values for the statistic.\n",
    "\n",
    "Importantly, these values _will be distributed exactly as though the labels were meaningless_.\n",
    "That is, they represent the distribution of the statistic under the hypothesis\n",
    "that the groups are identical in every way, except for which label was chosen to put on them.\n",
    "\n",
    "Thus the null of a randomization test is much broader\n",
    "than the null of a similar parametric test:\n",
    "we no longer assume that the data was generated according to any particular model,\n",
    "as we did in the traditional and the pyMC approaches.\n",
    "\n",
    "The fact that it involves returning to your data and sampling from it\n",
    "makes it kin to bootstrapping.\n",
    "Both methods are known as _resampling_ methods,\n",
    "and they are furthermore _nonparametric_\n",
    "because they do not require the specification of a model with parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out\n",
    "[this slick visualization](https://www.jwilber.me/permutationtest/)\n",
    "for a thorough introduction to this idea,\n",
    "in the context of alpaca shampooing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_group(df):\n",
    "    rs_df = pd.DataFrame()\n",
    "    \n",
    "    rs_df[\"yield\"] = df[\"yield\"]\n",
    "    rs_df[\"variety\"] = df[\"variety\"].sample(frac=1).values\n",
    "    \n",
    "    return rs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_dfs = [randomize_group(barley_df) for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_ts = [compute_t_df(df) for df in resample_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(\n",
    "    resample_ts, ax=null_ax, label=\"Resample Estimate\", bins=25);\n",
    "null_ax.legend(ncol=2); null_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sample sizes are small,\n",
    "as in this case,\n",
    "the distribution under the permutation test can look a bit wonky,\n",
    "as this one does,\n",
    "but it becomes smoother as the sample size gets bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(resample_ts) > np.abs(barley_t)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with the `delta_mu`, or \"difference in means\", statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_delta_mus = [compute_delta_mu_from_df(df) for df in resample_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(resample_delta_mus) > np.abs(barley_delta_mu)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you must perform null hypothesis testing,\n",
    "rather than spending your time\n",
    "agonizing over the definition of the statistic and where it comes from,\n",
    "the assumptions of the associated test, etc.,\n",
    "spend your time coming up with a natural statistic for your data\n",
    "and apply null-sampling methods to it.\n",
    "\n",
    "If you can come up with a detailed null model,\n",
    "you can write a pyMC model to draw the samples.\n",
    "This same model can also be used to draw posterior samples.\n",
    "\n",
    "If you cannot come up with such a model,\n",
    "because you don't have the requisite knowledge\n",
    "to apply assumptions to your data,\n",
    "then you can apply randomization tests.\n",
    "\n",
    "In neither case is it absolutely necessary\n",
    "to use an existing statistic and its associated test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This view is argued forcefully\n",
    "in the blogpost\n",
    "[\"There is Only One Test\"](http://allendowney.blogspot.com/2011/05/there-is-only-one-test.html),\n",
    "by Allen Downey,\n",
    "and elaborated on in a follow-up,\n",
    "[\"There is Still Only One Test\"](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html),\n",
    "from which the below graphic id derived.\n",
    "That post also includes links to short video lectures,\n",
    "by the author and by others.\n",
    "One of them is [a tutorial](https://youtu.be/-7I7MWTX0gA)\n",
    "by Jake Vanderplas, author of\n",
    "[_The Python Data Science Handbook_](https://jakevdp.github.io/PythonDataScienceHandbook/)\n",
    "and\n",
    "[prolific blogger](http://jakevdp.github.io/)\n",
    "on all things data science and Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\n",
    "      \"https://lh4.googleusercontent.com/Bud31guq0w0FvylY57VMR0zHkYqxIpYAfOqgZietyvv1n2ToNEHwHKZWYix8pwct8kDKsZKiwvOWm6PIFEL3gBIQmbakQYHwVT02nn9_H8Fht_zaSBlrRNcqwZa950Vb5nt-5B84\",\n",
    "     width=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of modeling and simulating data can be done\n",
    "with parametric methods, e.g. in pyMC,\n",
    "or with non-parametric methods, i.e. by applying an appropriate randomization procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Non-Null Models to Calculate Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using a generative pyMC model\n",
    "is that we can, by writing it down correctly,\n",
    "also determine what happens when the null is false.\n",
    "\n",
    "This is often very difficult in traditional approaches,\n",
    "and impossible in a fully non-parametric approach,\n",
    "which does not specify any alternative hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question to ask about the behavior of a significance test when the null hypothesis is false is \"what is the chance that I correctly reject the null?\"\n",
    "\n",
    "This chance is known as the _power_ of the test,\n",
    "and it critically depends on what we assume the true value of the effect is.\n",
    "\n",
    "That is,\n",
    "just as we were able to calculate the chance of rejecting the null hypothesis when it was true\n",
    "by specifying the null hypothesis,\n",
    "we can calculate the chance of rejecting the null hypothesis when an alternate is true\n",
    "by specifying an alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Power _post hoc_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote `make_t_model` in such a way that it can be used to model\n",
    "the behavior of the $t$ statistic under non-null hypotheses:\n",
    "just set `delta_mean`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And therefore we can easily calculate power according to one of the most common methods:\n",
    "use the results of an experiment to make a _post hoc_ estimate of the experiment's power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we take as out alternative to the null the hypothesis that the effect we observed was the correct value,\n",
    "and then we draw samples from a model where that is the case.\n",
    "\n",
    "Graphically, this model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_t_test_model(\n",
    "    mean_observed=True, sd_observed=True, value_observed=False,\n",
    "    title=\"Graphical Representation of\\nPost-Hoc $t$-Test Power Calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is observed except the value,\n",
    "just as in the null,\n",
    "but now the values in $\\mu$ are different:\n",
    "they correspond to the values under the alternative,\n",
    "rather than the null, hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below specify and then sample from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barley_A_mean = barley_A_yield.mean()\n",
    "observed_delta_mean = barley_B_yield.mean() - barley_A_mean\n",
    "\n",
    "non_null_t, group_labels = make_t_model(sigma_pooled, group_size, barley_A_mean, delta_mean=observed_delta_mean)\n",
    "samples = shared_util.samples_to_dataframe(shared_util.sample_from(non_null_t, draws=1000, chains=4))\n",
    "\n",
    "sample_to_experiment_df(samples[\"yields\"].iloc[0], group_labels).head()\n",
    "\n",
    "non_null_t_dfs = [sample_to_experiment_df(sample[\"yields\"], group_labels) for _, sample in samples.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again,\n",
    "we estimate the distributions of the statistics of interest,\n",
    "$t$ and $p$,\n",
    "by applying our statistic-calculating functions to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_distribution_t = pd.Series(\n",
    "    [compute_t_df(non_null_t_df) for non_null_t_df in non_null_t_dfs], name=\"$t$\")\n",
    "\n",
    "non_null_distribution_p = pd.Series(\n",
    "    [compute_p_df(non_null_t_df) for non_null_t_df in non_null_t_dfs], name=\"$p$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(null_distribution_t, label=\"Null, MC Est.\");\n",
    "sns.distplot(non_null_distribution_t, label=\"Post-Hoc Alt, MC Est.\");\n",
    "ax.set_title(\"Distributions of $t$\"); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the distribution of $t$ is different under the null\n",
    "is what makes this hypothesis test work:\n",
    "when we observe large negative values of $t$,\n",
    "the alternative hypothesis is a much more likely explanation than the null is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.distplot(null_distribution_p, kde=False, norm_hist=True, label=\"Null, MC Est.\");\n",
    "sns.distplot(non_null_distribution_p, kde=False, norm_hist=True, label=\"Post-Hoc Alt, MC Est.\");\n",
    "ax.set_title(\"Distributions of $p$\"); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference in the distribution of $t$ is then passed forward\n",
    "into a difference in the distribution of $p$.\n",
    "\n",
    "Though the distribution of $p$ is always uniform under the null hypothesis,\n",
    "the distribution of $p$ under the alternative is generally not.\n",
    "In a good statistical test,\n",
    "this distribution is shifted towards 0.\n",
    "it would certainly be bad if it was shifted towards 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can obtain the power:\n",
    "it is the fraction of $p$s in our samples that are below the threshold,\n",
    "aka the area of the bar corresponding to $p$ values less than $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.distplot(non_null_distribution_p, kde=False, norm_hist=True, bins=20, label=\"False Negatives\", color=\"C1\");\n",
    "bar_heights, _ = np.histogram(non_null_distribution_p, bins=20, density=True)\n",
    "ax.bar(0.025, bar_heights[0], width=0.05, color=\"C1\", label=\"True Positives\")\n",
    "\n",
    "ax.set_title(\"Distribution of $p$ under Post-Hoc Alternate, MC Estimates\"); ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bar_heights[0] * 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value above is (an estimate of) the power of the $t$-test for this choice of settings of all the other relevant parameters:\n",
    "- the true value of the pooled standard deviation\n",
    "- the true value of the difference in means\n",
    "- the sample size\n",
    "- the choice of $\\alpha$/threshold on $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Power _a priori_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample size and choice of $\\alpha$ are known to us, and even under our control.\n",
    "If we wish to calculate the power for a different setting of those parameters,\n",
    "we just repeat the above analysis with those parameters changed.\n",
    "\n",
    "In general, we don't know what the true value of the pooled standard deviation is\n",
    "and we don't know what the true value of the difference in means is,\n",
    "or else we wouldn't need statistics!\n",
    "In running the above power calculation,\n",
    "we assumed that the values we observed were correct.\n",
    "\n",
    "Firstly, this is typically not true, nor even close to true,\n",
    "unless the sample sizes are very large.\n",
    "Secondly, this is of little help in choosing whether to run an experiment,\n",
    "since it requires the experiment to be run first.\n",
    "\n",
    "One way to compute the power of a test before running an experiment\n",
    "is to fix these values at some reasonable level:\n",
    "either _a priori_ or by taking a small amount of \"pilot data\".\n",
    "Instead of picking a single value,\n",
    "you might also pick a series of values and get out a series of possible powers:\n",
    "the lowest power we might have is this, the highest is that.\n",
    "\n",
    "But not knowing the value of the mean is just a special case of\n",
    "not knowing the value of a parameter in a model.\n",
    "That is, the problem of trying to determine the power\n",
    "is just a very important example of an inference problem.\n",
    "\n",
    "To compute the power of the $t$-test without assuming some fixed value,\n",
    "we therefore must place a prior over the difference in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_t_test_model(\n",
    "    mean_observed=False, sd_observed=True, value_observed=False,\n",
    "    title=\"Graphical Representation of\\nPrior $t$-Test Power Calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the values and the means have not been observed,\n",
    "and so they will be free random variables in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_t_model_w_prior(estimated_sd, group_size, group1mean=0, delta_mean_sd=2):\n",
    "    \n",
    "    group_labels = [0] * group_size + [1] * group_size\n",
    "    \n",
    "    with pm.Model() as t_model:\n",
    "        delta_mean = pm.Normal(\"delta_mean\", mu=0, sd=delta_mean_sd)\n",
    "        group = pm.Categorical(\"varieties\", p=[1/2, 1/2],\n",
    "                               observed=group_labels)\n",
    "        value = pm.Normal(\"yields\",\n",
    "                          mu=group1mean + pm.math.switch(group, delta_mean, 0),\n",
    "                          sd=estimated_sd, shape=len(group_labels))\n",
    "        \n",
    "    return t_model, group_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now we need to provide a parameter to our priod over the difference in means.\n",
    "In general, setting this prior requires us to think about what kinds of effect sizes we are likely to see.\n",
    "\n",
    "In this case, I chose a `Normal` prior centered at 0,\n",
    "so the only parameter was the width of that distribution, `delta_mean_sd`.\n",
    "I chose a symmetric prior because we weren't sure, _a priori_,\n",
    "which variety was more likely to give a larger yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_t, group_labels = make_t_model_w_prior(1, group_size, 0, 2)\n",
    "power_t_samples = shared_util.samples_to_dataframe(shared_util.sample_from(\n",
    "    power_t, draws=2500, chains=4, progressbar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_t_dfs = [sample_to_experiment_df(sample[\"yields\"], group_labels)\n",
    "               for _, sample in power_t_samples.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = random.choice(power_t_dfs)\n",
    "t_plot(target_df, print_p=True, style=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply our functions that compute $t$ and $p$\n",
    "to get our prior distributions over those statistics,\n",
    "which tell us our guess, before seeing the data,\n",
    "that we will be able to correctly reject the null if it is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_t_samples = pd.Series([compute_t_df(power_t_df) for power_t_df in power_t_dfs], name=\"$t$\")\n",
    "\n",
    "power_p_samples = pd.Series([compute_p_df(power_t_df) for power_t_df in power_t_dfs], name=\"$p$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(power_p_samples, kde=False, norm_hist=True, bins=20, label=\"MC Estimate\");\n",
    "ax.set_title(\"Prior Distribution of $p$\"); ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_estimate = (power_p_samples < 0.05).mean()\n",
    "power_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suggested Exercises with this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try increasing the `group_size` up to `10` or `30` or `100`.\n",
    "What happens to the distribution of $p$ under the null?\n",
    "What about under the post-hoc alternate?\n",
    "What effect does this have on the power?\n",
    "\n",
    "Try decreasing the `delta_mean` in the post-hoc alternate model (`non_null_t`).\n",
    "What does this do the distributions of $p$ and $t$ under that model?\n",
    "What does this do to the power?\n",
    "Do the same with increasing `true_sigma` (the first argument to `make_t_model`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
