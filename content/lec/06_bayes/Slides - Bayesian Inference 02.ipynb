{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Bayesian Inference 02 - Modeling Differences of Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import daft\n",
    "from IPython.display import HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import theano.tensor as tt\n",
    "\n",
    "def make_probability(distribution, **params):\n",
    "    \"\"\"Constructs a function that evaluates the exponential of\n",
    "    distribution's logp function for a given set of parameters,\n",
    "    provided as kwargs.\n",
    "    \n",
    "    For continuous distributions, this is a probability density.\n",
    "    For discrete distributions, this is a probability mass.\n",
    "    \"\"\"\n",
    "    logp = distribution.dist(**params).logp\n",
    "    \n",
    "    def probability(vals):\n",
    "        return np.exp(logp(shared_util.to_pymc(vals)).eval())\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Rather than"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p(\\text{hypothesis}\\vert \\text{test result}) = \\frac{p(\\text{test result}\\vert \\text{hypothesis}) p(\\text{hypothesis})}{p(\\text{test result})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "go back to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p(\\text{hypothesis}\\vert \\text{data}) = \\frac{p(\\text{data}\\vert \\text{hypothesis}) p(\\text{hypothesis})}{p(\\text{data})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "that is, use data more complicated than just a binary test result to determine our posterior beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Binary hypothesis testing, and its special case of null hypothesis significance testing,\n",
    "are a specific form of inferential thinking.\n",
    "\n",
    "NHST has, for the past century or so,\n",
    "been the dominant method for inferential thinking in science,\n",
    "for the essentially historical and technological reasons\n",
    "outlined last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With Bayesian inference, each hypothesis will be a concrete choice for the parameters of our model,\n",
    "and this will lead to a much simpler approach to understanding how to interpret our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Inference for Differences in Means: Guinness and Barley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problem setup:\n",
    "William Gosset, alias \"Student\",\n",
    "is interested in determining which variety of barley produces a higher yield when planted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_A_yield = pd.Series([3, 1, 4, 5, 2])\n",
    "barley_B_yield = pd.Series([7, 5, 3, 4, 6])\n",
    "\n",
    "yields = pd.concat([barley_A_yield, barley_B_yield])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_df = pd.DataFrame({\"yield\": yields, \"variety\": [\"A\"] * 5 + [\"B\"] * 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since it's clear that sometimes Variety A produces more,\n",
    "while sometimes Variety B produces more,\n",
    "we have to frame the question in terms of some statistic or parameter.\n",
    "\n",
    "The typical choice is the _mean_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First model: `Normal` and `Exponential`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Setting the priors for a model is an unsolved problem.\n",
    "\n",
    "It's something of an art to transform qualitative knowledge\n",
    "into quantitative statements about probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, we run into numerical and mathematical issues when we specify our priors badly.\n",
    "\n",
    "See [this GitHub wiki page](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)\n",
    "by the authors of Stan, another MCMC library, for some generic tips.\n",
    "These are world experts on Bayesian inference, and note how informal and loose these recommendations are.\n",
    "This is an unsolved problem, and the presence of these subjective unsolved problems in Bayesian inference\n",
    "workflows is a major impediment to its adoption,\n",
    "since often the goal of quantitative methods is to eliminate subjectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For our first model, let's use some ideas from the $t$-test:\n",
    "\n",
    "1. Both groups are normally-distributed\n",
    "2. The two groups have the same standard deviation\n",
    "\n",
    "The first statement means our likelihood will be `Normal`.\n",
    "\n",
    "The `Normal` has two parameters, `mu` and `sd`.\n",
    "The second statement means that `sd` is shared between the groups.\n",
    "\n",
    "And so our model has three latent, or hidden, variables:\n",
    "the variance parameter of the likelihood\n",
    "and the two mean parameters of the likelihood,\n",
    "one for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Means: `pm.Normal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most objective way to set this prior\n",
    "would be to look at past data about barley yields,\n",
    "allowing us to get a sense for what's likely\n",
    "for these novel barley varieties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But that's usually not possible:\n",
    "we're often working with new data,\n",
    "for which there isn't a large database.\n",
    "The closest thing we have is\n",
    "the data we have collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_A_yield.mean(), barley_B_yield.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.std([barley_A_yield.mean(), barley_B_yield.mean()], ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It's somewhat cheating to use your data in setting your prior.\n",
    "\n",
    "To remedy this somewhat,\n",
    "we will just increase the standard deviation by a factor of about 2.\n",
    "\n",
    "This reduces the impact of our prior on our posterior\n",
    "by spreading out the distribution.\n",
    "More widely-spread priors have less impact on posteriors,\n",
    "as you saw in the lab on parameterized models.\n",
    "\n",
    "If you're concerned about \"double-dipping\", you can always just increase the standard deviation further.\n",
    "\n",
    "We'll see below that this choice of parameters has a fairly modest impact on inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as barley_model:\n",
    "    # priors on parameters\n",
    "    means = pm.Normal(\"means\", mu=4, sd=3, shape=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Standard Deviation: `pm.Exponential`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Originally introduced back in the second lecture on random variables as \"time in between events in a memoryless process\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A _memoryless process_ is one where events have no influence on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples: raindrops, Amazon orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Counterexamples: [buses](http://jakevdp.github.io/blog/2018/09/13/waiting-time-paradox/), parliamentary elections, bedtimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `Exponential` is also a common choice whenever we want to express the belief\n",
    "\n",
    "#### This variable is positive, and larger values get less likely fairly quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with barley_model:\n",
    "    # priors on parameters\n",
    "    pooled_sd = pm.Exponential(r\"$\\sigma$\", lam=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It has one parameter, `lam` or $\\lambda$, which is 1 / mean, or the 1 / average time between events,\n",
    "aka the average rate at which events occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this model, we are saying that the standard deviation is positive,\n",
    "and that very large values of the standard deviation are very unlikely:\n",
    "we don't expect that a variety will sometimes produce 1, other times 100, bushels\n",
    "with very high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Look up `pm.HalfNormal`, `pm.HalfStudentT`, and `pm.Lognormal` for two more distributions\n",
    "that express a similar belief.\n",
    "They are also positive-only:\n",
    "the first two are \"positive-only\" versions of the `Normal` and the `StudentT` distribution.\n",
    "\n",
    "The `HalfNormal` most strongly discounts large values,\n",
    "while the `HalfStudentT` is somewhere in between `HalfNormal` and `Exponential`,\n",
    "depending on its parameter.\n",
    "\n",
    "The distribution `pm.Lognormal` says that we can guess the order of magnitude\n",
    "of the variable, plus or minus some spread.\n",
    "This can be a very weak prior if the spread is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finishing the model with a likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with barley_model:\n",
    "    # likelihood to relate parameters to data\n",
    "    varieties = pd.Series(barley_df[\"variety\"] == \"B\", dtype=int)\n",
    "    yields = pm.Normal(\"yields\", mu=means[varieties], sd=pooled_sd,\n",
    "                       observed=barley_df[\"yield\"])\n",
    "    delta_means = pm.Deterministic(\"$\\mu_1 - \\mu_0$\", means[1] - means[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now, let's take a look at our prior by sampling with `sample_prior_predictive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_model_prior_samples = shared_util.samples_to_dataframe(pm.sample_prior_predictive(\n",
    "    model=barley_model, samples=5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shared_scales = True\n",
    "f, axs = plt.subplots(nrows=3, figsize=(12, 12), sharex=shared_scales, sharey=shared_scales)\n",
    "sns.distplot(barley_model_prior_samples[r\"$\\sigma$\"], ax=axs[0]);\n",
    "sns.distplot(barley_model_prior_samples[\"means\"].apply(lambda xs: xs[0]), ax=axs[1], axlabel=r\"$\\mu_0$\");\n",
    "sns.distplot(barley_model_prior_samples[\"$\\mu_1 - \\mu_0$\"], ax=axs[2]);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that we didn't _explicitly_ specify a prior on the latter:\n",
    "our prior on the value of \"$\\mu_1 - \\mu_0$\" is a consequence of our other priors.\n",
    "\n",
    "This is something like the sampling distribution of the \"difference in means\"\n",
    "statistic under the prior.\n",
    "It's not precisely the same because this the distribution of the _true difference in means_.\n",
    "To get the sampling distribution of the difference in means,\n",
    "we'd need to compute the difference in means on the samples of yields. \n",
    "\n",
    "If we observe a value of this variable that is very unlikely under our prior distribution,\n",
    "that suggests our prior might be wrong,\n",
    "just as observing a value of a statistic that is very unlikely under\n",
    "the sampling distribution of the null hypothesis (a low $p$ value)\n",
    "suggests that the null hypothesis might be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### And then look at the posterior given the data with `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_model_trace = shared_util.sample_from(barley_model, draws=2500, chains=4, progressbar=True)\n",
    "barley_model_samples = shared_util.samples_to_dataframe(barley_model_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(barley_model_samples[\"$\\sigma$\"], color=\"C2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, the posterior for the standard deviation.\n",
    "\n",
    "It's somewhat hard to interpret without comparing to the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(barley_model_prior_samples[\"$\\sigma$\"], color=\"C0\", label=\"prior\");\n",
    "sns.distplot(barley_model_samples[\"$\\sigma$\"], color=\"C2\", label=\"posterior\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our posterior is much tighter than our prior:\n",
    "where before, we thought there was about a 50% chance\n",
    "that the standard deviation was below 1 or above 4,\n",
    "we now put a vanishly small chance on that being true.\n",
    "\n",
    "The cells below compute these probabilities more exactly.\n",
    "See the discussion around `compute_posterior_p`\n",
    "for more on how/why this is done in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "(barley_model_prior_samples[\"$\\sigma$\"] < 1).mean() + (barley_model_prior_samples[\"$\\sigma$\"] > 4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "(barley_model_samples[\"$\\sigma$\"] < 1).mean() + (barley_model_samples[\"$\\sigma$\"] > 4).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: determining something like this while running a $t$-test would have required\n",
    "the elaboration of _another_ statistical test,\n",
    "likely with additional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But we were more interested in the difference of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sns.distplot(barley_model_prior_samples[\"$\\mu_1 - \\mu_0$\"], color=\"C0\", label=\"prior\");\n",
    "sns.distplot(barley_model_samples[\"$\\mu_1 - \\mu_0$\"], color=\"C2\", label=\"posterior\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Again,\n",
    "even though we only observed a relatively small amount of data,\n",
    "it's enough to massively change our prior,\n",
    "since it reflected our state of very extreme ignorance.\n",
    "\n",
    "Just from visually inspecting these distributions,\n",
    "we can draw some inferences.\n",
    "For example, the difference in means is\n",
    "very unlikely to be extremely large.\n",
    "\n",
    "If we want to be more quantitative in our inferences,\n",
    "e.g. if we'd like to infer whether the\n",
    "mean of Variety B is higher, in order to drive a business decision,\n",
    "we just need to check what the probability\n",
    "of that claim is, under the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(barley_model_samples[\"$\\mu_1 - \\mu_0$\"] > 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that the resulting value is dramatically different from\n",
    "what we had in the prior:\n",
    "about 50-50 odds.\n",
    "\n",
    "This experiment was very informative,\n",
    "even if it wasn't definitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "(barley_model_prior_samples[\"$\\mu_1 - \\mu_0$\"] > 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice what is being done here,\n",
    "and how it is similar to the qualitative form of inference:\n",
    "we are checking whether the inference we wanted to draw\n",
    "was true on each sample,\n",
    "and then calculating the fraction of samples on which it was true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can be generalized to all kinds of different inferences,\n",
    "without any need to do more than change what we calculate on our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def compute_posterior_p(posterior_samples, check_inference):\n",
    "    \"\"\"Given some posterior samples and function that checks\n",
    "    whether an inference is true or false, returns the probability\n",
    "    under the posterior given by the samples that the inference is true.\n",
    "    \"\"\"\n",
    "    inference_true_booleans = []\n",
    "    for _, sample in posterior_samples.iterrows():\n",
    "        inference_true_booleans.append(check_inference(sample))\n",
    "    return pd.Series(inference_true_booleans).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the inference we were interested in\n",
    "def mu1_greater(sample):\n",
    "    return sample[\"$\\mu_1 - \\mu_0$\"] > 0\n",
    "\n",
    "# what's the chance that these varieties have a low value of sigma\n",
    "def sigma_under_3(sample):  \n",
    "    return sample[\"$\\sigma$\"] < 3\n",
    "\n",
    "# a wacky inference, but that's no barrier to computing it\n",
    "def mu0_less_than_sigma(sample):  # \n",
    "    return sample[\"$\\sigma$\"] > sample[\"means\"][0]\n",
    "\n",
    "print(compute_posterior_p(barley_model_samples, mu1_greater),\n",
    "      compute_posterior_p(barley_model_samples, sigma_under_3),\n",
    "      compute_posterior_p(barley_model_samples, mu0_less_than_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Credible Intervals: Confidence Intervals for Bayesians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Confidence Interval was intended to give an estimate of what values of a variable were plausible or likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But remember, that's not what a Confidence Interval really is:\n",
    "it is merely an interval-valued statistic that,\n",
    "on 95% of samples, covers the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Credible Intervals are the Bayesian equivalent of Confidence Intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A **Bayesian Credible Interval** is _any_ interval that covers some given percentage of the posterior density.\n",
    "\n",
    "For example, a **95% Credible Interval** covers 95% of the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That is, if the 95% Credible Interval is `(-1, 1)` we believe there is 95% chance that the value lies between `-1` and `1`.\n",
    "\n",
    "Just as there were many ways to construct Confidence Intervals,\n",
    "there are many ways to construct Credible Intervals.\n",
    "However, because it's easier to construct and compute a variety of Bayesian Credible Intervals,\n",
    "a wider variety get used.\n",
    "\n",
    "The two most common varieties are **Highest Posterior Density Intervals**\n",
    "and **Equal Tail Intervals**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Highest Posterior Density Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The _Highest Posterior Density Interval_ is the shortest credible interval.\n",
    "\n",
    "It can be computed from a `Series`, list, or array with `pm.stats.hpd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.stats.hpd(barley_model_samples[\"$\\mu_1 - \\mu_0$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `plot_posterior`\n",
    "\n",
    "Given the output of `pm.sample` or `shared_util.sample_from`\n",
    "(not a `DataFrame`, aka the output of `shared_util.samples_to_dataframe`),\n",
    "pyMC can make a convenient plot of the posterior and the Highest Posterior Density Interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(barley_model_trace, varnames=[\"$\\mu_1 - \\mu_0$\"], figsize=(12, 6), text_size=24,\n",
    "                  color=\"C2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is a histogram, just like in `sns.distplot`.\n",
    "\n",
    "The black bar covers, by default, the 95% HPD.\n",
    "The endpoints are indicated by hovering text,\n",
    "as is the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "From this 95% Credible Interval,\n",
    "we can determine that the difference in means has a 95% chance of being somewhere between roughly -0.3 and roughly 4.\n",
    "The exact boundaries of the interval will depend on the samples drawn, and can be read off from the text hovering above the black bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Quantiles: Equal Tail Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The _Equal Tail Interval_ is the credible interval with equal total probability\n",
    "above it and below it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is given by computing the percentiles of the posterior samples,\n",
    "e.g. with `pm.stats.quantiles`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.stats.quantiles(barley_model_samples[\"$\\mu_1 - \\mu_0$\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The endpoints of the 50% Equal Tail Interval are given by the values at\n",
    "`\"25\"` and `\"75\"` in the dictionary returned above,\n",
    "roughly `1.1` and `2.6`.\n",
    "\n",
    "The 95% Equal Tail Interval's endpoints\n",
    "are given by the values at `2.5` annd `97.5`\n",
    "and should be roughly `-0.4` to `4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the Equal Tail Interval covering 95% of the posterior\n",
    "is not the same as the Highest Posterior Density Interval  covering 95% of the posterior.\n",
    "\n",
    "They will be different whenever the samples are asymmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.boxplot(barley_model_samples[\"$\\mu_1 - \\mu_0$\"], width=0.2, linewidth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We display quantile information using a _box plot_,\n",
    "aka a _box-and-whisker plot_,\n",
    "accessible with `sns.boxplot`.\n",
    "\n",
    "The middle half of the data is indicated with the box:\n",
    "its left edge is at the 25th percentile\n",
    "and its right edge is at the 75th percentile.\n",
    "The width of this box is called the \"interquartile range\".\n",
    "The median is indicated with a bar through the box.\n",
    "\n",
    "The \"whiskers\" extend to cover all data points up to a maximum length equal to some number\n",
    "times the width of the box in the middle.\n",
    "The keyword argument in seaborn is `whis` and the default value is `1.5`,\n",
    "which is standard.\n",
    "\n",
    "Any points outside of this range are plotted individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparing the prior and posterior with `boxplot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The cell below combines the samples from the posterior with the samples from the prior into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "posterior_prior_comparison_df = pd.concat(\n",
    "    [barley_model_samples, barley_model_prior_samples])\n",
    "\n",
    "posterior_prior_comparison_df[\"distribution\"] = \\\n",
    "    [\"posterior\"] * len(barley_model_samples) + [\"prior\"] * len(barley_model_prior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "posterior_prior_comparison_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Side note: you might notice a column $\\sigma$`_log__`. Internally, pyMC works with logarithms for positive-only variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The additional `distribution` column identifies where a given sample was from the `prior` or the `posterior`.\n",
    "\n",
    "We can use this column for `groupby` operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "posterior_prior_comparison_df.groupby(\"distribution\")[\"$\\mu_1 - \\mu_0$\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And for hooking into seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Many seaborn plotting functions, including `boxplot`,\n",
    "can use columns of the dataframe to split up the data and automatically produce\n",
    "the same visualization for multiple subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.boxplot(x=\"$\\mu_1 - \\mu_0$\", data=posterior_prior_comparison_df,\n",
    "            y=\"distribution\", hue=\"distribution\",\n",
    "            palette=[\"C2\", \"C0\"], linewidth=4);\n",
    "ax.legend([], frameon=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For `boxplot` the `y` argument determines which variable sets the height of the boxes,\n",
    "while the `hue` argument determines which variable sets their color.\n",
    "\n",
    "If you use learn to use these features of seaborn,\n",
    "you can make very rich and informative plots in just a few lines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A model with weaker priors: `agnostic`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sometimes, we want to bring even less prior information to bear on our modeling problem.\n",
    "\n",
    "Our previous model very strongly discounted the possibility that the mean number of bushels\n",
    "would be in the hundreds or the hundreds of thousands.\n",
    "\n",
    "But perhaps that was too strong of an assumption?\n",
    "\n",
    "There are several \"go-to\" choices of prior that are common when trying to make as few assumptions as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `pm.HalfCauchy` and `pm.Cauchy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These two distributions have very \"long tails\":\n",
    "the chance of producing a value very far away from their center is relatively small,\n",
    "but substantially higher than for the `Exponential` or `Normal` distributions.\n",
    "\n",
    "They are used when we want to say that even extremely large values aren't too unsurprising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `HalfCauchy`, like the `HalfNormal`, the `HalfStudentT`, and the `HalfFlat`,\n",
    "is the positive-only version of the `Cauchy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These distributions are so broad that sampling from them is difficult,\n",
    "so instead of showing what they look like by drawing samples,\n",
    "the code below plots their distribution functions directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "half_cauchy_probability = make_probability(pm.HalfCauchy, beta=0.5)\n",
    "\n",
    "exponential_probability = make_probability(pm.Exponential, lam=2)\n",
    "\n",
    "sigmas = np.logspace(-5, 5, num=1000)\n",
    "half_cauchy_ps = half_cauchy_probability(sigmas)\n",
    "exponential_ps = exponential_probability(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(sigmas, exponential_ps, lw=4); plt.xlim([0, 5]);\n",
    "ax.plot(sigmas, half_cauchy_ps, lw=4); plt.xlim([0, 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you can see, the `HalfCauchy` is ever so slightly above the `Exponential`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This difference is much easier to see if we log-transform the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.semilogy(sigmas, exponential_ps, lw=4); plt.xlim([0, 40]);\n",
    "\n",
    "ax.semilogy(sigmas, half_cauchy_ps, lw=4); plt.xlim([0, 40]);\n",
    "ax.set_ylim([1e1, 1e-25]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The probabilities are exponentially decreasing for the `Exponential` distribution,\n",
    "as indicated by the fact that the log probabilities are decreasing in a straight line.\n",
    "\n",
    "The probabilities are decreasing much more slowly than exponentially for the `Cauchy` distribution:\n",
    "even though they are small, they are not dropping nearly as low as for the `Exponential`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The difference is much easier to see\n",
    "if we just look at a `rugplot` of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as barley_model_agnostic:\n",
    "    pooled_sd = pm.HalfCauchy(r\"$\\sigma$\", beta=10)\n",
    "    means = pm.Cauchy(\"means\",\n",
    "                      alpha=4,  # center\n",
    "                      beta=1,  # spread\n",
    "                      shape=2)\n",
    "    \n",
    "    varieties = pd.Series(barley_df[\"variety\"] == \"B\", dtype=int)\n",
    "    yields = pm.Normal(\"yields\", mu=means[varieties], sd=pooled_sd,\n",
    "                       observed=barley_df[\"yield\"])\n",
    "    delta_means = pm.Deterministic(\"$\\mu_1 - \\mu_0$\", means[1] - means[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_model_agnostic_prior_samples = shared_util.samples_to_dataframe(pm.sample_prior_predictive(\n",
    "    model=barley_model_agnostic, samples=5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shared_scales = True\n",
    "f, axs = plt.subplots(nrows=3, ncols=2, figsize=(12, 12), sharex=shared_scales, sharey=shared_scales)\n",
    "\n",
    "sns.distplot(barley_model_prior_samples[r\"$\\sigma$\"], ax=axs[0, 0], rug=True);\n",
    "sns.distplot(barley_model_prior_samples[\"means\"].apply(lambda xs: xs[0]), ax=axs[1, 0], rug=True, axlabel=r\"$\\mu_0$\");\n",
    "sns.distplot(barley_model_prior_samples[\"$\\mu_1 - \\mu_0$\"], ax=axs[2, 0], rug=True);\n",
    "\n",
    "sns.distplot(barley_model_agnostic_prior_samples[r\"$\\sigma$\"],\n",
    "             ax=axs[0, 1], kde=False, rug=True, norm_hist=True, bins=1000, color=\"C1\");\n",
    "sns.distplot(barley_model_agnostic_prior_samples[\"means\"].apply(lambda xs: xs[0]),\n",
    "             ax=axs[1, 1], kde=False, rug=True, norm_hist=True, bins=1000, color=\"C1\", axlabel=r\"$\\mu_0$\");\n",
    "sns.distplot(barley_model_agnostic_prior_samples[\"$\\mu_1 - \\mu_0$\"],\n",
    "             ax=axs[2, 1], kde=False, rug=True, norm_hist=True, bins=1000, color=\"C1\");\n",
    "axs[0, 0].set_title(\"Model A:\\nExponential-Normal Prior\"); axs[0, 1].set_title(\"Model B:\\nHalfCauchy-Cauchy Prior\")\n",
    "axs[-1, -1].set_xlim([-100, 100]); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While the draws from the original model are fairly tightly concentrated around the regions around 0,\n",
    "the draws from the agnostic model, with the `HalfCauchy` and `Cauchy` prior,\n",
    "are much more broadly distributed.\n",
    "\n",
    "Intuitively, this model is much less opinionated than the other about the data.\n",
    "\n",
    "Of course, the chance of a variety of barley having a yield\n",
    "that is an order or of magnitude or more higher than all the others is quite small,\n",
    "and so the `Exponential`-`Normal` model is very reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "shared_scales = True\n",
    "f, axs = plt.subplots(nrows=3, figsize=(12, 12), sharex=shared_scales, sharey=shared_scales)\n",
    "\n",
    "sns.distplot(barley_model_agnostic_prior_samples[r\"$\\sigma$\"],\n",
    "             ax=axs[0], kde=False, norm_hist=True, bins=1000, color=\"C1\");\n",
    "sns.distplot(barley_model_agnostic_prior_samples[\"means\"].apply(lambda xs: xs[0]),\n",
    "             ax=axs[1], kde=False, norm_hist=True, bins=1000, color=\"C1\", axlabel=r\"$\\mu_0$\");\n",
    "sns.distplot(barley_model_agnostic_prior_samples[\"$\\mu_1 - \\mu_0$\"],\n",
    "             ax=axs[2], kde=False, norm_hist=True, bins=1000, color=\"C1\");\n",
    "axs[-1].set_xlim([-100, 100]); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: the kernel density estimates don't quite do these distributions justice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Once again, we draw some samples,\n",
    "package them into a dataframe, and visualize the posterior\n",
    "with a box-and-whisker plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "barley_model_agnostic_trace = shared_util.sample_from(barley_model_agnostic, draws=2500, chains=4, progressbar=True)\n",
    "barley_model_agnostic_samples = shared_util.samples_to_dataframe(barley_model_agnostic_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "agnostic_posterior_prior_comparison_df = pd.concat(\n",
    "    [barley_model_agnostic_samples, barley_model_agnostic_prior_samples])\n",
    "\n",
    "agnostic_posterior_prior_comparison_df[\"distribution\"] = \\\n",
    "    [\"posterior\"] * len(barley_model_agnostic_samples) + [\"prior\"] * len(barley_model_agnostic_prior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "agnostic_posterior_prior_comparison_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.boxplot(x=\"$\\mu_1 - \\mu_0$\", y=\"distribution\",\n",
    "            data=agnostic_posterior_prior_comparison_df, hue=\"distribution\",\n",
    "            palette=[\"C2\", \"C0\"], linewidth=4);\n",
    "ax.legend([], frameon=False);\n",
    "ax.set_xlim([-20, 20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If combine the samples from both of our models into a single dataframe,\n",
    "including samples from the prior and the posterior for both,\n",
    "we can plot the original and updated beliefs for both models\n",
    "together and with minimal fuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_comparison_df = pd.concat([posterior_prior_comparison_df,  # stack the two dataframes on top of each other\n",
    "                                 agnostic_posterior_prior_comparison_df])\n",
    "# add a column indicating which model each sample came from\n",
    "model_comparison_df[\"model\"] = \\\n",
    "    [\"original\"] * len(posterior_prior_comparison_df) + [\"agnostic\"] * len(agnostic_posterior_prior_comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_comparison_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.boxplot(x=\"$\\mu_1 - \\mu_0$\", y=\"model\", data=model_comparison_df, hue=\"distribution\",\n",
    "            palette=[\"C2\", \"C0\"], linewidth=4);\n",
    "ax.set_xlim([-20, 20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This plot separates out the original and the agnostic model on the y-axis, with `y=\"model\"`,\n",
    "and then uses color to indicate which distribution the samples are drawn from, with `hue=\"distribution\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Directly from this plot,\n",
    "we can see that though the centers of the two priors are similar,\n",
    "the prior of the agnostic model is much more widely distributed.\n",
    "\n",
    "We can also see that the difference in posteriors is much smaller\n",
    "than the difference in priors.\n",
    "At least on the scale of the priors,\n",
    "the centers are fairly close,\n",
    "and the widths are about the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model with the weakest possible priors: `improper`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we wanted to make no assumption about what values of the standard deviation or the mean were more or less likely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `pm.Flat` and `pm.HalfFlat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "pyMC provides access to two distributions that aren't really probability distributions at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "flat_probability = make_probability(pm.Flat)\n",
    "\n",
    "half_flat_probability = make_probability(pm.HalfFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.arange(-10, 10)\n",
    "plt.step(xs, half_flat_probability(xs), lw=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.arange(-10, 10)\n",
    "plt.step(xs, flat_probability(xs), lw=4);\n",
    "plt.ylim([0, 1.1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Because the values are the same everywhere,\n",
    "except where they are 0,\n",
    "in the case of `HalfFlat`,\n",
    "they have no effect on the posterior\n",
    "except to say that some values are impossible.\n",
    "Whenever they show up in e.g. a Bayes' rule calculation,\n",
    "they have no effect on the output.\n",
    "\n",
    "Because they aren't probability distributions,\n",
    "as they don't add up to 1, they can't be sampled from\n",
    "with `sample_prior_predictive`.\n",
    "\n",
    "But they still result in a valid posterior,\n",
    "so we draw samples with `pm.sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as barley_model_improper:\n",
    "    pooled_sd = pm.HalfFlat(r\"$\\sigma$\")\n",
    "    means = pm.Flat(\"means\", shape=2)\n",
    "    \n",
    "    varieties = pd.Series(barley_df[\"variety\"] == \"B\", dtype=int)\n",
    "    yields = pm.Normal(\"yields\", mu=means[varieties], sd=pooled_sd,\n",
    "                       observed=barley_df[\"yield\"])\n",
    "    delta_means = pm.Deterministic(\"$\\mu_1 - \\mu_0$\", means[1] - means[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_model_improper_trace = shared_util.sample_from(barley_model_improper, draws=2500, chains=4)\n",
    "barley_model_improper_samples = shared_util.samples_to_dataframe(barley_model_improper_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And then add them to our model comparison dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "barley_model_improper_samples[\"model\"] = \"improper\"\n",
    "barley_model_improper_samples[\"distribution\"] = \"posterior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_comparison_df = model_comparison_df.append(barley_model_improper_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And, thanks to the power of seaborn,\n",
    "we automatically get a comparative visualization\n",
    "of the three models using the same code as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.boxplot(x=\"$\\mu_1 - \\mu_0$\", y=\"model\", data=model_comparison_df, hue=\"distribution\",\n",
    "            palette=[\"C2\", \"C0\"], linewidth=4);\n",
    "ax.set_xlim([-20, 20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the improper model has no prior distribution shown,\n",
    "since its prior cannot be sampled from.\n",
    "\n",
    "Notice also that the width of the posterior, the uncertainty, is greater than for the other models.\n",
    "This will generically be the case, as the width of the prior in the improper model is effectively infinite.\n",
    "\n",
    "Lastly, the center for the `Cauchy`-`HalfCauchy` model, the agnostic model,\n",
    "is closer to the center than either of the other models.\n",
    "This is because the peak of the prior is at 0,\n",
    "like the original model and unlike the improper model,\n",
    "and the greater width of the prior over the variance relative to the original model\n",
    "means it is updated less aggressively\n",
    "in response to the data.\n",
    "\n",
    "This makes the `Cauchy`-`HalfCauchy` model a good _conservative_ choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What would you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Normal-Exponential:\\t\", compute_posterior_p(barley_model_samples, mu1_greater),\n",
    "      \"\\nCauchy-HalfCauchy:\\t\", compute_posterior_p(barley_model_agnostic_samples, mu1_greater),\n",
    "      \"\\nFlat-HalfFlat:\\t\\t\", compute_posterior_p(barley_model_improper_samples, mu1_greater))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each of these models produces a slightly different posterior,\n",
    "and none of the posteriors results in a completely unambiguous inference about\n",
    "the difference in the group means.\n",
    "\n",
    "So what is to be done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is still a decent chance that Variety A\n",
    "has a slightly larger yield,\n",
    "even though it's unlikely to have a much larger yield,\n",
    "on the order of several bushels.\n",
    "\n",
    "If the downside to choosing the wrong variety is minimal,\n",
    "especially when the difference is on the order of a single bushel per planting,\n",
    "as is likely the case,\n",
    "then plant Variety B and reassess after the next harvest\n",
    "brings in more data.\n",
    "\n",
    "If this is a decision with a large downside,\n",
    "where even a small difference in yields\n",
    "could result in a major loss,\n",
    "either repeat the experiment,\n",
    "using some combination of these posteriors as a prior,\n",
    "or maybe plant some of each!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
