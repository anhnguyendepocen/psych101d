{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Categorical Effects 01 - Bayesian Inference for Categorical Effects Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import daft\n",
    "from IPython.display import HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "import utils.daft\n",
    "import utils.plot\n",
    "import utils.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "normal = lambda x, mu, sd: scipy.stats.norm(loc=mu, scale=sd).pdf(x)\n",
    "exponential = lambda x, mu, lam: scipy.stats.expon(loc=mu, scale=1/lam).pdf(x)\n",
    "cauchy = lambda x, loc, scale: scipy.stats.cauchy(loc=loc, scale=scale).pdf(x)\n",
    "uniform = lambda x, lower, width: scipy.stats.uniform(loc=lower, scale=width).pdf(x)\n",
    "f_dist = lambda x, loc, scale: scipy.stats.f(dfn=10, dfd=2, loc=loc, scale=scale).pdf(x)\n",
    "binom = lambda xs, n, p: scipy.stats.binom(n, p).pmf(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the last two weeks, we've primarily focused on differences in means between two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is the only thing a $t$-test can do (and then only if the samples are large or the population distribution of each group is normal.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Bayesian approach could do more (differences in $\\sigma$, compute other inferences, etc.)\n",
    "but our focus was on differences in means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## This week, we generalize to the case where there are more than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll do this by first \"turning the problem on its head\"\n",
    "and thinking about how to _generate_ data from multiple groups,\n",
    "using mixture distributions,\n",
    "rather than how to _analyze_ data from multiple groups,\n",
    "then finding posteriors of those generative models given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A mixture distribution is a weighted combination of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A mixture, just like baking mix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 2 cups sugar\n",
    "- 1 3/4 cups all-purpose flour\n",
    "- 3/4 cup unsweetened cocoa powder\n",
    "- 1 1/2 teaspoons baking powder\n",
    "- 1 1/2 teaspoons baking soda\n",
    "- 1 teaspoon salt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "From [FiveHeartHome.com](https://www.fivehearthome.com/homemade-chocolate-cake-mix-recipe/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A cake mix recipe has _ingredients_ that we combine in different _amounts_ to get a _mix_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's a \"distributional recipe\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 5% `Normal(-6, 1)`\n",
    "- 10% `Exponential(lam=1) + 2`\n",
    "- 50% `Normal(0, 1)`\n",
    "- 20% `Uniform(-3, 0)`\n",
    "- 15% `F(10, 2) + 5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"ingredients\" are known as mixture _components_.\n",
    "We bake with distributions instead of flour and sugar.\n",
    "\n",
    "The \"amounts\" are known as the mixture _weights_:\n",
    "we measure in fractions instead of in cups and teaspons.\n",
    "\n",
    "This \"weighted combination\" is the _mixture distribution_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "weights = [0.05, 0.1, 0.4, 0.3, 0.15]\n",
    "params = [(-6, 1), (2, 1), (0, 1), (-3, 3), (5, 1)]\n",
    "funks = [normal, exponential, normal, uniform, f_dist]  # defined above\n",
    "\n",
    "utils.plot.plot_mixture_marginal(weights, params, funks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If a variable is sampled according to this mixture distribution,\n",
    "the plot above shows what the histogram of samples would converge to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can compare that to all of the \"ingredients\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, funks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the top plot, we see the mixture distribution again.\n",
    "\n",
    "In the bottom plot, we see each of the \"ingredients\" in the mixture.\n",
    "\n",
    "If we think of the top plot as the _marginal_ distribution of the variable we observe,\n",
    "the bottom plot is the collection of _conditional distributions_:\n",
    "the distribution of the variable we observe _given_ which mixture component it came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can define a mixture distribution in pyMC by indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mixture_model:\n",
    "    # First, define each component of the mixture\n",
    "    rv1 = pm.Normal(\"norm1\",        # FIRST COMPONENT: Normal\n",
    "                    mu=-6, sd=1)\n",
    "    rv2 = pm.Deterministic(\"expo\",  # SECOND COMPONENT: Exponential\n",
    "                           pm.Exponential(\"_expo\", lam=1) + 2)\n",
    "    rv3 = pm.Normal(\"norm2\",        # THIRD COMPONENT: Normal\n",
    "                    mu=0, sd=1)\n",
    "    rv4 = pm.Uniform(\"unif\",        # FOURTH COMPONENT: Uniform\n",
    "                     lower=-3, upper=0)\n",
    "    rv5 = pm.Deterministic(\n",
    "        \"F\",                        # FIFTH COMPONENT: F\n",
    "         ((pm.ChiSquared(\"_u1\", 2) / 2) / (pm.ChiSquared(\"_u2\", 10) / 10)) + 5)\n",
    "    \n",
    "    # then, put all components in a \"list\"\n",
    "    list_of_components = shared_util.to_pymc([rv1, rv2, rv3, rv4, rv5])\n",
    "    \n",
    "    # then, determine which component is \"active\"\n",
    "    #  on this observation by chance, with p given by weights\n",
    "    component = pm.Categorical(\"component\", p=weights)\n",
    "    # and select it from that list\n",
    "    observed_value = pm.Deterministic(\"observed_value\",\n",
    "                             list_of_components[component])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is just like what we've been doing for models with two groups in them,\n",
    "but now there can be _any number of groups_:\n",
    "just define a variable for each group and add it to the \"list\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Programming Aside\n",
    "\n",
    "Feel free to skip this, especially on a first read,\n",
    "because it covers the internal details of pyMC.\n",
    "\n",
    "Internally to pyMC, `list_of_components` is something called a `Tensor`.\n",
    "It's very similar to an `array` in numpy.\n",
    "`shared_util.to_pymc` converts normal Python objects,\n",
    "like lists, into these special `Tensor` objects.\n",
    "\n",
    "The biggest difference is that the `Tensor` is like a placeholder:\n",
    "as you work with your pyMC model, e.g. by sampling or by doing MAP estimation,\n",
    "the values inside it will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you want to see what the `Tensor` looks like,\n",
    "for a given setting of all the variables,\n",
    "you need to define all of the input values to that `Tensor`\n",
    "and then call a method called `eval`,\n",
    "as in `eval`uate.\n",
    "\n",
    "In this example, the `Tensor` contains our five group random variables,\n",
    "and so if we want to know its contents,\n",
    "we need to define a value for each.\n",
    "\n",
    "This is done by providing `eval` a dictionary\n",
    "where the keys are the `RandomVariable`s\n",
    "and the values are the concrete values we want the variables to take on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The cells below show an example of this process.\n",
    "\n",
    "They also demonstrate what\n",
    "`list_of_components` and `list_of_components[component]`\n",
    "might look like for one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rvs_and_values = {rv1: 1., rv2: 2., rv3: 3., rv4: 4., rv5:10.}\n",
    "\n",
    "list_of_components.eval(inputs_to_values=rvs_and_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "list_of_components[4].eval(inputs_to_values=rvs_and_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## With a pyMC model, we can sample from a mixture distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with mixture_model:\n",
    "    samples = pm.sample(draws=1250, chains=4, target_accept=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(figsize=(12, 12), nrows=2, sharex=True)\n",
    "for var in [\"norm1\", \"expo\", \"norm2\", \"unif\", \"F\"]:\n",
    "    sns.distplot(samples[var], ax=axs[1]);\n",
    "sns.distplot(samples[\"observed_value\"], kde=False, bins=100, color=\"k\", norm_hist=True, ax=axs[0])\n",
    "utils.plot.plot_mixture_marginal(weights, params, funks, ax=axs[0])\n",
    "axs[1].set_xlim(-10, 10);\n",
    "axs[0].set_title(\"Mixture Distribution, pyMC Estimate and Actual\")\n",
    "axs[1].set_title(\"Mixture Components, pyMC Estimates\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## As graphs, all mixture models look the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_mixture_plate_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "g \\sim \\text{Categorical}(p=\\text{weights})\\\\\n",
    "x \\sim \\text{ComponentDistributions}_g(\\lambda[g], \\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is the graph for a mixture distribution over the variable $x$.\n",
    "\n",
    "Here $g$ is a variable that identifies which group, or component,\n",
    "a value $x$ comes from.\n",
    "Each group has $N$ observations in it.\n",
    "\n",
    "$\\text{ComponentDistributions}_i$ is the\n",
    "distribution of component number $i$.\n",
    "There are $K$ components, where `K = len(weights)`.\n",
    "Each is the \"population distribution\" of a group.\n",
    "\n",
    "These distributions typically differ in some of their parameters,\n",
    "$\\lambda$, and might share others, $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To clarify,\n",
    "here's what that model would look like for three total observations from two groups\n",
    "if we were to \"unravel\" the plates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_mixture_plateless_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The plate notation is much cleaner!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The most important special case: mixtures within the same family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But we typically focus on the case where each component comes from the same family of distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 5% `Normal(-6, 1)`\n",
    "- 10% `Normal(2, 1)`\n",
    "- 50% `Normal(0, 1)`\n",
    "- 20% `Normal(-3, 2)`\n",
    "- 15% `Normal(5, 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is a _mixture-of-Gaussians_ or _mixture-of-Normals_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Given a definition, we can plot the distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, normal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### draw a graph and specify its pieces,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_mixture_plate_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "g \\sim \\text{Categorical}(p=[5, 10, 50, 20, 15])\\\\\n",
    "x \\sim \\text{Normal}(\\mu=\\lambda[g], \\sigma=\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### and write a pyMC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as normal_mixture_model:\n",
    "    # First, define each component of the mixture\n",
    "    rv1 = pm.Normal(\"norm1\",\n",
    "                    mu=-6, sd=1)\n",
    "    rv2 = pm.Normal(\"norm2\",\n",
    "                    mu=2, sd=1)\n",
    "    rv3 = pm.Normal(\"norm3\",\n",
    "                    mu=0, sd=1)\n",
    "    rv4 = pm.Normal(\"norm4\",\n",
    "                    mu=-3, sd=2)\n",
    "    rv5 = pm.Normal(\"norm5\",\n",
    "                    mu=5, sd=1)\n",
    "    \n",
    "    # then, put all components in a \"list\"\n",
    "    list_of_components = shared_util.to_pymc([rv1, rv2, rv3, rv4, rv5])\n",
    "    \n",
    "    # then, determine which component is \"active\"\n",
    "    #  on this observation by chance, with p given by weights\n",
    "    component = pm.Categorical(\"component\", p=weights)\n",
    "    # and select it from that list\n",
    "    observed_value = pm.Deterministic(\"observed_value\",\n",
    "                             list_of_components[component])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with normal_mixture_model:\n",
    "    normal_samples = pm.sample(draws=1250, chains=4, target_accept=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(figsize=(12, 12), nrows=2, sharex=True)\n",
    "for var in [\"norm1\", \"norm2\", \"norm3\", \"norm4\", \"norm5\"]:\n",
    "    sns.distplot(normal_samples[var], ax=axs[1]);\n",
    "sns.distplot(normal_samples[\"observed_value\"], kde=False, bins=100, color=\"k\", norm_hist=True, ax=axs[0])\n",
    "utils.plot.plot_mixture_marginal(weights, params, normal, ax=axs[0])\n",
    "axs[1].set_xlim(-10, 10);\n",
    "axs[0].set_title(\"Mixture Distribution, pyMC Estimate and Actual\")\n",
    "axs[1].set_title(\"Mixture Components, pyMC Estimates\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can do the same with any family of distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, uniform);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: a histogram is a \"mixture-of-uniforms\" of sorts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, exponential);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, f_dist);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mixture models are _very general_, and you've already seen them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Whenever the likelihood component of the model has a parameter that varies\n",
    "discretely based on the value of a grouping variable,\n",
    "we can think of the observed variable as having a _mixture distribution_\n",
    "where each component of the mixture is a different group.\n",
    "\n",
    "We can basically _always do this_:\n",
    "notice the wide variety of shapes we achieved above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `kde` plotted by seaborn as part of `distplot`\n",
    "is a mixture-of-Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A histogram can be thought of as a mixture-of-Uniforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mixture distributions are most useful when\n",
    "the component labels are easy to measure or identify\n",
    "and the pieces are relatively simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Concrete Example: Dog Weight Modeling with Mixture-of-Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's say that dogs come in two breeds: larger ones, called hound dogs,\n",
    "and smaller ones, or lap dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://cdn.akc.org/content/hero/508386698_bigsmall.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pictured: elements from tail of the distribution.\n",
    "From the [American Kennel Club website](https://www.akc.org/expert-advice/lifestyle/why-small-dogs-behave-differently-than-large-dogs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We choose to model the dependence of weight on breed with a `Normal` distribution, aka a Gaussian.\n",
    "\n",
    "That is, if we know the breed, then the likelihood of the weight is `Normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "labels = [\"Hound\", \"Lap\"]\n",
    "weights = [0.5, 0.5]  # assume there are an equal number of hounds and lap dogs\n",
    "params = [(20, 3), (10, 2)]  # hounds are heavier and lap dogs are lighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, normal, xs=np.linspace(5, 30), labels=labels)\n",
    "ax = plt.gca(); ax.legend()\n",
    "ax.set_xlabel(\"Weight of Dog\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as normal_dog_weight_model:\n",
    "    # First, define each component of the mixture\n",
    "    rvs = [pm.Normal(\"hound\", mu=20, sd=3), pm.Normal(\"toy\", mu=10, sd=2)]\n",
    "    \n",
    "    # then, put all components in a \"list\"\n",
    "    list_of_components = shared_util.to_pymc(rvs)\n",
    "    \n",
    "    # then, determine which component is \"active\"\n",
    "    #  on this observation by chance, with p given by weights\n",
    "    component = pm.Categorical(\"component\", p=weights)\n",
    "\n",
    "    # and select it from that list\n",
    "    observed_value = pm.Deterministic(\"observed_value\",\n",
    "                             list_of_components[component])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note the similarity of this model to the models for differences in means\n",
    "that we've been working through.\n",
    "\n",
    "The big difference is that instead of having the measurable values be observed\n",
    "and the parameters unknown,\n",
    "here, because we're working theoretically instead of with data,\n",
    "we know the parameters and the data is unknown.\n",
    "\n",
    "Otherwise, the models are structurally the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Concrete Example: Piazza Post Modeling with Mixture-of-Poissons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Imagine, for the sake of argument,\n",
    "that there exists a class that uses the\n",
    "[Piazza](https://piazza.com) web service to host a forum for discussion about course material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Imagine further that this class has assignments due two days a week,\n",
    "say Monday and Friday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The teacher of the class might observe that the number of posts on a given day\n",
    "seems to vary according to whether an assignment is due.\n",
    "\n",
    "They might choose to model it with a _mixture-of-Poissons_:\n",
    "one `Poisson` with a larger mean for days when assignments are due,\n",
    "and one `Poisson` with a smaller mean for days when assignments are not due."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "labels= [\"Assignment Due\", \"No Assignment Due\"]\n",
    "weights = [2 / 7, 5 / 7]  # 2 days a week assignments are due, 5 days they are not\n",
    "params = [(8,), (1,)]  # when an assignment is due, average number of posts is higher\n",
    "poiss = lambda xs, mu: scipy.stats.poisson(mu).pmf(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The distribution for such a model would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, poiss, xs=np.arange(0, 15), labels=labels)\n",
    "ax = plt.gca(); ax.legend()\n",
    "ax.set_xlabel(\"Number of Piazza Posts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Concrete Example: Effort Modeling with Mixture-of-Binomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is not necessary for the group label to be observable,\n",
    "though it does make things easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If not, then the group label becomes an unobserved variable as well:\n",
    "a _hidden mixture model_ or _latent mixture model_,\n",
    "with the graph below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_mixture_plate_graph(group_observed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we might suspect that there are two kinds of participants in our study:\n",
    "ones who are engaged in the task and will put out a large amount of effort\n",
    "and have a high success rate\n",
    "and ones who are not engaged in the task and will only put out a small amount of effort\n",
    "and have a lower success rate.\n",
    "\n",
    "Building off the model from a few weeks back,\n",
    "we might measure the number of successes on multiple trials of a task,\n",
    "modeled as a binomial,\n",
    "and hypothesize that our participants have differing values of the parameters $N$ and $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "labels = [\"Low Effort\", \"High Effort\"]\n",
    "weights = [0.5, 0.5]  # assume there are an equal number of the two kinds of participants\n",
    "params = [(5, 0.1), (20, 0.6)]  # low effort, low success; high effort, high success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot.comparative_mixture_plot(weights, params, binom, xs=np.arange(0, 20), labels=labels)\n",
    "ax = plt.gca(); ax.legend();\n",
    "ax.set_xlabel(\"Number of Successes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We don't know whether an individual was a \"low effort\" or a \"high effort\" participant directly\n",
    "-- there is no objective way to measure that, unlike dog breed or day of the week.\n",
    "\n",
    "It is something we might be able to infer, by building a model and computing a posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Concrete Example: Attention Experiment with a Mixture-of-Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's return to an old favorite: the `attention` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "atten_df = sns.load_dataset(\"attention\", index_col=0, data_home=Path(\"..\") / \"..\" / \"shared\" / \"data\")\n",
    "print(atten_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this data, subjects completed a task with varying numbers of `solutions` and had their performance `score`d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've already used this data previously,\n",
    "but always by only looking at a pair of groups at a time.\n",
    "We're now ready to look at the `solutions` variable across all three values at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we performed a $t$-test on this dataset, we were implicitly making a mixture-of-Gaussians model\n",
    "where both of the mixture components had the same parameters.\n",
    "\n",
    "When we did Bayesian inference, we allowed the components to have different, unknown parameters.\n",
    "\n",
    "Now, let's see what the model of the data looks like if\n",
    "we use the means and standard deviations observed for each group as our parameters\n",
    "(rather than treating them as unknown, random variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weights = [1 / 3, 1 / 3, 1 / 3]\n",
    "\n",
    "means = atten_df.groupby(\"solutions\")[\"score\"].mean()\n",
    "sds = atten_df.groupby(\"solutions\")[\"score\"].std()\n",
    "params = list(zip(means, sds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "labels = [1, 2, 3]\n",
    "f, axs = utils.plot.comparative_mixture_plot(weights, params, normal, xs=np.linspace(0, 10), labels=labels)\n",
    "axs[1].legend(); axs[1].set_xlabel(\"Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Because we have data in this case,\n",
    "we can compare what we observed to what the model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "labels = [1, 2, 3]\n",
    "f, axs = utils.plot.comparative_mixture_plot(weights, params, normal, xs=np.linspace(0, 10), labels=labels)\n",
    "sns.distplot(atten_df[\"score\"], ax=axs[0], kde=False, norm_hist=True, label=\"Observed\", axlabel=False);\n",
    "axs[0].legend();\n",
    "[sns.distplot(atten_df[\"score\"][atten_df[\"solutions\"] == ii], color=\"C\" + str(ii - 1),\n",
    "              kde=False, norm_hist=True, label=\"Observed \" + str(ii)) for ii in labels];\n",
    "axs[1].legend(); axs[1].set_xlabel(\"Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This suggests that our mixture model is \"not that bad\"\n",
    "-- the components seem to line up with the observations from each group\n",
    "and the overall, marginal distribution looks about right.\n",
    "\n",
    "But what if, rather than applying a single, fixed mixture model to the data,\n",
    "we want to determine which mixture models (which settings of the parameters in a mixture model)\n",
    "are likely or plausible given the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian inference with a mixture distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we peform Bayesian inference on data from a mixture distribution,\n",
    "instead of taking the parameters to be known,\n",
    "we make the parameters random variables\n",
    "(including, now, priors!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.daft.make_mixture_plate_graph(params_known=False, value_observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We call a model that has a mixture distribution a _mixture model_.\n",
    "Though it's not always explicitly stated,\n",
    "most models with discrete components are mixture models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then give the values of the observed variable to the model\n",
    "and compute the posterior over the parameter variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "atten_model = pm.Model()\n",
    "N_groups = len(atten_df[\"solutions\"].unique())\n",
    "\n",
    "atten_grand_mean = atten_df[\"score\"].mean()\n",
    "pooled_sd = atten_df.groupby(\"solutions\")[\"score\"].std().mean()\n",
    "\n",
    "with atten_model:\n",
    "    mus = pm.Normal(\"mus\", mu=atten_grand_mean, sd=10, shape=N_groups)\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1 / pooled_sd)\n",
    "    \n",
    "    score = pm.Normal(\"score\", mu=mus[atten_df[\"solutions\"] - 1], sd=sigma,\n",
    "                          observed=atten_df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice how close this is to the models for differences in means!\n",
    "The only real change is that now we have three (or in general, $K$) groups,\n",
    "instead of two.\n",
    "\n",
    "Plus, we have a new way of thinking about it: as a mixture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The cell below contains an equivalent model to the above,\n",
    "but written more in the style of the mixture distributions in this lecture\n",
    "and less in the style of the models from previous assignments.\n",
    "\n",
    "You should compare the two blocks of code to each other\n",
    "and recognize their equivalence,\n",
    "then compare both to the mixture distributions above.\n",
    "\n",
    "If you want to play with the model or check that it runs,\n",
    "change the cell below from a Markdown cell to a Code cell using the Cell Menu\n",
    "and comment out the first and last lines (<code>\\`\\`\\`python</code> and <code>\\`\\`\\`</code>).\n",
    "This might break some of the downstream analysis code,\n",
    "so make sure to redefine the `atten_model` using the first cell\n",
    "before continuing further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```python\n",
    "atten_model = pm.Model()\n",
    "N_groups = len(atten_df[\"solutions\"].unique())\n",
    "\n",
    "atten_grand_mean = atten_df[\"score\"].mean()\n",
    "pooled_sd = atten_df.groupby(\"solutions\")[\"score\"].std().mean()\n",
    "\n",
    "with atten_model:\n",
    "    # First, define each component of the mixture\n",
    "    mu_0 = pm.Normal(\"mu_0\", mu=atten_grand_mean, sd=10)\n",
    "    mu_1 = pm.Normal(\"mu_1\", mu=atten_grand_mean, sd=10)\n",
    "    mu_2 = pm.Normal(\"mu_2\", mu=atten_grand_mean, sd=10)\n",
    "    \n",
    "    sigma = pm.Exponential(\"sigma\", lam=1 / pooled_sd)\n",
    "    \n",
    "    # then, put all components in a \"list\"\n",
    "    mus = shared_util.to_pymc([mu_0, mu_1, mu_2])\n",
    "    \n",
    "    # then, determine which component is \"active\"\n",
    "    # and select it from that list\n",
    "    \n",
    "    mus_for_each_observation = mus[atten_df[\"solutions\"] - 1]\n",
    "    \n",
    "    score = pm.Normal(\"score\", mu=mus_for_each_observation, sd=sigma,\n",
    "                      observed=atten_df[\"score\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with atten_model:\n",
    "    atten_trace = pm.sample(draws=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`plot_posterior` is our first choice for taking a look at the posterior approximated by `pm.sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(atten_trace,\n",
    "                  figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's somewhat hard to directly compare these values to one another.\n",
    "\n",
    "Let's start by using a new feature of `plot_posterior`:\n",
    "the `ref_val` keyword argument lets us plot a vertical line\n",
    "as a `ref`erence `val`ue to compare each posterior to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case,\n",
    "the value we want to compare to is the _overall mean_,\n",
    "or _grand mean_,\n",
    "which is the mean of our observed variable, the `score`,\n",
    "without taking into account which group an observation came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "atten_grand_mean = atten_df[\"score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(atten_trace, varnames=[\"mus\"],\n",
    "                  figsize=(12, 12), ref_val=atten_grand_mean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With this reference value in place,\n",
    "it's easier to see the information in our posterior:\n",
    "there's weak evidence that groups 0 and 2 have different means from the grand mean,\n",
    "but fairly strong evidence that the two means are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than compare to the grand mean,\n",
    "it's convenient to subtract the grand mean out\n",
    "before we specify our model,\n",
    "so that our parameters are in terms of\n",
    "_differences of the means from the overall mean_,\n",
    "rather than in their original units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "centered_atten_df = atten_df.copy()\n",
    "centered_atten_df[\"score\"] = centered_atten_df[\"score\"] - atten_grand_mean\n",
    "\n",
    "centered_atten_model = pm.Model()\n",
    "N_groups = len(atten_df[\"solutions\"].unique())\n",
    "\n",
    "with centered_atten_model:\n",
    "    mus = pm.Normal(\"mus\", mu=0, sd=10, shape=N_groups)\n",
    "    sigma = pm.Exponential(\"sigma\", lam= 1 / pooled_sd)\n",
    "    \n",
    "    score = pm.Normal(\"score\",\n",
    "                      mu=mus[centered_atten_df[\"solutions\"] - 1], sd=sigma,\n",
    "                      observed=centered_atten_df[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with centered_atten_model:\n",
    "    centered_atten_trace = pm.sample(draws=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(centered_atten_trace, figsize=(12, 12),\n",
    "                  ref_val=[0, 0, 0, pooled_sd]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this posterior, I've included `sigma` and used the `pooled_sd` as the reference value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now, we make judgements based on posterior samples, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "samples = shared_util.samples_to_dataframe(centered_atten_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples[\"mus\"].apply(lambda mus: int(mus[2] > 0)),\n",
    "             kde=False, norm_hist=True, bins=[0, 1, 2],\n",
    "             axlabel=\"mu_2 larger than grand mean\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples[\"mus\"].apply(lambda mus: int(mus[0] > 0)),\n",
    "             kde=False, norm_hist=True, bins=[0, 1, 2],\n",
    "            axlabel=\"mu_0 larger than grand mean\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples[\"mus\"].apply(lambda mus: int(mus[2] > mus[0])),\n",
    "             kde=False, norm_hist=True, bins=[0, 1, 2],\n",
    "             axlabel=\"mu_2 larger than mu_0\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But using binary thresholds like this is somewhat silly:\n",
    "sometimes, the difference between the means can be extremely small,\n",
    "so small that it would be practically meaningless,\n",
    "and yet the claim we are checking would still technically be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.util.get_smallest_difference(samples[\"mus\"], 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This function finds the smallest difference between the mean of groups 2 and 0 that counted\n",
    "for the inference above.\n",
    "I typically find a smallest value around `0.01`.\n",
    "This is quite small,\n",
    "given that the differences in the group means we observed were around 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And furthermore, these comparisons are still only pairwise:\n",
    "we're comparing $\\mu_2$ to $\\mu_0$, or either mean to the grand mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Better:\n",
    "if we have some notion of a \"big enough difference to matter\",\n",
    "then we can check whether enough of our posterior\n",
    "samples have a difference that big in _any_ of the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def max_absolute_difference(mus):\n",
    "    return np.max(np.abs(mus - np.mean(mus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples[\"mus\"].apply(max_absolute_difference), axlabel=\"Biggest Absolute Difference\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That is, we check whether the value of this absolute difference is at least `0.1`, or some other threshold.\n",
    "\n",
    "Importantly: this is the _difference in the means_, not the differences in the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's more common to use the squared difference,\n",
    "because it's connected to the variance and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def max_squared_difference(mus):\n",
    "    return np.max(np.square(mus - np.mean(mus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples[\"mus\"].apply(max_squared_difference),\n",
    "             axlabel=\"Max Squared Difference of Means\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But if we take the maximum of the squared differences,\n",
    "then the number we measure would only get bigger as we looked at more groups,\n",
    "even if there were no effect.\n",
    "\n",
    "So instead, we can look at the _average squared difference_,\n",
    "which is the same as the _variance_ of the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_difference(mus):\n",
    "    return np.var(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples[\"mus\"].apply(mean_squared_difference),\n",
    "             axlabel=\"Mean Squared Difference of Means\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But this still doesn't give us a sense of scale:\n",
    "is `1.0` large? Is `0.01` small?\n",
    "We might know this from our application,\n",
    "but it is nice to be able to get a normalized value.\n",
    "\n",
    "If we divide by the variance of the means by the variance of the data,\n",
    "we get a value that is between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def normalized_mean_squared_difference(mus):\n",
    "    return np.var(mus) / np.var(atten_df[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Quick explanation why: the two numbers we are dividing are positive, so the value can't be negative.\n",
    "The number on the bottom, the variance of the overall distribution, aka the variance of the mixture distribution,\n",
    "can't be larger than the variance of the means.\n",
    "\n",
    "Consider the most extreme cases:\n",
    "- If the top number is 0, because all the means are the same,\n",
    "then the value is 0. Therefore this ratio is at least 0.\n",
    "- We can make the variance of the data smaller by making the variance of each component smaller,\n",
    "but even if we make the width of each component infinitely thin,\n",
    "the overall variance of the data won't be 0,\n",
    "since there is still variability due to the group being different for different observations.\n",
    "Therefore the bottom number is at least as big as the top number,\n",
    "so the ratio is no more than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples[\"mus\"].apply(normalized_mean_squared_difference),\n",
    "             axlabel=\"Squared Difference in Means as Fraction of Variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is connected to a statistic called the _variance explained_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mixture distribution-based models are also called categorical effects models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Whenever the parameters of one variable depend on another grouping variable, like `\"solutions\"` above,\n",
    "we say that there is a _categorical effect_:\n",
    "\n",
    "the _category_ or _group_ that an observation comes from has an _effect_ on the distribution we choose to model the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The terminology implies, falsely, that finding a categorical effect establishes a \"cause-and-effect\" relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Counter-Example: Ice Cream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Say we measured, on each day, the temperature and whether we sold out of ice cream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is reasonable to expect that we will sell out of ice cream more when the temperature is higher,\n",
    "and so if we split up days by whether we sold out of ice cream and then compare temperatures,\n",
    "we'll see a \"categorical effect\", even though the direction of causation is the other way around!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Fundamentally, the problem of determining causation is as much philosophical as mathematical,\n",
    "and philosophically, even Bayesian statistics can't really accomodate causal claims.\n",
    "\n",
    "If you're interested in learning more about how to think about causal claims\n",
    "mathematically and rigorously, instead of the somewhat mushy way it's done in\n",
    "the statistics we're learning here, check out\n",
    "[_The Book of Why_](https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097609/)\n",
    "by Judea Pearl,\n",
    "who was one of the pioneers of the graphical modeling style\n",
    "we've been using, among other things.\n",
    "It's an accessible introduction to this exciting area of statistics research!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The `Normal` case is called an ANOVA model, and is especially important for frequentist approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The particular mixture model we worked with above,\n",
    "with `Normal` components that all have the same variance,\n",
    "is known as an **ANOVA model**\n",
    "because of its association with the **AN**alysis <b>O</b>f **VA**riance method,\n",
    "which combines an ANOVA model with a collection of statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On Wednesday, we will go over the ANOVA model and tests from a frequentist perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is perhaps _the most common_ statistical test in research psychology,\n",
    "and it includes the $t$-test as a special case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## As usual, the Bayesian approach is more flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can justify our choice to use a `Normal` likelihood\n",
    "if we think that many small, independent, unknown factors cause\n",
    "the participants' `score`s to vary relative to the group mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "How recently they had coffee, whether they've participated in a similar study before,\n",
    "how much sleep they've gotten recently,\n",
    "whether and how much they're trying to impress the experimenter, etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But imagine that sometimes, in our attention model,\n",
    "the `score` of a participant is wildly different from what it typically is\n",
    "due to some large, rare effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, a participant might fall asleep on one trial,\n",
    "and so score very low,\n",
    "or they might catch a glimpse of the solution when the experimenter,\n",
    "while doing their make-up, holds up a small hand mirror,\n",
    "and so score very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The resulting observation is called an _outlier_:\n",
    "it \"lies outside\" the distribution of the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can model outlier-prone data with the `StudentT` distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The go-to choices for likelihoods of data where outliers are a potential problem\n",
    "are the `StudentT` and the `Cauchy`.\n",
    "For the `StudentT`,\n",
    "you'll want to set the value of the `nu` parameter to be `1` or `2`;\n",
    "as that value increases, the shape of the distribution becomes more `Normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "robust_atten_model = pm.Model()\n",
    "N_groups = len(atten_df[\"solutions\"].unique())\n",
    "\n",
    "with robust_atten_model:\n",
    "    mus = pm.Normal(\"mus\", mu=0, sd=10, shape=N_groups)\n",
    "    sigma = pm.Exponential(\"sigma\", lam= 1 / pooled_sd)\n",
    "    \n",
    "    score = pm.StudentT(\"score\", nu=1,\n",
    "                      mu=mus[centered_atten_df[\"solutions\"] - 1], sd=sigma,\n",
    "                      observed=centered_atten_df[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with robust_atten_model:\n",
    "    robust_atten_trace = pm.sample(draws=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(robust_atten_trace, figsize=(12, 12),\n",
    "                  ref_val=[0, 0, 0, pooled_sd]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use MAP estimation to get a single best guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When we want a single \"best guess\" out of a Bayesian model,\n",
    "we use the method of _maximum a posteriori_ inference, or MAP inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This method seeks to select the values of the parameters, $\\theta$,\n",
    "that make the posterior (log-)probability as large as possible:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p(\\lambda\\vert\\text{data}) \\propto p(\\text{data}\\vert\\lambda) p(\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with centered_atten_model:\n",
    "    MAP_estimates = pm.find_MAP(start=centered_atten_trace[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "MAP_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## With a best guess for the parameters, we can make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The best test of a scientific model\n",
    "is whether it can predict new observations.\n",
    "\n",
    "The second best is to check how well it does predicting existing observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can ask our model to predict the `score`s of new participants\n",
    "based only on how many `solutions` the problem they are attempting has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a `Normal` mixture model, the natural prediction to make is the value of `mu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = MAP_estimates[\"mus\"][centered_atten_df[\"solutions\"] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.array(centered_atten_df[\"solutions\"]), predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And we can quantify the quality of our predictions by checking the averaged squared error,\n",
    "or squared difference between our prediction and what we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "average_prediction_error = np.mean(np.square(centered_atten_df[\"score\"] - predictions))\n",
    "average_prediction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is this good performance? The answer is unclear.\n",
    "\n",
    "Let's compare the prediction performance of a very silly model:\n",
    "one that always predicts that the participant will score the average,\n",
    "regardless of group.\n",
    "\n",
    "This is our \"baseline\" model:\n",
    "it is a point of comparison for all other models.\n",
    "It's what we would have to do if we didn't know what the value of `solutions` was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "baseline_predictions = [0] * len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "average_baseline_prediction_error = np.mean(np.square(centered_atten_df[\"score\"] - baseline_predictions))\n",
    "average_baseline_prediction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The error of the baseline model is larger, which is a good sign.\n",
    "\n",
    "It is typical to compare the prediction error of the baseline model\n",
    "and the actual model by a ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "1 - average_prediction_error / average_baseline_prediction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is the same quantity computed by the `normalized_mean_squared_difference` function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "normalized_mean_squared_difference(MAP_estimates[\"mus\"])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
