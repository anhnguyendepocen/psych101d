{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Multi-way Modeling 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def make_plot(mus, ax=None, **plot_kwargs):\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(figsize=(12, 6))\n",
    "    xs = np.arange(mus.shape[0])\n",
    "    ax.plot(xs, mus[:, 0], lw=4, color=\"C0\", **plot_kwargs)\n",
    "    ax.plot(xs, mus[:, 1], lw=4, color=\"C1\", **plot_kwargs)\n",
    "\n",
    "    \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def make_line(color, linewidth=4):\n",
    "    return Line2D([0], [0], linewidth=linewidth, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Previously, we worked with data that varied only along one category, or factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example:\n",
    "- how does a participant's performance on a task vary depending on whether they are given caffeine or not?\n",
    "- how do a participant's brain activity patterns vary with the type of music they are listening to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In these lectures, we will learn how to work with data that varies along two or more categories, which might _interact_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example:\n",
    "- how do whether a participant drinks caffeine and whether they're over 40 interact to determine their performance on a task?\n",
    "- if we show a person movies inside an fMRI machine, how do both the sound and the image being presented together determine the pattern of activity in their brain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's look for interactions in some real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First, a toy dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I ran a version of the \"accuracy\" experiment described in the other set of slides:\n",
    "with my left and right eyes open and closed,\n",
    "I threw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "toss_data = pd.DataFrame({\"success\":\n",
    "                          [1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "                           0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
    "                           1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
    "                           1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              \"left_eye_closed\": [0] * 10 + [0] * 10 + [1] * 10 + [1] * 10,\n",
    "              \"right_eye_closed\": [0] * 10 + [1] * 10 + [0] * 10 + [1] * 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(toss_data.groupby([\"left_eye_closed\", \"right_eye_closed\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.pointplot(x=\"left_eye_closed\", y=\"success\", hue=\"right_eye_closed\",\n",
    "              data=toss_data, scale=2, errwidth=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as accuracy_model:\n",
    "    accuracies = pm.Uniform(\"accuracies\", shape=(2, 2))\n",
    "    \n",
    "    successes = pm.Bernoulli(\n",
    "        \"successes\",\n",
    "        p=accuracies[toss_data[\"right_eye_closed\"], toss_data[\"left_eye_closed\"]],\n",
    "        observed=toss_data[\"success\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with accuracy_model:\n",
    "    accuracy_trace = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_df = shared_util.samples_to_dataframe(accuracy_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(accuracy_trace, figsize=(12, 12), text_size=16, ref_val=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_interaction_effect(accuracies):\n",
    "    baseline = accuracies[0, 0]\n",
    "    delta_left_eye = accuracies[0, 1] - baseline\n",
    "    delta_right_eye = accuracies[1, 0] - baseline\n",
    "    \n",
    "    prediction_from_separate = baseline + delta_left_eye + delta_right_eye\n",
    "    actually_observed_value = accuracies[1, 1]\n",
    "    \n",
    "    return actually_observed_value - prediction_from_separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interaction_effect_posterior = accuracy_df[\"accuracies\"].apply(compute_interaction_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(interaction_effect_posterior, label=\"posterior\",\n",
    "             axlabel=\"Interaction Effect of Closing Left and Right Eye\");\n",
    "ax.hlines(0, *pm.stats.hpd(interaction_effect_posterior), lw=6)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(interaction_effect_posterior > 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Important: we don't need to just think about differences in means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Any difference in parameters across multiple groups,\n",
    "e.g. the difference in `p` above,\n",
    "can be examined in a multi-way model,\n",
    "not just differences in means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example,\n",
    "we could look at differences in standard deviations,\n",
    "or in the `p` parameter of a `ZeroInflatedPoisson`,\n",
    "across multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now, a dataset from a real psychological experiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll be using some EEG experiment data graciously provided by the [Voytek lab](http://voyteklab.com/about-us/) of UCSD. Participants of varying ages were asked to perform a working memory task with varying levels of difficulty. The raw EEG signal has been summarized into the following two measures:\n",
    "\n",
    "* [Contralateral Delay Activity](https://www.ncbi.nlm.nih.gov/pubmed/26802451), or CDA, is used to measure the engagement of visual working memory.\n",
    "\n",
    "* [Frontal Midline Theta](https://www.ncbi.nlm.nih.gov/pubmed/9895201) oscillation amplitude has been correlated with sustained, internally-directed cognitive activity.\n",
    "\n",
    "The performance of the subjects has also been summarized using the measure\n",
    "[d'](https://en.wikipedia.org/wiki/Sensitivity_index) (pronounced \"d-prime\"),\n",
    "also known as the *sensitivity index*.\n",
    "d' is a measure of the subject's performance in a task,\n",
    "based on the \"linear signal model\" we've looked at previously.\n",
    "\n",
    "In this lecture, we will look at `d`, the subject performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shared_data_path = Path(\"..\") / \"..\" / \"shared\" / 'data'\n",
    "\n",
    "df = pd.read_csv(shared_data_path / 'voytek_working_memory_aging_split.csv', index_col=None)\n",
    "\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(df.groupby(\"group\")[\"age\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(df[\"d\"], ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we've been doing,\n",
    "we'd like to try breaking this data down\n",
    "by thinking of it as a mixture distribution,\n",
    "using the categories as the mixture elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# If we split this data up by one variable at a time, we know what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have a frequentist approach, based on one-way $F$ tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.violinplot(y=\"d\", x=\"group\", data=df, ax=ax, linewidth=4, width=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby([\"group\"])[\"d\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scipy.stats.f_oneway(df[\"d\"][df[\"group\"] == 1],\n",
    "                     df[\"d\"][df[\"group\"] == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This approach works with more than two groups,\n",
    "unlike the first approach we considered, the $t$-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax, = plt.subplots(figsize=(12, 6))\n",
    "sns.violinplot(y=\"d\", x=\"difficulty\", data=df, ax=ax, linewidth=4, width=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby([\"difficulty\"])[\"d\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scipy.stats.f_oneway(df[\"d\"][df[\"difficulty\"] == 1],\n",
    "                     df[\"d\"][df[\"difficulty\"] == 2],\n",
    "                     df[\"d\"][df[\"difficulty\"] == 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We've already worked with one-way models in pyMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's simplify and format our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "data[\"age_group\"] = df[\"group\"] - 1  # subtract 1 so that it starts from 0, like Python indexing\n",
    "data[\"difficulty\"] = df[\"difficulty\"] - 1\n",
    "# watch out: we can't use boolean Series as our indexers!\n",
    "\n",
    "data[\"d\"] = df[\"d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## For a one-way model, we define something like a list of parameters, then index into that list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\mu \\sim \\text{Normal}(0, 1\\mathrm{e}6, \\text{shape}=3)\\\\\n",
    "\\sigma \\sim \\text{Exponential}(0.1)\\\\\n",
    "d \\sim \\text{Normal}(\\mu[i], \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where $i$ is a variable that indexes into the \"list\" $\\mu$,\n",
    "selecting out the appropriate mean for each datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "difficulty_indexer = data[\"difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as eeg_difficulty_model:\n",
    "    means = pm.Normal(\"mus\", mu=0, sd=1e6, shape=3)\n",
    "    sd = pm.Exponential(\"sigma\", lam=0.1)\n",
    "    \n",
    "    observations = pm.Normal(\"d\", mu=means[difficulty_indexer], sd=sd, observed=data[\"d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In class, we won't sample from and work with these models,\n",
    "since we've already seen them,\n",
    "but feel free to add cells and look at the posteriors in your copy of the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For a different one-way model, we define a different indexer and change the shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\mu[j] \\sim \\text{Normal}(0, 1\\mathrm{e}6, \\text{shape}=2)\\\\\n",
    "\\sigma \\sim \\text{Exponential}(0.1)\\\\\n",
    "d \\sim \\text{Normal}(\\mu[j], \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "age_indexer = data[\"age_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as eeg_age_model:\n",
    "    means = pm.Normal(\"mus\", mu=0, sd=1e6, shape=2)\n",
    "    sd = pm.Exponential(\"sigma\", lam=1)\n",
    "    \n",
    "    observations = pm.Normal(\"d\", mu=means[age_indexer], sd=sd, observed=data[\"d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# It's natural to consider our  data in terms of multiple factors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax, = plt.subplots(figsize=(12, 6))\n",
    "sns.violinplot(y=\"d\", x=\"difficulty\", hue=\"group\", data=df, ax=ax, linewidth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `violinplot` shows the entire distribution,\n",
    "which can be useful for noticing severe departures from normality,\n",
    "but if we just want to compare the means,\n",
    "a `pointplot` works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax, = plt.subplots(figsize=(12, 6))\n",
    "sns.pointplot(y=\"d\", x=\"difficulty\", hue=\"group\", data=df, ax=ax, scale=2, errwidth=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is a hint of a different slope here,\n",
    "suggesting there might be an interaction effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Both of these plots are effectively a \"double `groupby`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby([\"group\", \"difficulty\"])[\"d\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ... but then we need to update our inference procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`scipy` does not provide functions for performing statistical tests\n",
    "about multiple factors at once:\n",
    "hence the `one` in `f_oneway`.\n",
    "\n",
    "Later,\n",
    "we'll see how this is done using a different Python library,\n",
    "`statsmodels`.\n",
    "\n",
    "The fact that the analytical statistical testing approach requires a new library\n",
    "is another sign of its inflexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We've already been working around this problem.\n",
    "The `attention` dataset also has multiple possible grouping factors:\n",
    "the `attention` column and the `solutions` column.\n",
    "Previously, we either ignored one column\n",
    "or looked at only rows where one of the two was fixed.\n",
    "\n",
    "But we'll get a more complete understanding of our data\n",
    "if we include all of the factors we measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In pyMC, multi-way models are only a small adjustment: we define something like a list-of-lists for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, we have one parameter for each combination of all factors.\n",
    "\n",
    "$$\n",
    "d \\sim \\text{Normal}(\\mu[i, j], \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as eeg_combined_model:\n",
    "    means = pm.Normal(\"mus\", mu=0, sd=1e6, shape=(3, 2))\n",
    "    sigma = pm.Exponential(\"sigma\", lam=0.1)\n",
    "    \n",
    "    observations = pm.Normal(\"d\", mu=means[difficulty_indexer, age_indexer],\n",
    "                             sd=sigma, observed=data[\"d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Technically, `means` is not a list-of-lists,\n",
    "it's a specific type of list-of-lists called a `Tensor`\n",
    "(even more accurately, a `TensorVariable`)\n",
    "from the `theano` library.\n",
    "\n",
    "It is also a `FreeRV`, just like a particular chihuahua is a `Dog` and also a `Mammal` and a `Pet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import theano.tensor as tt\n",
    "\n",
    "isinstance(means, tt.TensorVariable), isinstance(means, pm.model.FreeRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with eeg_combined_model:\n",
    "    eeg_combined_trace = pm.sample(draws=1000)\n",
    "    eeg_combined_posterior = shared_util.samples_to_dataframe(eeg_combined_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(eeg_combined_posterior.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eeg_combined_posterior[\"mus\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Each sample contains a mean for each combination of age group (column)\n",
    "and task difficulty (row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## As always, the first move is to visualize our posterior,\n",
    "\n",
    "ideally in a manner similar to how we visualized our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "[make_plot(eeg_combined_posterior.iloc[ii][\"mus\"], ax=ax, alpha=0.05)\n",
    " # posterior, plotted transparently\n",
    " for ii in random.sample(range(len(eeg_combined_posterior)), 200)];\n",
    "ax.set_xticks([0, 1, 2]); ax.set_xlabel(\"Difficulty Index\");\n",
    "ax.set_ylabel(\"Group Average d\")\n",
    "sns.pointplot(x=\"difficulty\", y=\"d\", hue=\"age_group\",  # real data\n",
    "              data=data, ax=ax, axlabel=False, scale=2, errwidth=6); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice how the slope of the line connecting difficulty 0 to difficulty 2 looks slightly steeper\n",
    "for the yellow lines (the old age group)\n",
    "than for the blue lines (the young age group)?\n",
    "\n",
    "That suggests there is an interaction:\n",
    "one way to phrase it is that the harder tasks are even harder for the older age group\n",
    "than for the younger age group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "One thing that makes multi-way models harder is that the claims we are interested in\n",
    "are not directly present in the group means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That is, to get at the things we find interesting,\n",
    "we typically need to `apply` some Python functions to the entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We then compute the effects of factors from the entries of the `mu` array on each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def compute_delta_age_easy(mus):\n",
    "    young_easy = mus[0, 0]\n",
    "    old_easy = mus[0, 1]\n",
    "    return old_easy - young_easy\n",
    "\n",
    "def compute_delta_age_hard(mus):\n",
    "    young_hard = mus[2, 0]\n",
    "    old_hard = mus[2, 1]\n",
    "    return old_hard - young_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(eeg_combined_posterior[\"mus\"].apply(compute_delta_age_easy), label=\"Easy Task\")\n",
    "sns.distplot(eeg_combined_posterior[\"mus\"].apply(compute_delta_age_hard), label=\"Hard Task\",\n",
    "             axlabel=\"Effect of Age on Performance\"); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There appears to be an effect of age on performance only when the task is hard: an interaction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_interaction_effect(mus):\n",
    "    return compute_delta_age_hard(mus) - compute_delta_age_easy(mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(\n",
    "    eeg_combined_posterior[\"mus\"].apply(compute_interaction_effect),\n",
    "    axlabel=\"Interaction Effect of Age and Difficulty\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other approaches that we might compare our results to are closer to MAP estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Maximum A Posteriori_ estimation means estimating the true values of the parameters by _maximizing_ the output of the _posterior_ as a function of the parameters, $\\theta$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p\\left(\\theta \\vert \\text{data}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Treat the above as a function of $\\theta$,\n",
    "the values of all of the parameters,\n",
    "and make it as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remember what `pyMC` uses to compute this value (up to a proportion that doesn't depend on the parameters:\n",
    "\n",
    "$$\n",
    "p\\left(\\theta \\vert \\text{data}\\right) \\propto p\\left(\\text{data} \\vert \\theta\\right) p(\\theta)\n",
    "$$\n",
    "\n",
    "that is, using the likelihood of the data given the parameters and the prior probability of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "MAP_estimates = pm.find_MAP(model=eeg_combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "MAP_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "[make_plot(eeg_combined_posterior.iloc[ii][\"mus\"], ax=ax, alpha=0.05)\n",
    " for ii in random.sample(range(len(eeg_combined_posterior)), 200)];\n",
    "ax.set_xticks([0, 1, 2]); ax.set_xlim([-0.5, 2.5]); ax.set_xlabel(\"Difficulty Index\");\n",
    "ax.set_ylabel(\"Group Average d\")\n",
    "\n",
    "ax.plot(MAP_estimates[\"mus\"], lw=6)\n",
    "ax.legend([make_line(\"C0\"), make_line(\"C1\")], [\"Young MAP\", \"Old MAP\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In many, but not all, cases,\n",
    "the parameters are close to the average values of the parameters\n",
    "under the posterior,\n",
    "as they are here.\n",
    "\n",
    "MAP estimation is often done when \"fully Bayesian\" inference,\n",
    "as we've been doing with `pyMC`,\n",
    "is intractable.\n",
    "Many machine learning algorithms\n",
    "that are important in data science and psychology,\n",
    "like neural networks trained by gradient descent\n",
    "or any of the various flavors of linear or logistic regression,\n",
    "can be framed as MAP estimation\n",
    "in some (implicit) probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Any function that we can apply to individual posterior samples\n",
    "can also be applied to our MAP estimates,\n",
    "since a MAP estimate and a sample are both just possible values of the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compute_delta_age_easy(MAP_estimates[\"mus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "compute_delta_age_hard(MAP_estimates[\"mus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "compute_interaction_effect(MAP_estimates[\"mus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The result is often the most likely value of the function being computed,\n",
    "under the posterior, but not always:\n",
    "if the function being used maps multiple inputs to the same output,\n",
    "then there can be a difference.\n",
    "\n",
    "In that case, checking whether the MAP estimate also maximizes the probability\n",
    "of a given statistic is much harder,\n",
    "but for the case of these factor effects, it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We could alternatively have written our model directly in terms of the effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{baseline} = \\mu[0, 0] \\\\\n",
    "\\Delta_\\text{old age} = \\mu[0, 1] - \\text{baseline} \\\\\n",
    "\\Delta_\\text{hard difficulty} = \\mu[2, 0] - \\text{baseline} \\\\\n",
    "\\Delta_\\text{interact old and hard} = \\mu[2, 1] - (\\text{baseline} + \\Delta_\\text{old age} + \\Delta_\\text{hard difficulty})\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{d} \\sim \\text{Normal}(\\text{baseline}\n",
    "+ \\text{is_young}\\cdot\\text{is_easy}\\cdot\\Delta_\\text{old easy}\n",
    "+ \\text{is_young}\\cdot\\text{is_hard}\\cdot\\Delta_\\text{young hard}\n",
    "+ \\text{is_young}\\cdot\\text{is_hard}\\cdot\\Delta_\\text{old hard},\n",
    "\\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\text{is_old}$ is a binary variable that records whether the participant is in the older age group. $\\text{is_hard}$ is a binary variable that records whether the task was in the hard difficulty group.\n",
    "\n",
    "$\\text{is_old_and_hard}$ is a binary variable that records whether both of the above are true (aka `and`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "eeg_interaction_subset = data[data[\"difficulty\"] != 1]\n",
    "\n",
    "is_old = eeg_interaction_subset[\"age_group\"] == 1\n",
    "is_hard = eeg_interaction_subset[\"difficulty\"] == 2\n",
    "is_old_hard = is_old & is_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as linear_model:\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    \n",
    "    baseline = pm.Normal(\"baseline\", mu=0, sd=1e1)\n",
    "    \n",
    "    delta_old_easy = pm.Normal(\"delta_old_easy\", mu=0, sd=1e1)\n",
    "    delta_hard_young = pm.Normal(\"delta_hard_young\", mu=0, sd=1e1)\n",
    "    \n",
    "    interact_old_hard = pm.Normal(\"interact_old_hard\", mu=0, sd=1e1)\n",
    "    \n",
    "    d = pm.Normal(\"d\",\n",
    "                  mu=baseline\n",
    "                  + delta_old_easy * is_old\n",
    "                  + delta_hard_young * is_hard\n",
    "                  + interact_old_hard * is_old_hard,\n",
    "                  sd=sigma,\n",
    "                  observed=eeg_interaction_subset[\"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "linear_model_trace = shared_util.sample_from(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(linear_model_trace, figsize=(12, 12), text_size=16,\n",
    "                  varnames=[\"baseline\", \"delta_old_easy\",\n",
    "                            \"delta_hard_young\", \"interact_old_hard\"],\n",
    "                  ref_val=[4.5, 0, 0, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of this approach is that the random variables we define are more closely related to the inferences we are trying to draw.\n",
    "\n",
    "The disadvantage is that these models get very complicated very quickly as we add more factors.\n",
    "They are tedious to specify by hand, especially inside a `pyMC` model.\n",
    "It's usually easier to define the quantities we want to infer afterwards,\n",
    "as Python functions we apply to samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This style of writing a model is called a _linear model_,\n",
    "because the means, as a function of the data, look like\n",
    "$$y=m\\cdot x +b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, for the datapoints from the young participants doing the hard task,\n",
    "we have that the mean performance is\n",
    "\n",
    "$$\n",
    "\\hat{d} = \\Delta_\\text{hard} \\cdot \\text{is_hard} + \\text{baseline}\n",
    "$$\n",
    "\n",
    "where $\\hat{d}$, the mean, is our \"dependent variable\" $y$,\n",
    "$\\text{is_hard}$ is the \"independent variable\" $x$,\n",
    "$\\Delta_\\text{hard}$ is the \"slope\" $m$,\n",
    "and $\\text{baseline}$ is the \"intercept\" $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the other advantage to this way of specifying models is that\n",
    "we can more easily relate these kinds of models to a broad array of models called\n",
    "_generalized linear models_ or _GLM_s.\n",
    "\n",
    "Next week, we'll look at a different kind of basic linear model,\n",
    "a _linear regression model_,\n",
    "and then see how the GLM framework includes both,\n",
    "along with lots of other kinds of models."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
