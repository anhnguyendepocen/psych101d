{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Regression 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.7)\n",
    "\n",
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def datalikelihood_scatter(ys, xs, intercept, slope, vmin=-100, vmax=0):\n",
    "    lls = compute_normal_ll(ys, xs, slope, intercept, sigma=1)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(10, 8));\n",
    "\n",
    "    h = ax.scatter(xs, ys, c=lls, vmin=vmin, vmax=vmax);\n",
    "    \n",
    "    xs = np.linspace(60, 75);\n",
    "    ax.plot(xs, predict_height(xs, slope, intercept), lw=4, color=\"k\");\n",
    "    \n",
    "    ax.set_xlabel(\"midparental_height\")\n",
    "    ax.set_ylabel(\"height\"); cb = plt.colorbar(h);\n",
    "    cb.ax.set_title(\"Log-Likelihood\\n\");\n",
    "    total_ll = int(np.round(sum(lls)))\n",
    "    ax.set_title(f\"Total LL = {total_ll}\");\n",
    "    \n",
    "\n",
    "def perturb_parameters(slope, intercept):\n",
    "    return slope + np.random.standard_normal() * 0.01, intercept + np.random.standard_normal() * 0.5\n",
    "\n",
    "\n",
    "def predict_height(midparental_heights, slope, intercept):\n",
    "    return slope * midparental_heights + intercept\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d  # noqa: F401\n",
    "\n",
    "\n",
    "def datalikelihood_surface(compute_ll, xrange=(60, 75), yrange=(55, 80), num=20):\n",
    "    xs = np.linspace(*xrange, num=num)\n",
    "    ys = np.linspace(*yrange, num=num)\n",
    "\n",
    "    Xs, Ys = np.meshgrid(xs, ys)\n",
    "    \n",
    "    Zs = compute_ll(Ys, Xs)\n",
    "    \n",
    "    _ = plt.figure(figsize=(8, 8)); ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(Xs, Ys, Zs, cmap=\"hot\")\n",
    "    \n",
    "    ax.scatter(df[\"midparental_height\"], df[\"height\"], zs=20, c=\"b\");\n",
    "    ax.set_xlabel(\"midparental_height\")\n",
    "    ax.set_ylabel(\"height\")\n",
    "    ax.set_zlabel(\"Log-Likelihood\");# ax.set_zlim([-100, 0]);\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def annotate_normal_likelihood(ax):\n",
    "    ax.set_xlabel(\"Observed Value\"); ax.set_ylabel(\"Likelihood\\nUnder Normal\");\n",
    "    ax.annotate(\"Prediction\\n$=m\\cdot x + b$\", xy=(0, 0.4), xytext=(0.5, 0.5),\n",
    "                arrowprops=dict(facecolor='black')); ax.set_ylim(0, 1);\n",
    "    ax.set_xticks([]); \n",
    "\n",
    "    \n",
    "def annotate_normal_log(ax):\n",
    "    ax.set_xlabel(\"Observed Value\"); ax.set_ylabel(\"Log-Probability\\nUnder Normal\");\n",
    "    ax.annotate(\"Prediction\\n$=m\\cdot x + b$\", xy=(0, -0.75), xytext=(0.5, 0),\n",
    "                arrowprops=dict(facecolor='black'));\n",
    "    ax.set_ylim(-6, 2); ax.set_xticks([]); plt.tight_layout()\n",
    "\n",
    "    \n",
    "def annotate_normal_ll(ax):\n",
    "    ax.set_xlabel(\"Prediction$=m\\cdot x + b$\"); ax.set_ylabel(\"Log-Likelihood\\nUnder Normal\");\n",
    "    ax.annotate(\"Observed Value\", xy=(observed_value, -0.75), xytext=(observed_value + 0.5, 0),\n",
    "                arrowprops=dict(facecolor='black'));\n",
    "    ax.set_ylim(-6, 2); ax.set_xticks([]); \n",
    "\n",
    "def make_yerr(predictions, observations):\n",
    "    errors = predictions - observations\n",
    "    positive_errors = np.where(errors>0, errors, 0)\n",
    "    negative_errors = np.where(errors<0, -errors, 0)\n",
    "    return np.stack([positive_errors, negative_errors])\n",
    "\n",
    "def make_error_plot(slope, intercept, observation_df):\n",
    "    f, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.scatter(observation_df[\"midparental_height\"], observation_df[\"height\"], s=144, label=\"observations\")\n",
    "    xs = np.linspace(62, 72)\n",
    "    ax.plot(xs, predict_height(xs, slope, intercept),\n",
    "            lw=4, color=\"k\", label=\"prediction function\");\n",
    "\n",
    "    predictions =  predict_height(observation_df[\"midparental_height\"], slope, intercept)\n",
    "    yerr = make_yerr(predictions, small_sample[\"height\"])\n",
    "    ax.errorbar(observation_df[\"midparental_height\"], predictions,\n",
    "                yerr=yerr, ecolor=\"r\", elinewidth=4, ls=\"none\", zorder=0, label=\"errors\");\n",
    "    ax.set_ylim(60, 85); ax.legend();\n",
    "    MSE = round(np.sum(np.square(yerr)) / len(observation_df), 1)\n",
    "    ax.set_title(f\"MSE = {MSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression models relate one continuous variable to the parameters of another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For an independent variable $x$ and dependent variable $y$, a regression model takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "y \\sim \\text{Foo}\\left(\\beta= f(x)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where $\\text{Foo}$ is some distribution with parameter $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We then put a prior over the function $f$ that relates the two variables\n",
    "and then obtain a posterior that tells us how the two variables are likely to be related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In an ideal world, we'd put a prior over all possible functions $f$:\n",
    "say, over all Python programs that take in a number and spit out a number.\n",
    "\n",
    "But that's impractical, for any of a number of reasons,\n",
    "and so we work with smaller families of functions:\n",
    "all possible lines, all possible parabolas, etc.\n",
    "\n",
    "We then put priors over the parameters that describe that family of functions\n",
    "(e.g. the intercept and slope in the case of linear regression)\n",
    "instead of directly on the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Choosing \"all possible lines\" as the family of functions is particularly common and useful.\n",
    "\n",
    "When we choose that family, we are doing _linear regression_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression is the most important regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this lecture,\n",
    "we're taking a deep dive into linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$y \\sim \\text{Foo}(\\text{slope}\\cdot x + \\text{intercept}, \\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Among linear regression models, the case where the likelihood is Normal is particularly important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "So we'll in particular (mostly) focus on the case where the likelihood is Normal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$y \\sim \\text{Normal}(\\text{slope}\\cdot x + \\text{intercept}, \\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As with the previous lecture, today we'll work with a famous dataset:\n",
    "Sir Francis Galton's parent-child height dataset ([source](https://doi.org/10.7910/DVN/T0HSJ1)),\n",
    "on which the technique of regression was named and invented\n",
    "([original paper](http://www.stat.ucla.edu/~nchristo/statistics100C/history_regression.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/galton_height.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It contains the heights of a nearly 1000 English individuals, their sex, and the height of both their parents, collected in 1885.\n",
    "\n",
    "Following Galton, we summarize the parental heights by averaging them to obtain a \"`midparental_height`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"midparental_height\", y=\"height\",  data=df, kind=\"hex\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# As always in modeling, our goal is a posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The posterior represents our updated belief about how the two variables relate to one another,\n",
    "once we've observed our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again as always, the posterior has two pieces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\color{green}{p(\\text{slope}, \\text{intercept}, \\sigma \\vert \\text{data})}\n",
    "\\propto \\color{darkgoldenrod}{p(\\text{data} \\vert \\text{slope}, \\text{intercept}, \\sigma)}\n",
    "\\cdot \\color{darkblue}{p(\\text{slope}, \\text{intercept}, \\sigma)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, our\n",
    "\n",
    "$\\color{green}{\\text{updated belief about the plausibility of a given relationship between x and y}}$\n",
    "\n",
    "is proportional to\n",
    "\n",
    "$\\color{darkgoldenrod}{\\text{how likely the data is under that relationship}}$\n",
    "\n",
    "multiplied by\n",
    "\n",
    "$\\color{darkblue}{\\text{how plausible we thought that relationship was before we saw the data}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's first take a look at the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Normal distribution is, famously, shaped like a bell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "xs = np.linspace(-3, 3)\n",
    "ax.plot(xs, scipy.stats.norm(0, 1).pdf(xs), lw=6);\n",
    "annotate_normal_likelihood(ax); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But the logarithm of the Normal distribution has an even simpler shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logarithms of probabilities are often more natural than probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Fundamentally, it is because probabilities interact through _multiplication_:\n",
    "just look at Bayes' Rule.\n",
    "\n",
    "But we naturally think in terms of _addition_.\n",
    "For example space and time are additive -- distances and epochs\n",
    "are measured in terms of differences, not ratios.\n",
    "\n",
    "\n",
    "Logarithms _turn multiplication into addition_,\n",
    "so they make some things in probability more antural.\n",
    "\n",
    "Negative log probabilities are also called _surprises_\n",
    "For more see\n",
    "[this blog post](https://charlesfrye.github.io/stats/2017/11/09/the-surprise-game.html)\n",
    "on expected surprise as a modeling criterion\n",
    "or, for a more intuitive approach,\n",
    "[this blog post](https://charlesfrye.github.io/stats/2016/03/29/info-theory-surprise-entropy.html)\n",
    "on where surprise comes from or and how it relates to information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def normal_log(x, mu, sigma=1):\n",
    "    return np.log(scipy.stats.norm(mu, 1).pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## For example, the logarithm of a Normal distribution is a parabola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The peak of the parabola is at the mean.\n",
    "\n",
    "In our particular case, the mean is the value we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "xs = np.linspace(-3, 3)\n",
    "normal_log_as_function_of_x = normal_log(xs, 0)\n",
    "ax.plot(xs, normal_log_as_function_of_x, lw=6);\n",
    "annotate_normal_log(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice two things:\n",
    "first, the value most probably observed is equal to the prediction,\n",
    "and second, the lots of other values are also quite probable --\n",
    "more so the closer they are to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The \"steepness\" of the parabola and its height also depend on the standard deviation,\n",
    "as we can see if we use the logarithm rules on the logarithm of the Normal distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\log p (x\\vert\\mu,\\sigma) = \\underbrace{-\\log\\left(\\sqrt{2\\pi}\\sigma\\right)}_\\text{uncertainty penalty} -\n",
    "\\underbrace{(x - \\mu)^2 / 2\\sigma^2}_\\text{scaled squared error}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Mathematical details below:\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x\\vert\\mu,\\sigma) &= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\mathrm{e}^{\\frac{-(x-\\mu^2)}{2\\sigma^2}}\\\\\n",
    "\\log p (x\\vert\\mu,\\sigma) &= \\log \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma} \\mathrm{e}^{\\frac{-(x-\\mu^2)}{2\\sigma^2}}\\right)\\\\\n",
    "&= \\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right) + \\log\\left( \\mathrm{e}^{\\frac{-(x-\\mu^2)}{2\\sigma^2}}\\right)\\\\\n",
    "&= -\\log\\left(\\sqrt{2\\pi}\\sigma\\right) -(x - \\mu)^2/2\\sigma^2\\\\\n",
    "&= \\underbrace{-\\log\\left(\\sqrt{2\\pi}\\sigma\\right)}_\\text{lower if spread increases}\n",
    "\\underbrace{-(x - \\mu)^2}_{\\text{higher if mean and value are close}}/ \\underbrace{2\\sigma^2}_\\text{spread controls scale of errors}\\\\\n",
    "&= \\underbrace{-\\log\\left(\\sqrt{2\\pi}\\sigma\\right)}_\\text{uncertainty penalty} -\n",
    "\\underbrace{(x - \\mu)^2 / 2\\sigma^2}_\\text{scaled squared error}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But in general, we can ignore the standard deviation,\n",
    "since it only tells us.\n",
    "\n",
    "As a function of the mean,\n",
    "that is, as a _likelihood_,\n",
    "the shape is also a parabola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.linspace(-3, 3)\n",
    "observed_value = 0.5\n",
    "normal_log_as_function_of_prediction =\\\n",
    "    [normal_log(observed_value, mu) for mu in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(predictions, normal_log_as_function_of_prediction, lw=6);\n",
    "annotate_normal_ll(ax); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the peak of the log-likelihood is located at the observed value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Therefore the Normal likelihood term in our regression model is telling us to minimize the squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a regression model, the mean is the prediction and the \"height\" of the parabola is the negative squared prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "NB: maximizing the negative squared error is the same as minimizing the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def negative_squared_error(observed_value, predicted_value):\n",
    "    return -(observed_value - predicted_value) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we compute the negative squared error,\n",
    "we get the same rough shape,\n",
    "a parabola,\n",
    "up to a y-axis shift and a change in the steepness.\n",
    "\n",
    "And importantly, the maximum is still at the same spot: right at the observed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "errors = np.linspace(-3, 3)\n",
    "ax.plot(errors, - (errors ** 2), lw=4);\n",
    "ax.set_xlabel(\"$y-\\mu$\"); ax.set_ylabel(\"$-(y - \\mu)^2$\"); plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In a real model, we have more than one observed value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To determine the overall log-likelihood of a given choice of the parameters,\n",
    "we have to look at their log-likelihood on each data point.\n",
    "\n",
    "The overall log-likelihood is the sum of all of the individual log-likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def compute_normal_ll(y, x, slope, intercept, sigma=1):\n",
    "    \n",
    "    # u = m * x + b\n",
    "    predictions = slope * x + intercept\n",
    "    \n",
    "    # e = y - u\n",
    "    errors = y - predictions\n",
    "    \n",
    "    # e / s\n",
    "    scaled_errors = errors / (np.sqrt(2) * sigma)\n",
    "    \n",
    "    # (e / s) ^2\n",
    "    scaled_squared_error = scaled_errors ** 2\n",
    "    \n",
    "    uncertainty_penalty = - np.log(np.sqrt(2 * np.pi) * sigma)\n",
    "    \n",
    "    return uncertainty_penalty - scaled_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, let's look at the log-likelihood of a baseline model:\n",
    "one that predicts that the individual's height is average, no matter their parents' heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mean_height = df[\"height\"].mean()\n",
    "compute_ll_baseline = lambda y, x: compute_normal_ll(y, x, 0, mean_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = datalikelihood_surface(compute_ll_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This plot is very dense and rewards close study and interaction\n",
    "-- click and drag to change the camera view to see the various details.\n",
    "\n",
    "My favorite view puts the x- and y- axes in approximately the same spot as they are in\n",
    "the `jointplot`, but at a slight angle so that we can see the 3-D shape of the surface.\n",
    "\n",
    "The x- and y-axes of this plot are the midparental height (the value we are using to predict)\n",
    "and the child's height (the value we are trying to predict).\n",
    "\n",
    "The z-axis is the Normal log-probability,\n",
    "for the given choice of parameters,\n",
    "for that combination of midparental and child heights.\n",
    "To aid in reading the plot,\n",
    "the log-probability is colored by its height on the z-axis.\n",
    "Darker colors mean lower log-probability.\n",
    "\n",
    "The observed data is plotted as well,\n",
    "scattered \"on top\" of the log-probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our analysis above suggests the following:\n",
    "- the maximum value of this log-probability along the y-axis for a fixed x should be where the model predictions are.\n",
    "- along the y-axis, the shape of the surface is a parabola\n",
    "\n",
    "To confirm that you understand what is being visualized, make sure you can see those facets of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To calculate the total log-likelihood of the parameters,\n",
    "we add up the log-probability of all of the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum(compute_ll_baseline(df[\"height\"], df[\"midparental_height\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's compare that log-probability and total log-likelihood to those\n",
    "of a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These parameters are based on the posterior samples in the previous lecture.\n",
    "\n",
    "Below, we'll see how to obtain good parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compute_ll_good = lambda y, x: compute_normal_ll(y, x, 0.7, 20, sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = datalikelihood_surface(compute_ll_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that for these parameters,\n",
    "the brightly-colored region of the log-probability,\n",
    "where the  values are high,\n",
    "now overlaps with most of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we add up the total log-likelihood for this model, we get a much less negative value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum(compute_ll_good(df[\"height\"], df[\"midparental_height\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The log-likelihood is lower for a worse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compute_ll_bad = lambda y, x: compute_normal_ll(y, x, -0.3, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This model is predicting that the child's height should decrease when the parents' height increases,\n",
    "as evidenced by the negative slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = datalikelihood_surface(compute_ll_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For this setting of the parameters,\n",
    "the high values of the log-probability do not overlap with the data as much,\n",
    "and a few of the datapoints are located in very low-probability regions,\n",
    "e.g. where the log-probability is close to -250."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting total log-likelihood of the parameter values is much more negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum(compute_ll_bad(df[\"height\"], df[\"midparental_height\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Changing the parameter to maximize the value of the log-likelihood is known as _maximum likelihood estimation_, or MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maximum likelihood estimation is the core technique for\n",
    "- classical frequentist estimation\n",
    "- much of contemporary machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the latter context,\n",
    "the negative log-probability of the observed data according to the model\n",
    "typically appears as part of the \"loss function\" or \"cost\".\n",
    "\n",
    "This gives a probabilistic and Bayesian interpretation of these algorithms,\n",
    "which can be handy in understanding them,\n",
    "even when they aren't approached with the tools we use in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you take a machine learning, pattern recognition, or AI class\n",
    "in which you \"fit models to data\"\n",
    "by using gradient descent or something similar\n",
    "to minimize a loss,\n",
    "you are most likely doing MAP or MLE\n",
    "without realizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`pyMC` doesn't have a `find_MLE` function directly,\n",
    "but as it turns out,\n",
    "we can still do MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## With the right choice of prior, we can turn MAP inference into maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Recall the posterior was defined only up to a proportionality constant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\color{green}{p(\\text{slope}, \\text{intercept}, \\sigma \\vert \\text{data})}\n",
    "\\propto \\color{darkgoldenrod}{p(\\text{data} \\vert \\text{slope}, \\text{intercept}, \\sigma)}\n",
    "\\cdot \\color{darkblue}{p(\\text{slope}, \\text{intercept}, \\sigma)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we choose a prior where\n",
    "$p(\\text{slope}, \\text{intercept}, \\sigma)$ is constant,\n",
    "then we can simplify to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\color{green}{p(\\text{slope}, \\text{intercept}, \\sigma \\vert \\text{data})}\n",
    "\\propto \\color{darkgoldenrod}{p(\\text{data} \\vert \\text{slope}, \\text{intercept}, \\sigma)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That is, the more likely the data looks given the parameters,\n",
    "the more likely it is that those parameters are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if we apply `find_MAP` to a model with a constant prior,\n",
    "then the result will also maximize the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For continuous variables, the constant priors are the `Flat` and `HalfFlat` priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as ordinary_least_squares:\n",
    "    Intercept = pm.Flat(\"Intercept\")\n",
    "    Slope = pm.Flat(\"Slope\")\n",
    "    \n",
    "    Sigma = 1\n",
    "    \n",
    "    Heights = pm.Normal(\"Heights\",\n",
    "                       mu=Slope * df[\"midparental_height\"] + Intercept,\n",
    "                       sd=Sigma,\n",
    "                       observed=df[\"height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is called the \"ordinary least squares\" model because\n",
    "- maximizing the likelihood means minimizing the squared error\n",
    "- it's the \"ordinary\" or \"typical\" model for frequentists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When most folks say \"linear regression\", this is the model they are thinking of.\n",
    "\n",
    "When they say \"fit a linear regression model\",\n",
    "they mean to do something equivalent to the `find_MAP` call in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "OLS_MAP = pm.find_MAP(model=ordinary_least_squares)\n",
    "\n",
    "MAP_slope, MAP_intercept = OLS_MAP[\"Slope\"], OLS_MAP[\"Intercept\"]\n",
    "OLS_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice the `logp` output in red:\n",
    "for this model with `Flat` priors,\n",
    "this is equal to the log-likelihood.\n",
    "\n",
    "If we feed the correct parameters to `compute_normal_ll`,\n",
    "we'll get the same value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lls = compute_normal_ll(df[\"height\"], df[\"midparental_height\"], MAP_slope, MAP_intercept, sigma=1)\n",
    "np.sum(lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can check the value of the log-posterior probability at any time using a `Model`'s\n",
    "`.logp` method,\n",
    "which takes a dictionary of parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ordinary_least_squares.logp({\"Intercept\": MAP_intercept, \"Slope\": MAP_slope})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The results will be consistent for other choices of the `Slope` and `Intercept`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(ordinary_least_squares.logp({\"Intercept\": mean_height, \"Slope\": 0}))\n",
    "sum(compute_normal_ll(df[\"height\"], df[\"midparental_height\"], 0, mean_height, sigma=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The next visualization is a 2-D version of the information in the 3-D plot above,\n",
    "now with the predictions for a given setting of the parameters added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "slope, intercept = MAP_slope, MAP_intercept\n",
    "## check randomly-chosen parameters close to the MAP\n",
    "# slope, intercept = perturb_parameters(MAP_slope, MAP_intercept)\n",
    "# check \"baseline\" model that always predicts mean\n",
    "slope, intercept = 0, mean_height\n",
    "\n",
    "datalikelihood_scatter(df[\"height\"], df[\"midparental_height\"],\n",
    "                    intercept=intercept,\n",
    "                    slope=slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The predictions appear as a black line over a colored scatter plot of the observed data.\n",
    "\n",
    "Once again, the log-likelihood values correspond to color:\n",
    "brightly-colored datapoints contribute a small negative number to the log-likelihood,\n",
    "while more darkly-colored datapoints contribute larger negative numbers.\n",
    "\n",
    "The total log-likelihood is printed at the top of the chart.\n",
    "\n",
    "You're encouraged to try uncommenting some of the commented lines above to see\n",
    "how the predictions and log-likelihood relate for other choices of the parameters\n",
    "and to try other values and see if you can predict what you will see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# It is typical to standardize data and measures of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Log-probabilities are nice, but the numbers are fairly hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Is -6000 good or bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For this model,\n",
    "it is good relative to the other parameter values we checked,\n",
    "but it's unclear whether it means our model is highly accurate or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We'd like a number for our performance that varies between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where $0$ means \"as bad as a strawman\" and $1$ means \"the best possible\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We start by defining a baseline value for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "baseline_predictions = predict_height(df[\"midparental_height\"],\n",
    "                                      0, mean_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is meant to be something whose performance we can exceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Then, we compute the errors and mean squared error for those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "baseline_errors = baseline_predictions - df[\"height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(baseline_errors ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_MSE(predictions, observed_values):\n",
    "    return ((predictions - observed_values) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### And do the same for the parameters we want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = predict_height(df[\"midparental_height\"],\n",
    "                             MAP_slope, MAP_intercept)\n",
    "errors = predictions - df[\"height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction_MSE(predictions, df[\"height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Then we take a ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction_MSE(predictions, df[\"height\"]) / prediction_MSE(baseline_predictions, df[\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def ratio_of_errors(predictions, baseline_predictions, observed_values):\n",
    "    baseline_MSE = prediction_MSE(baseline_predictions, observed_values)\n",
    "    actual_MSE = prediction_MSE(predictions, observed_values)\n",
    "    \n",
    "    return actual_MSE / baseline_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ratio_of_errors(predictions, baseline_predictions, df[\"height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### If our predictions were perfect, we'd get a ratio of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ratio_of_errors(df[\"height\"], baseline_predictions, df[\"height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### If our predictions were no better than baseline, we'd get a ratio of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ratio_of_errors(baseline_predictions, baseline_predictions, df[\"height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Almost there: if we subtract from 1, then 0 means baseline performance and 1 means perfect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "1 - ratio_of_errors(predictions, baseline_predictions, df[\"height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## This value is known as the _variance explained_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is one way of capturing the fraction of the uncertainty in the data\n",
    "that was \"explained away\" by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unlike the log-probability, it's directly tied to the Normal likelihood,\n",
    "since we used mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "small_sample = df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "make_error_plot(0, mean_height, small_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "make_error_plot(MAP_slope, MAP_intercept, small_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If our errors are zero on average, then the ratio of mean squared errors is equal to the ratio of variances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(np.var(errors), np.var(baseline_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(np.var(errors) / np.var(baseline_errors), 1 - np.var(errors) / np.var(baseline_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The square root of this value is known as the _correlation_, aka _Pearson's $r$_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And hence the value is sometimes called $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can calculate it directly ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(1 - ratio_of_errors(predictions, baseline_predictions, df[\"height\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or use `scipy.stats`, which calls it `pearsonr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(df[\"height\"], df[\"midparental_height\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or use `numpy`, which calls it `corr`elation `coef`ficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.corrcoef(df[\"height\"], df[\"midparental_height\"])[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### If the variance explained by a linear model is not 0, we say the two variables are _correlated_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## It is also common to standardize data before performing any modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Is a midparental height of 72 large or small?\n",
    "\n",
    "We know that it's quite above average because of our experience with the data,\n",
    "but without that context, it's unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we subtract off the mean,\n",
    "then we know that values above 0 are bigger than average\n",
    "and below 0 are smaller than average.\n",
    "\n",
    "If we divide by the standard deviation,\n",
    "then most values will be between ±2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "standardized_heights = standardize(df[\"height\"])\n",
    "\n",
    "standardized_heights.mean(), standardized_heights.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pm.stats.hpd(standardized_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.distplot(standardized_heights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is also known as _$z$-scoring_.\n",
    "\n",
    "Once data is $z$-scored,\n",
    "we can immediately tell whether a value is close to average: it will be close to 0.\n",
    "And we also know the \"scale\" of our data: typical values should be within ±2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standardization doesn't fundamentally change the relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It's equivalent to a change of units,\n",
    "like going from measuring heights in centimeters to inches\n",
    "or switching from measuring temperatures in Fahrenheit to Celsius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can see this in the `jointplot` of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "standardized_midparental_heights = standardize(df[\"midparental_height\"])\n",
    "\n",
    "sns.jointplot(standardized_midparental_heights, standardized_heights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you compare this plot to the original `jointplot`,\n",
    "you'll see that the \"shapes\" haven't changed, only the values on the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standardization doesn't affect the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "correlation = scipy.stats.pearsonr(standardized_heights, standardized_midparental_heights)[0]\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The correlation is also equal to the slope of the MLE regression line for standardized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can define a pyMC model for standardized data with minimal changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as standardized_OLS:\n",
    "    Slope = pm.Flat(\"Slope\")\n",
    "    Intercept = 0  # consider: why is this value 0?\n",
    "    \n",
    "    ObservedValues = pm.Normal(\"ObservedValues\",\n",
    "                               mu=Slope * standardized_midparental_heights + Intercept,\n",
    "                               sd=1,\n",
    "                               observed=standardized_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `Intercept` variable is included only to make the connection between the two models clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "standardized_trace = shared_util.sample_from(standardized_OLS)\n",
    "standardized_posterior_df = shared_util.samples_to_dataframe(standardized_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(standardized_trace, ref_val=correlation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The interpretation of this slope is as follows:\n",
    "\n",
    "> for every increment of the midparental height by one standard deviation, the expected height for their children increased by about a third of a standard deviation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the correlation is close to the center of the posterior.\n",
    "\n",
    "If we use `find_MAP`, the match is even better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "standardized_OLS_MAP = pm.find_MAP(model=standardized_OLS)\n",
    "standardized_OLS_MAP, correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the `logp` is very different, even though the $R^2$ is the same.\n",
    "\n",
    "This is a major issue with using `logp` to compare models:\n",
    "it is very sensitive to irrelevant details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can also calculate the correlation coefficient \"by hand\" and obtain the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "predictions_standardized = predict_height(\n",
    "    standardized_midparental_heights, standardized_OLS_MAP[\"Slope\"], 0)\n",
    "baseline_predictions_standardized = predict_height(\n",
    "    standardized_midparental_heights, 0, 0)\n",
    "\n",
    "np.sqrt(1 - ratio_of_errors(predictions_standardized, baseline_predictions_standardized , standardized_heights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we know when we can ignore the linear relationship between two variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example,\n",
    "Galton's original explanation of his findings presumed that individuals married\n",
    "without respect to height:\n",
    "if not, then the predicted heights of grandchildren might not exhibit \"regression to mediocrity\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's check the data for whether that was a reasonable assumption to make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "standardized_maternal_heights = standardize(df[\"mother\"])\n",
    "standardized_paternal_heights = standardize(df[\"father\"])\n",
    "\n",
    "with pm.Model() as smaller_effect_model:\n",
    "    Slope = pm.Flat(\"Slope\")\n",
    "    ObservedValues = pm.Normal(\"MaternalHeights\",\n",
    "                               mu=Slope * standardized_maternal_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_paternal_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "smaller_effect_trace = shared_util.sample_from(smaller_effect_model)\n",
    "smaller_effect_posterior_df = shared_util.samples_to_dataframe(smaller_effect_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(smaller_effect_trace, ref_val=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It seems that the 95% posterior density does not include 0,\n",
    "so by the method we've been using so far in the class,\n",
    "we'd have to conclude that there's a flaw in Galton's analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standardization also lets us consider the _magnitude_ of a relationship in standard terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The posterior above indicates that we should change our expectation of a mother's height\n",
    "by a bit under a tenth of a standard deviation every time the father gets taller or shorter by a standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a _very small_ effect.\n",
    "\n",
    "For a father who is three standard deviations away from average height,\n",
    "aka someone who is six feet, two inches,\n",
    "we predict a maternal height of less than an inch above average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter the ROPE: Region of Practical Equivalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before running our analysis, we define a set of values, close to 0,\n",
    "which we consider to be _practically equivalent to 0_.\n",
    "\n",
    "#### We call this the Region Of Practical Equivalence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we obtain a posterior, we can check the overlap between our posterior and this region:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For a big effect, this overlap will be small or 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(standardized_trace, ref_val=0, rope=(-0.05, 0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "I selected as my ROPE here -0.05 to 0.05:\n",
    "a correlation is _effectively_ 0 if it suggests\n",
    "that I only need to change my prediction by a factor of 1 in 20 or less\n",
    "when I take into account the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For a small effect, this overlap will be larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(smaller_effect_trace, ref_val=0, rope=(-0.05, 0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As usual, we test the probability we assign to the statement\n",
    "\"the correlation is within the region of practical equivalence\"\n",
    "by checking whether it is true on our posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def is_in_ROPE(sample, ROPE=(-0.05, 0.05)):\n",
    "    return ROPE[0] < sample < ROPE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "smaller_effect_posterior_df[\"Slope\"].apply(is_in_ROPE).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is a fairly decent chance, around 25%,\n",
    "that the correlation between maternal and paternal heights is negligible,\n",
    "according to this definition of the ROPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# In addition to a likelihood, the definition of a regression model includes a prior over the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far today, we've focused on _flat_ priors,\n",
    "which are technically not probability distributions,\n",
    "so that we could connect to more mainstream MLE methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unlike in tests for differences of means,\n",
    "it's actually quite common for regression models to include\n",
    "Bayesian elements, even if they aren't always thought of that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### For example, a common technique called _ridge regression_ is equivalent to placing a Normal prior on the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as ridge_regression_model:\n",
    "    # ridge regresion <> Normal prior on slope\n",
    "    Slope = pm.Normal(\"Slope\", mu=0, sd=2.5e-2)\n",
    "    # This prior says: I think it is very likely that\n",
    "    #  the correlation is between -5e-2 and 3e-2 (and so inside the ROPE)\n",
    "    \n",
    "    \n",
    "    ObservedValues = pm.Normal(\"ObservedValues\",\n",
    "                               mu=Slope * standardized_maternal_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_paternal_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ridge_trace = shared_util.sample_from(ridge_regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(ridge_trace, rope=(-0.05, 0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The overlap between the posterior and the ROPE is now much stronger.\n",
    "\n",
    "But the MAP value is still not 0.\n",
    "\n",
    "See the following section, on LASSO regression, for an indication of how to set a prior that gives MAP estimates that are exactly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pm.find_MAP(model=ridge_regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### With a strong prior that there is no relationship, strong evidence is required to conclude a relationship is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as ridge_regression_model:\n",
    "    Slope = pm.Normal(\"Slope\", mu=0, sd=2.5e-2)\n",
    "    # This prior says: I think it is extremely likely that\n",
    "    #  the correlation is between -5e-2 and 5e-2 (and so inside the ROPE)\n",
    "    \n",
    "    \n",
    "    ObservedValues = pm.Normal(\"ObservedValues\",\n",
    "                               mu=Slope * standardized_midparental_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ridge_trace = shared_util.sample_from(ridge_regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(ridge_trace, rope=(-0.05, 0.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pm.find_MAP(model=ridge_regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Another technique, called LASSO regression, is used to obtain MAP estimates that are exactly 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "LASSO is equivalent to placing a [_Laplace_](https://en.wikipedia.org/wiki/Laplace_distribution)\n",
    "prior on the slope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "xs = np.linspace(-3, 3, num=1000)\n",
    "ax.plot(xs, np.exp(pm.Laplace.dist(mu=0, b=0.1).logp(xs).eval()), lw=4);\n",
    "ax.plot(xs, np.exp(pm.Laplace.dist(mu=0, b=1).logp(xs).eval()), lw=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as lasso_regression_model:\n",
    "    # lasso regression <> Laplace prior on slope\n",
    "    Slope = pm.Laplace(\"Slope\", mu=0, b=0.01)\n",
    "    ObservedValues = pm.Normal(\"ObservedValues\",\n",
    "                               mu=Slope * standardized_maternal_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_paternal_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lasso_trace = shared_util.sample_from(lasso_regression_model, target_accept=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(lasso_trace, rope=(-0.05, 0.05));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pm.find_MAP(model=lasso_regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Try with the standardized height/midparental height data, and you'll see that the MAP estimate is not 0.\n",
    "\n",
    "Using either of the priors specified,\n",
    "we can verify Galton's assumption\n",
    "that the maternal and paternal heights are negligibly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If our data is not `Normal`, we can use a different likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One of the most common causes of non-Normality is _outliers_:\n",
    "large, rare effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, in the `mpg` dataset that you might work with in lab on Friday,\n",
    "there's at least one incorrect data entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Recall how quickly the log-likelihood for `Normal` data dropped:\n",
    "when a data point is far away from the prediction, the log-likelihood suffers immensely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A technique for doing accurate regression in the presence of outliers is called _robust regression_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In a Bayesian model, robust regressions correspond to different choices of likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Specifically, choices of probability distributions with \"heavy tails\",\n",
    "like the `Cauchy`, the `StudentT`, or the `Laplace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as robust_linear_regression:\n",
    "    Slope = pm.Flat(\"Slope\")\n",
    "    Intercept = 0\n",
    "    Beta = 1  # equivalent to Sigma in a Normal\n",
    "    \n",
    "    ObservedValues = pm.Cauchy(\"ObservedValues\",\n",
    "                               alpha=Slope * standardized_midparental_heights + Intercept,\n",
    "                               beta=1,\n",
    "                               observed=standardized_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "robust_trace = shared_util.sample_from(robust_linear_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Because our data doesn't suffer as much from outliers, the results are fairly similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(robust_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Bayesian methods allow our data to tell us whether we need to use a robust method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `StudentT` distribution has a parameter, `nu`, that determines whether it has heavy tails.\n",
    "\n",
    "When `nu` is close to 1, the `StudentT` has very heavy tails;\n",
    "when `nu` is large, say above 20, the `StudentT` looks more like a `Normal`,\n",
    "and no longer has a heavy tail.\n",
    "\n",
    "If we make this parameter part of our model,\n",
    "we can get a posterior over how necessary robust regression is\n",
    "for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as optionally_robust_linear_regression:\n",
    "    Slope = pm.Flat(\"Slope\")\n",
    "    Intercept = 0\n",
    "    # \"Degrees of Freedom\": ~30 means data is normal-ish,\n",
    "    #   under 10 means data has outliers\n",
    "    Nu = pm.DiscreteUniform(\"Nu\", lower=1, upper=30)\n",
    "    Sigma = pm.Exponential(\"Sigma\", lam=1)\n",
    "    \n",
    "    ObservedValues = pm.StudentT(\"ObservedValues\",\n",
    "                                 mu=Slope * standardized_midparental_heights,\n",
    "                                 sd=Sigma,\n",
    "                                 nu=Nu,\n",
    "                                 observed=standardized_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "optionally_robust_trace = shared_util.sample_from(optionally_robust_linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(optionally_robust_trace, figsize=(8, 4), ref_val=[0, 1, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The fact that the posterior for `Nu` is shifted to the right, i.e. close to 30,\n",
    "indicates that there are not substantial outliers in our data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
