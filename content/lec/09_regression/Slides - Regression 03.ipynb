{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Regression 03 - Priors in Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)\n",
    "\n",
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression is the most important regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this lecture,\n",
    "we continue our deep dive into linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$y \\sim \\text{Foo}(\\text{slope}\\cdot x + \\text{intercept}, \\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Among linear regression models, the case where the likelihood is Normal is particularly important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "So we'll in particular (mostly) focus on the case where the likelihood is Normal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$y \\sim \\text{Normal}(\\text{slope}\\cdot x + \\text{intercept}, \\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As with the previous lecture, today we'll work with a famous dataset:\n",
    "Sir Francis Galton's parent-child height dataset ([source](https://doi.org/10.7910/DVN/T0HSJ1)),\n",
    "on which the technique of regression was named and invented\n",
    "([original paper](http://www.stat.ucla.edu/~nchristo/statistics100C/history_regression.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/galton_height.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It contains the heights of a nearly 1000 English individuals, their sex, and the height of both their parents, collected in 1885.\n",
    "\n",
    "Following Galton, we summarize the parental heights by averaging them to obtain a \"`midparental_height`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"midparental_height\", y=\"height\",  data=df, kind=\"hex\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# In regression, data is often standardized with $z$-scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    return (data - data.mean()) / data.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "standardized_midparental_heights = standardize(df[\"midparental_height\"])\n",
    "standardized_maternal_heights = standardize(df[\"mother\"])\n",
    "standardized_paternal_heights = standardize(df[\"father\"])\n",
    "standardized_heights = standardize(df[\"height\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Once standardized with $z$-scoring, the means of all variables are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(0,\n",
    "            [standardized_heights.mean(), standardized_maternal_heights.mean(),\n",
    "             standardized_paternal_heights.mean(), standardized_midparental_heights.mean()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore our intercept can be fixed at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# As always in modeling, we specify a prior and a likelihood in order to obtain a posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The posterior represents our updated belief about how the two variables relate to one another,\n",
    "once we've observed our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\color{green}{p(\\text{slope} \\vert \\text{data})}\n",
    "\\propto \\color{darkgoldenrod}{p(\\text{data} \\vert \\text{slope})}\n",
    "\\cdot \\color{darkblue}{p(\\text{slope})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, our\n",
    "\n",
    "$\\color{green}{\\text{updated belief about the plausibility of a given relationship between x and y}}$\n",
    "\n",
    "is proportional to\n",
    "\n",
    "$\\color{darkgoldenrod}{\\text{how likely the data is under that relationship}}$\n",
    "\n",
    "multiplied by\n",
    "\n",
    "$\\color{darkblue}{\\text{how plausible we thought that relationship was before we saw the data}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# These slides focus on the role of and common choices for the prior in regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the previous lecture, we focused on _flat_ priors,\n",
    "which are technically not probability distributions,\n",
    "so that we could connect to more mainstream MLE methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A linear regression model with a normal likelihood and a flat prior\n",
    "is known as an _ordinary least squares model_\n",
    "because the MAP for the parameters also minimizes the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as galton_model:\n",
    "    Slope = pm.Flat(\"Slope\")\n",
    "    ObservedValues = pm.Normal(\"Heights\",\n",
    "                               mu=Slope * standardized_midparental_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_heights)\n",
    "\n",
    "galton_trace = shared_util.sample_from(galton_model)\n",
    "\n",
    "galton_MLE = pm.find_MAP(model=galton_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unlike in tests for differences of means,\n",
    "it's actually quite common for regression models to include\n",
    "priors, even if they aren't always thought of as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To understand why,\n",
    "we'll consider how a prior can help us answer the following question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we know when we can ignore the linear relationship between two variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example,\n",
    "Galton's original explanation of his findings\n",
    "presumed that individuals selected mates\n",
    "without respect to height:\n",
    "if not,\n",
    "then the predicted heights of grandchildren might not exhibit\n",
    "\"regression to mediocrity\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead of making this assumption, we can check the data:\n",
    "we have both maternal and paternal heights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If the paternal height can be used to\n",
    "predict the maternal height to great accuracy,\n",
    "then the assumption that mate selection mostly ignores height\n",
    "is unlikely to be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as selection_effect_model:\n",
    "    Slope = pm.Flat(\"Slope\")\n",
    "    ObservedValues = pm.Normal(\"MaternalHeights\",\n",
    "                               mu=Slope * standardized_maternal_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_paternal_heights)\n",
    "\n",
    "selection_effect_trace = shared_util.sample_from(selection_effect_model)\n",
    "selection_effect_posterior_df = shared_util.samples_to_dataframe(selection_effect_trace)\n",
    "\n",
    "selection_effect_MLE = pm.find_MAP(model=selection_effect_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = pm.plot_posterior(selection_effect_trace, ref_val=0, figsize=(12, 6), text_size=16);\n",
    "ax.vlines(selection_effect_MLE[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C4\", label=\"Flat Prior MAP / MLE\");\n",
    "ax.set_ylim(1.2 * np.array(ax.get_ylim())); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It seems that the 95% posterior density does not include 0,\n",
    "so by the method we've been using so far in the class,\n",
    "we'd have to conclude that there's a flaw in Galton's analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Furthermore, the Maximum Likelihood Estimate is not 0 either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## It is important to consider the _magnitude_ of a relationship, not just whether it is above or below 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The posterior above indicates that we should change our expectation of a mother's height\n",
    "by somewhere around a tenth of a standard deviation every time the father gets taller or shorter by a standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is a _very small_ effect.\n",
    "\n",
    "For a father who is three standard deviations away from average height,\n",
    "aka someone who is six feet, two inches,\n",
    "we predict a maternal height of less than an inch above average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter the ROPE: Region of Practical Equivalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before running our analysis, we define a set of values, close to 0,\n",
    "which we consider to be _practically equivalent to 0_.\n",
    "\n",
    "#### We call this the Region Of Practical Equivalence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we obtain a posterior, we can check the overlap between our posterior and this region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For posteriors with more mass close to 0, this overlap will be large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = pm.plot_posterior(selection_effect_trace, ref_val=0, rope=(-0.05, 0.05),\n",
    "                       figsize=(12, 6), text_size=16);\n",
    "ax.vlines(selection_effect_MLE[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C4\", label=\"Flat Prior MAP / MLE\");\n",
    "ax.set_ylim(1.2 * np.array(ax.get_ylim())); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "I selected as my ROPE here -0.05 to 0.05:\n",
    "a correlation is _effectively_ 0 if it suggests\n",
    "that I only need to change my prediction by a factor of 1 in 20 or less\n",
    "when I take into account the independent variable.\n",
    "\n",
    "Note that the ROPE would be different if we had not standardized our data --\n",
    "the size of the ROPE is dependent on the meaning of the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As usual, we test the probability we assign to the statement\n",
    "\"the correlation is within the region of practical equivalence\"\n",
    "by checking whether it is true on samples from our posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def is_in_ROPE(sample, ROPE=(-0.05, 0.05)):\n",
    "    return ROPE[0] < sample < ROPE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "selection_effect_posterior_df[\"Slope\"].apply(is_in_ROPE).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is a fairly decent chance, around 25%,\n",
    "that the correlation between maternal and paternal heights is negligible,\n",
    "according to this choice of ROPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## But the MAP estimate of the parameter is still outside the ROPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So according to our model,\n",
    "we should say that the most likely effect is neither 0 nor negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If we change our priors, and so incorporate different beliefs into our model, we can change our posteriors and MAP estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# In particular, we can choose a &#8220;skeptical&#8221; prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our choice of `Flat` prior indicated that we thought _any_ slope was possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We might want to instead adopt\n",
    "[Ockham's Razor](https://plato.stanford.edu/entries/ockham/),\n",
    "which states that simpler models are to be preferred to more complex ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A model in which there is no effect is considered simpler than a model in which there is an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This principle is motivated less by an understanding of mechanism and more by what is essentially a preference:\n",
    "we would prefer to work with simpler models,\n",
    "if we can get away with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some elevate Ockham's Razor all the way to a law of nature,\n",
    "like Isaac Newton:\n",
    "> [Nature does nothing in vain, and more is in vain when less will serve; for Nature is pleased with simplicity, and affects not the pomp of superfluous causes.](https://en.wikisource.org/wiki/Page:Newton%27s_Principia_(1846).djvu/390)\n",
    "\n",
    "That is, nature itself is simple.\n",
    "\n",
    "I consider that position\n",
    "[somewhat dubious](https://www.theatlantic.com/science/archive/2016/08/occams-razor/495332/),\n",
    "especially when it comes to\n",
    "claims about things like regression slopes.\n",
    "To say there is _absolutely no relationship_ between the heights of mothers and fathers\n",
    "seems far too strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is,\n",
    "if our prior puts high weight on values inside the ROPE,\n",
    "then it will require strong evidence,\n",
    "in terms of the likelihood,\n",
    "in order for our posterior to put high weight on values outside the ROPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Normal prior centered at 0 can be a skeptical prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Normal distribution puts almost all of its weight\n",
    "within a few standard deviations of its mean,\n",
    "and so with the correct choice of parameters,\n",
    "we can put a high weight on the ROPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The choice of a Normal prior for slope parameters is known as\n",
    "[_ridge regression_ in statistics](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b),\n",
    "[weight decay in the neural network literature](https://metacademy.org/graphs/concepts/weight_decay_neural_networks),\n",
    "or [$\\ell_2$ regularization in machine learning](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$\\ell_2$, pronounced \"ell-two\", is the more abstract mathematical equivalent of the sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general,\n",
    "if you come across a \"regularization\" mechanism\n",
    "in a machine learning algorithm,\n",
    "it is almost always expressing a Bayesian prior --\n",
    "implicitly or explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's see how incorporating this \"skeptical Normal prior\"\n",
    "changes our posterior and MAP estimate\n",
    "for the relationship between maternal and paternal heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as selection_ridge_model:\n",
    "    # ridge regression <> Normal prior on slope\n",
    "    Slope = pm.Normal(\"Slope\", mu=0, sd=2.5e-2)\n",
    "    # This prior says: I think there is a 95% chance that\n",
    "    #  the correlation is between -5e-2 and +5e-2 (and so inside the ROPE)\n",
    "    \n",
    "    \n",
    "    MaternalHeights = pm.Normal(\"MaternalHeights\",\n",
    "                               mu=Slope * standardized_maternal_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_paternal_heights)\n",
    "\n",
    "selection_ridge_trace = shared_util.sample_from(selection_ridge_model)\n",
    "selection_ridge_posterior_df = shared_util.samples_to_dataframe(selection_ridge_trace)\n",
    "\n",
    "selection_ridge_MAP = pm.find_MAP(model=selection_ridge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = pm.plot_posterior(selection_ridge_trace, rope=(-0.05, 0.05), ref_val=0,\n",
    "                       figsize=(12, 6), text_size=16)\n",
    "\n",
    "ax.vlines(selection_ridge_MAP[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6, color=\"C1\",\n",
    "          label=\"Gaussian Prior MAP\")\n",
    "ax.vlines(selection_effect_MLE[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C4\", label=\"Flat Prior MAP / MLE\");\n",
    "ax.set_ylim(1.2 * np.array(ax.get_ylim())); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The overlap between the posterior and the ROPE is now much greater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "selection_ridge_posterior_df[\"Slope\"].apply(is_in_ROPE).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "and the MAP value is in the Region of Practical Equivalence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "is_in_ROPE(selection_effect_MLE[\"Slope\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But the MAP value is still not exactly 0.\n",
    "\n",
    "Is it possible to express our skepticism about effects in\n",
    "a prior that gives us _eactly_ 0 values for the MAP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To obtain MAP values that are exactly 0, we use a different prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the following distributions\n",
    "which are members of the\n",
    "[_Laplace_](https://en.wikipedia.org/wiki/Laplace_distribution)\n",
    "family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "xs = np.linspace(-3, 3, num=1000)\n",
    "ax.plot(xs, np.exp(pm.Laplace.dist(mu=0, b=0.1).logp(xs).eval()), lw=4);\n",
    "ax.plot(xs, np.exp(pm.Laplace.dist(mu=0, b=1).logp(xs).eval()), lw=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A Laplace distribution is made of two exponential distributions,\n",
    "one positive and one negative,\n",
    "\"stapled\" together at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Relative to a Normal distribution,\n",
    "the Laplace places even greater weight on 0 value\n",
    "and on relatively large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Laplace distribution is also used as a &#8220;skeptical&#8221; prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The choice of a Laplace prior for slope parameters is known as\n",
    "[_lasso regression_ in statistics](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b),\n",
    "[sparse weight prior in the neural network literature](https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a),\n",
    "or [$\\ell_1$ regularization in machine learning](https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the right choice of parameters,\n",
    "we can even ensure that it puts approximately the same amount of total probability on the slope being inside the Region of Practical Equivalence as did the Normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(pm.Laplace.dist(mu=0, b=0.015).random(size=5000)).apply(is_in_ROPE).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's see how incorporating this \"skeptical Laplace prior\"\n",
    "changes our posterior and MAP estimate\n",
    "for the relationship between maternal and paternal heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as selection_lasso_model:\n",
    "    # lasso regression <> Laplace prior on slope\n",
    "    Slope = pm.Laplace(\"Slope\", mu=0, b=0.015)\n",
    "    MaternalHeights = pm.Normal(\"MaternalHeights\",\n",
    "                               mu=Slope * standardized_maternal_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_paternal_heights)\n",
    "\n",
    "selection_lasso_trace = shared_util.sample_from(selection_lasso_model, target_accept=0.9)\n",
    "selection_lasso_posterior_df = shared_util.samples_to_dataframe(selection_lasso_trace)\n",
    "selection_lasso_MAP = pm.find_MAP(model=selection_lasso_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = pm.plot_posterior(selection_lasso_trace, rope=(-0.05, 0.05), ref_val=0,\n",
    "                       figsize=(12, 6), text_size=16)\n",
    "\n",
    "ax.vlines(selection_ridge_MAP[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6, color=\"C1\",\n",
    "          label=\"Gaussian Prior MAP\")\n",
    "ax.vlines(selection_effect_MLE[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C4\", label=\"Flat Prior MAP / MLE\");\n",
    "ax.vlines(selection_lasso_MAP[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C5\", label=\"Laplace Prior MAP\");\n",
    "ax.set_ylim(1.2 * np.array(ax.get_ylim())); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The overlap between the posterior and the ROPE remains high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "selection_lasso_posterior_df[\"Slope\"].apply(is_in_ROPE).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "and the MAP value is not only in the Region of Practical Equivalence, but it's exactly 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "is_in_ROPE(selection_effect_MLE[\"Slope\"]), selection_lasso_MAP[\"Slope\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But note that the posterior probability of the parameter being 0\n",
    "is 0,\n",
    "because the posterior is still a density,\n",
    "over continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "(selection_effect_MLE[\"Slope\"] == 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Resolving this and actually obtaining a model that places\n",
    "non-zero probability on the parameter being 0 requires more advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Note that with sufficient evidence, it is still possible for the posterior to be outside the ROPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, we still recover a non-zero value\n",
    "for the slope for the original problem considered by Galton,\n",
    "predicting the heights of offspring from the midparental height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as galton_ridge_model:\n",
    "    Slope = pm.Normal(\"Slope\", mu=0, sd=2.5e-2)\n",
    "    # This prior says: I think there is a 95% chance\n",
    "    #  the correlation is between -5e-2 and 5e-2 (and so inside the ROPE)\n",
    "    \n",
    "    Heights = pm.Normal(\"Heights\",\n",
    "                               mu=Slope * standardized_midparental_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_heights)\n",
    "\n",
    "galton_ridge_trace = shared_util.sample_from(galton_ridge_model)\n",
    "\n",
    "galton_ridge_MAP = pm.find_MAP(model=galton_ridge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = pm.plot_posterior(galton_ridge_trace, rope=(-0.05, 0.05), ref_val=0,\n",
    "                       figsize=(12, 6), text_size=16)\n",
    "\n",
    "ax.vlines(galton_ridge_MAP[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6, color=\"C1\",\n",
    "          label=\"Gaussian Prior MAP\")\n",
    "ax.vlines(galton_MLE[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C4\", label=\"Flat Prior MAP / MLE\");\n",
    "ax.set_ylim(1.2 * np.array(ax.get_ylim())); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that the posterior is still closer to 0\n",
    "than is the MLE.\n",
    "\n",
    "This is because our prior, the Normal centered at 0,\n",
    "puts substantially more weight on values with lower magnitude.\n",
    "\n",
    "This effect is called\n",
    "[shrinkage](https://en.wikipedia.org/wiki/Shrinkage_estimator).\n",
    "\n",
    "It is not necessarily a bad thing,\n",
    "since shrinkage often counters\n",
    "overestimation bias in Maximum Likelihood Estimates,\n",
    "but the shrinkage caused by a Normal prior is often too strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's compare the results for the Laplace prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as galton_lasso_model:\n",
    "    # lasso regression <> Laplace prior on slope\n",
    "    Slope = pm.Laplace(\"Slope\", mu=0, b=0.015)\n",
    "    Heights = pm.Normal(\"Heights\",\n",
    "                               mu=Slope * standardized_heights,\n",
    "                               sd=1,\n",
    "                               observed=standardized_midparental_heights)\n",
    "\n",
    "galton_lasso_trace = shared_util.sample_from(galton_lasso_model, target_accept=0.9)\n",
    "\n",
    "galton_lasso_MAP = pm.find_MAP(model=galton_lasso_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ax = pm.plot_posterior(galton_lasso_trace, rope=(-0.05, 0.05), ref_val=0,\n",
    "                       figsize=(12, 6), text_size=16)\n",
    "\n",
    "ax.vlines(galton_ridge_MAP[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6, color=\"C1\",\n",
    "          label=\"Gaussian Prior MAP\")\n",
    "ax.vlines(galton_MLE[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C4\", label=\"Flat Prior MAP / MLE\");\n",
    "ax.vlines(galton_lasso_MAP[\"Slope\"], 0, ax.get_ylim()[1] * 0.5, lw=6,\n",
    "          color=\"C5\", label=\"Laplace Prior MAP\");\n",
    "ax.set_ylim(1.2 * np.array(ax.get_ylim())); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the posterior is much closer to the MLE value,\n",
    "while still slightly lower.\n",
    "\n",
    "The Laplace prior, while placing more weight at 0 than the Normal,\n",
    "also places more weight in the tails.\n",
    "Remember that it is equivalent to a pair of Exponential distributions,\n",
    "and the Exponential distribution,\n",
    "which we've used as a prior for the standard deviation in many models,\n",
    "has a heavy tail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we want a prior that expresses the same \"skepticism\"\n",
    "that inspired null hypothesis significance testing\n",
    "without biasing our results too far downwards,\n",
    "then the Laplace prior is often a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This property of the Laplace prior gives this regression another name:\n",
    "_sparse linear regression_,\n",
    "since the values of the MAP parameters are often 0."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
