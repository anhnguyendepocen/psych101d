{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 10a - Formulas and Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)\n",
    "\n",
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Least squares regression models have a Normal likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y \\sim \\text{Normal}(f(X), \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear least-squares models relate the independent to the dependent variable with a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y \\sim \\text{Normal}(\\text{slope}\\cdot f(X) + \\text{intercept}, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iris_df = sns.load_dataset(\"iris\", data_home=Path(\"..\") / \"..\" / \"shared\" / \"data\")\n",
    "\n",
    "iris_df[\"is_setosa\"] = iris_df[\"species\"].apply(lambda s: bool(s == \"setosa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_df, hue=\"is_setosa\", vars=[\"sepal_length\", \"sepal_width\"], height=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The simplest linear model always predicts the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is the model used to set the baseline for calculating variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_sepal_length = iris_df[\"sepal_length\"].mean()\n",
    "sd_sepal_length = iris_df[\"sepal_length\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.distplot(iris_df[\"sepal_length\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mean_model_by_hand:\n",
    "    Mean_Sepal_Length = pm.Normal(\"Mean\", mu=mean_sepal_length, sd=1e6)\n",
    "    Sigma_Sepal_Length = pm.Exponential(\"Sigma\", lam=0.5 / sd_sepal_length)\n",
    "    Sepal_Lengths = pm.Normal(\"Sepal Lengths\",\n",
    "                              mu=Mean_Sepal_Length, sd=Sigma_Sepal_Length,\n",
    "                              observed=iris_df[\"sepal_length\"])\n",
    "\n",
    "mean_model_by_hand_samples = shared_util.sample_from(mean_model_by_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(mean_model_by_hand_samples, figsize=(12, 6), text_size=16,\n",
    "                  ref_val=[mean_sepal_length, sd_sepal_length]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Technically, this model doesn't quite look like the canonical linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "y \\sim \\text{Normal}\\left(\\mu, \\sigma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "versus the linear model form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "y \\sim \\text{Normal}\\left(\\text{slope}\\cdot \\texttt{f}(X), \\sigma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the `Intercept` is gone.\n",
    "With this approach,\n",
    "the `Intercept` ends up just becoming\n",
    "the \"slope\" of a data feature that is all 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## To match the two, we just need do define the right `f`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def map_to_1(x):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "y \\sim \\text{Normal}\\left(\\text{slope}\\cdot 1, \\sigma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is called a _data feature_:\n",
    "a non-linearly transformed version of a variable\n",
    "that we want to use to in a linear model.\n",
    "\n",
    "We are always allowed to compute features of our data before performing linear regression --\n",
    "this is sometimes called _linearization_.\n",
    "\n",
    "One view is that [neural networks are \"just\" an automated version of linearization](https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Homework 05 includes a deeper dive on feature encoding for linnear models of categorical data,\n",
    "which is a special case of linearization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "all_ones = iris_df[\"sepal_length\"].apply(map_to_1)\n",
    "\n",
    "with pm.Model() as mean_model_by_hand_two:\n",
    "    Mean_Sepal_Length = pm.Normal(\"Intercept\", mu=mean_sepal_length, sd=1e6)\n",
    "    sd = pm.Exponential(\"sd\", lam=0.5 / sd_sepal_length)\n",
    "    Sepal_Lengths = pm.Normal(\"Sepal Lengths\",\n",
    "                              mu=Mean_Sepal_Length * all_ones,\n",
    "                              sd=sd,\n",
    "                              observed=iris_df[\"sepal_length\"])\n",
    "\n",
    "mean_model_by_hand_samples = shared_util.sample_from(mean_model_by_hand_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(mean_model_by_hand_samples, figsize=(12, 6), text_size=16,\n",
    "                  ref_val=[mean_sepal_length, sd_sepal_length]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today we'll consider a different way to specify models: formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A formula is a string that describes a model,\n",
    "indicating which variables are to be used as linear predictors for which other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The variable to be predicted is placed on the far left,\n",
    "then followed by a `~`, or \"distributed as\" symbol\n",
    "(a tilde on many keyboards),\n",
    "then a description of the variables to be used in the model follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "y \\sim \\text{Normal}(\\text{slope}\\cdot x + \\text{intercept}, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "for a `DataFrame` with columns `x` and `y` becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "\"y ~ x\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also call Python functions inside a formula,\n",
    "meaning that the generic least-squares regression model might be written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "y \\sim \\text{Normal}(\\text{slope}\\cdot \\texttt{f}(x) + \\text{intercept}, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "\"y ~ f(x)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "and the predictions would differ depending on our definition of `f`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We can produce the simple mean-based model of sepal length with a simple formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It uses a special symbol: `1`,\n",
    "to represent the all-ones `Series`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "\"sepal_length ~ 1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We can create (generalized) linear models from formulas in pyMC with `pm.GLM.from_formula`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `G` for _generalized_ means we can choose different likelihoods than the Normal,\n",
    "but we won't see that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as mean_glm_model:\n",
    "    # we define a model context and then call the function,\n",
    "    #  providing the formula as an argument:\n",
    "    mean_glm = pm.GLM.from_formula(\n",
    "        \"sepal_length ~ 1\",\n",
    "        data=iris_df,  # the df must be provided so the formula can be used\n",
    "        family=\"normal\"  # which family is used for the likelihood? (defaults to \"normal\")\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we print the resulting object,\n",
    "we can see which variables are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sampling and MAP inference proceed exactly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mean_glm_samples = shared_util.sample_from(mean_glm_model)\n",
    "mean_glm_MAP = pm.find_MAP(model=mean_glm_model)\n",
    "mean_glm_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As with all models with a Normal likelihood,\n",
    "we can measure how good our model is by looking\n",
    "at the mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mse(predictions, observations):\n",
    "    return ((predictions - observations) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We just need a function to compute our predictions\n",
    "given the input data and the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def compute_prediction_mean(data, parameters):\n",
    "    return parameters[\"Intercept\"] * data.apply(map_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mse(compute_prediction_mean(iris_df[\"sepal_width\"], mean_glm_MAP), iris_df[\"sepal_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: this is the `mse` that goes into the denominator of the variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "pm.plot_posterior_predictive_glm(  # pymc function to visualize posterior of simple GLMs\n",
    "    mean_glm_samples,  # samples from posterior\n",
    "    eval=iris_df[\"sepal_width\"],  # `data` in compute_prediction f'n above\n",
    "    lm=compute_prediction_mean  # function that takes in data and element of sample,\n",
    "                                #  and returns a prediction\n",
    ")\n",
    "\n",
    "sns.scatterplot(\"sepal_width\", \"sepal_length\", data=iris_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# We can also specify categorical models with the `GLM`+Formula interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"sepal_length\", x=\"is_setosa\", data=iris_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The formula remains simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We just need to note, with a `C`, which variables in the model are categorical:\n",
    "\n",
    "```\n",
    "\"sepal_length ~ C(is_setosa)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `1`, for `Intercept`,\n",
    "is included automatically in every formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as categorical_glm_model:\n",
    "    categorical_glm = pm.GLM.from_formula(\n",
    "        \"sepal_length ~ C(is_setosa)\",\n",
    "        data=iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "categorical_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "By default, categorical variables are handled by converting them to \"Treatment Coding\",\n",
    "\n",
    "`Intercept` corresponds to what we called the \"mean of the baseline group\" (`is_setosa` being `False`)\n",
    "and\n",
    "`C(is_setosa)[T.True]` corresponds to what we called the \"effect of the `is_setosa` factor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_glm_samples = shared_util.sample_from(categorical_glm_model)\n",
    "categorical_MAP = pm.find_MAP(model=categorical_glm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to compute predictions in this model,\n",
    "we need to define the preidction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_prediction_categorical(data, parameters):\n",
    "    return parameters[\"Intercept\"] * data.apply(map_to_1)\\\n",
    "            + parameters[\"C(is_setosa)[T.True]\"] * data  # add this value only when `is_setosa` is True\n",
    "\n",
    "categorical_MAP_predictions = compute_prediction_categorical(iris_df[\"is_setosa\"], categorical_MAP)\n",
    "\n",
    "mse(categorical_MAP_predictions, iris_df[\"sepal_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "1 - mse(categorical_MAP_predictions, iris_df[\"sepal_length\"]) / mse(mean_glm_MAP[\"Intercept\"], iris_df[\"sepal_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "pm.plot_posterior_predictive_glm(categorical_glm_samples, eval=iris_df[\"is_setosa\"],\n",
    "                                 lm=compute_prediction_categorical)\n",
    "\n",
    "sns.stripplot(y=\"sepal_length\", x=\"is_setosa\", data=iris_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Specification is even simpler for a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{sepal length} \\sim \\text{Normal}(\\text{slope}\\cdot \\text{sepal width} + \\text{intercept}, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "\"sepal_length ~ sepal_width\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It doesn't seem particularly promising,\n",
    "since there isn't an obvious linear relationship\n",
    "between the values,\n",
    "but we can try fitting a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"sepal_width\", y=\"sepal_length\", data=iris_df, height=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as regression_glm_model:\n",
    "    regression_glm = pm.GLM.from_formula(\n",
    "        \"sepal_length ~ 1 + sepal_width\",\n",
    "        data=iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "regression_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "regression_glm_samples = shared_util.sample_from(regression_glm)\n",
    "\n",
    "regression_MAP = pm.find_MAP(model=regression_glm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(regression_glm_samples, figsize=(12, 12), text_size=16,\n",
    "                  ref_val=[mean_sepal_length, 0, sd_sepal_length]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_regression(data, parameters):\n",
    "    return parameters[\"Intercept\"] * data.apply(map_to_1) + parameters[\"sepal_width\"] * data\n",
    "\n",
    "regression_MAP_predictions = compute_prediction_regression(iris_df[\"sepal_width\"], regression_MAP)\n",
    "\n",
    "mse(regression_MAP_predictions, iris_df[\"sepal_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice that the MSE is close to the MSE of the baseline, mean-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "pm.plot_posterior_predictive_glm(regression_glm_samples, eval=iris_df[\"sepal_width\"],\n",
    "                                 lm=compute_prediction_regression)\n",
    "ax.scatter(iris_df[\"sepal_width\"], iris_df[\"sepal_length\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# With the right approach, it seems like a linear model could fit this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "sns.scatterplot(\"sepal_width\", \"sepal_length\", data=iris_df, hue=\"is_setosa\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We'd like to be able to fit two linear models: one for setosas and one for other irises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Formulas make it easy to create complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `:` symbol is used to create models with interactions in them.\n",
    "\n",
    "In this case,\n",
    "an \"interaction\" means that the slope of the line is different for setosas and non-setosas,\n",
    "in addition to the intercept being different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as combined_glm_model:\n",
    "    combined_glm = pm.GLM.from_formula(\"sepal_length ~ 1 + sepal_width + C(is_setosa) + sepal_width:C(is_setosa)\",\n",
    "        data=iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "combined_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`sepal_width:C(is_setosa)[T.True]` is the \"interaction effect\":\n",
    "the difference in slopes for the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "combined_glm_samples = shared_util.sample_from(combined_glm)\n",
    "\n",
    "combined_glm_posterior_df = shared_util.samples_to_dataframe(combined_glm_samples)\n",
    "\n",
    "combined_glm_MAP = pm.find_MAP(model=combined_glm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ref_vals = [0, 0, 0, 0, sd_sepal_length]\n",
    "pm.plot_posterior(combined_glm_samples, figsize=(8, 12), text_size=12,\n",
    "                 ref_val=ref_vals, rope=(-0.1, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "On casual inspection,\n",
    "it seems that our hypothesis was wrong:\n",
    "our posteriors for the interaction and for the categorical effect\n",
    "straddle 0 and overlap with the ROPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def in_ROPE(value, rope=(-0.1, 0.1)):\n",
    "    return rope[0] < value < rope[1]\n",
    "\n",
    "combined_glm_posterior_df[\"C(is_setosa)[T.True]\"].apply(in_ROPE).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_glm_posterior_df[\"sepal_width:C(is_setosa)[T.True]\"].apply(in_ROPE).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But what we should really check is whether _both_ are in the ROPE,\n",
    "and so _both_ can be ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(combined_glm_posterior_df[\"sepal_width:C(is_setosa)[T.True]\"].apply(in_ROPE)\n",
    " * combined_glm_posterior_df[\"C(is_setosa)[T.True]\"].apply(in_ROPE)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The values of parameters in the posterior are almost always _correlated_,\n",
    "and so we cannot reason about any single parameter without considering the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More complex linear models require us to encode our data properly in order to calculate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def encode_data_combined(iris_df):\n",
    "    data = np.zeros(shape=(len(iris_df), 3))\n",
    "    data[:, 0] = 1\n",
    "    data[:, 1] = iris_df[\"sepal_width\"].values\n",
    "    data[:, 2] = np.array(iris_df[\"is_setosa\"].values, dtype=np.int)\n",
    "    return data\n",
    "\n",
    "\n",
    "def compute_prediction_combined(data, posterior_sample):\n",
    "    data = np.atleast_2d(data)\n",
    "    all_ones = data[:, 0]\n",
    "    sepal_width = data[:, 1]\n",
    "    is_setosa = data[:, 2]\n",
    "    return posterior_sample[\"Intercept\"] * all_ones\\\n",
    "        + posterior_sample[\"sepal_width\"] * sepal_width\\\n",
    "        + posterior_sample[\"C(is_setosa)[T.True]\"] * is_setosa\\\n",
    "        + posterior_sample[\"sepal_width:C(is_setosa)[T.True]\"] * (is_setosa * sepal_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice how the prediction is equal to the sum of the parameters times the data values?\n",
    "This is the signature of a linear model.\n",
    "\n",
    "You'll work through data encoding for categorical models with interactions in Homework 05,\n",
    "where you'll use `DataFrame`s instead of `array`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Predictions in hand, we can compute the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoded_data = encode_data_combined(iris_df)\n",
    "combined_MAP_predictions = compute_prediction_combined(encoded_data, combined_glm_MAP)\n",
    "\n",
    "mse(combined_MAP_predictions, iris_df[\"sepal_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is quite a bit lower than the regression or categorical models separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "setosa_selector = iris_df[\"is_setosa\"]\n",
    "ax.plot(encoded_data[~setosa_selector, 1], combined_MAP_predictions[~setosa_selector], lw=4);\n",
    "ax.plot(encoded_data[setosa_selector, 1], combined_MAP_predictions[setosa_selector], lw=4);\n",
    "\n",
    "sns.scatterplot(\"sepal_width\", \"sepal_length\", data=iris_df, hue=\"is_setosa\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The approach of specifying models from formulas has advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One major advantage of this approach is speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We specify our model by writing down a string\n",
    "and selecting a few hyperparameters,\n",
    "which can make the process as fast as running a `seaborn` plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One major disadvantage is the loss in explicitness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A `pm.Model` specification usually writes out every single variable\n",
    "and their relationships directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The assumptions are immediately legible to you and to anyone working with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In contrast,\n",
    "the assumptions of a GLM are hidden inside\n",
    "some very opaque code in pyMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Another advantage is that it matches frequentist practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The `statsmodels` package for frequentist testing in Python uses formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Creating our simple models is just as straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mean_MLE_model = smf.ols(  # specifically for models with normal likelihood\n",
    "    \"sepal_length ~ 1\",\n",
    "    data=iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The equivalent of `.find_MAP` is `.fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_MLE_results = mean_MLE_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results are reported in the form of a summary table,\n",
    "which describes the outcomes and parameters of a number of statistical tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_MLE_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Most folks focus on a few of entries in this summary:\n",
    "- `Prob (F-statistic)` tells you the p-value of an ANOVA applied to this model.\n",
    "- `P>|t|` tells you the p-value for a t-test applied to that parameter by itself.\n",
    "\n",
    "Unsurprisingly,\n",
    "we find that the null hypothesis that the data has mean 0\n",
    "is not supported by the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## If we wish to test a different model, we simply change the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_MLE_model = smf.ols(\"sepal_length ~ C(is_setosa)\", data=iris_df)\n",
    "\n",
    "categorical_MLE_results = categorical_MLE_model.fit()\n",
    "\n",
    "categorical_MLE_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "regression_MLE_model = smf.ols(\"sepal_length ~ sepal_width\", data=iris_df)\n",
    "\n",
    "regression_MLE_results = regression_MLE_model.fit()\n",
    "\n",
    "regression_MLE_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But beware, because using frequentist techniques can lead you astray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "combined_MLE_model = smf.ols(\"sepal_length ~ sepal_width*C(is_setosa)\", data=iris_df)\n",
    "\n",
    "combined_MLE_results = combined_MLE_model.fit()\n",
    "\n",
    "combined_MLE_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`\"a*b\"` is short-hand for `\"a + b + a:b\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice how the p-value for the interaction\n",
    "`sepal_width:C(is_setosa)[T.True]`\n",
    "and for the categorical effect `C(is_setosa)[T.True]`\n",
    "are both above the traditional threshold for significance.\n",
    "\n",
    "And yet, the data very obviously supports the conclusion\n",
    "that the model that excludes those factors is extremely poor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "setosa_selector = iris_df[\"is_setosa\"]\n",
    "ax.plot(encoded_data[~setosa_selector, 1], combined_MAP_predictions[~setosa_selector], lw=4);\n",
    "ax.plot(encoded_data[setosa_selector, 1], combined_MAP_predictions[setosa_selector], lw=4);\n",
    "sns.scatterplot(\"sepal_width\", \"sepal_length\", data=iris_df, hue=\"is_setosa\", ax=ax);\n",
    "pm.plot_posterior_predictive_glm(regression_glm_samples, eval=iris_df[\"sepal_width\"],\n",
    "                                 lm=compute_prediction_regression)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The black lines represent the posterior predictions from the regression model,\n",
    "and their lack of agreement with the data,\n",
    "relative to the combined model, is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generalized linear models allow us to work with data\n",
    "that's not well described by a Gaussian likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "y \\sim \\text{Foo}(f(\\text{slope}\\cdot X + \\text{intercept}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, the model we used for the putting success data looked like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\text{successes} \\sim \\text{Binomial}(N, \\texttt{squash}(\\text{slope}\\cdot \\text{distance} + \\text{intercept}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we choose a different `family`,\n",
    "we can obtain posterior estimates for other kinds of GLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.GLM.from_formula??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similarly, we call a different function, `smf.glm`,\n",
    "to create GLMs in `statsmodels`,\n",
    "and provide it with a `fmaily` argument that specifies the likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "smf_binomial = smf.glm(?, data=?,\n",
    "                       family=sm.families.Binomial)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
